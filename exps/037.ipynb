{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "# 学習データ5週分(検証データ1週含む)\n",
    "# 候補作り12週分\n",
    "# trendingで候補作り\n",
    "# articlesとcustomersの特徴量をtarget_weekごとに作る\n",
    "# 候補の良さを測る\n",
    "# 顧客毎の最終週で候補作り\n",
    "# コラボ商品で候補作り\n",
    "# 説明文KNN(tsne, 重複除外なし)\n",
    "# ペア商品\n",
    "# コラボのtarget_week=-1修正したやつ\n",
    "# albertでKNN\n",
    "# 説明文KNNのtsneのcos距離の掛け算を修正する\n",
    "# BertでKNN\n",
    "# 画像KNN\n",
    "# New: 購入履歴Word2VecでKNN(tsne)\n",
    "# MAP@12 (all): 0.032402\n",
    "# MAP@12 (cold start): 0.006806\n",
    "EXP = '037'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from psutil import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "from time import time\n",
    "import warnings\n",
    "from functools import reduce\n",
    "\n",
    "from line_notify import send_line_notification\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = None\n",
    "warnings.simplefilter('ignore', pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "data_path = Path('../input/h-and-m-personalized-fashion-recommendations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>t_diff</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31788319</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>929511001</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788320</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>891322004</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788321</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371721</td>\n",
       "      <td>918325001</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788322</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371747</td>\n",
       "      <td>833459002</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788323</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371960</td>\n",
       "      <td>898573003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_dat  customer_id  article_id     price  sales_channel_id  \\\n",
       "31788319 2020-09-22      1371691   929511001  0.059305                 2   \n",
       "31788320 2020-09-22      1371691   891322004  0.042356                 2   \n",
       "31788321 2020-09-22      1371721   918325001  0.043203                 1   \n",
       "31788322 2020-09-22      1371747   833459002  0.006763                 1   \n",
       "31788323 2020-09-22      1371960   898573003  0.033881                 2   \n",
       "\n",
       "          t_diff  week  \n",
       "31788319       0     0  \n",
       "31788320       0     0  \n",
       "31788321       0     0  \n",
       "31788322       0     0  \n",
       "31788323       0     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transactions = pd.read_csv(\n",
    "    data_path / f'transactions_train.csv',\n",
    "    # set dtype or pandas will drop the leading '0' and convert to int\n",
    "    dtype={'article_id': 'int32'},\n",
    "    parse_dates=['t_dat'])\n",
    "customers = pd.read_csv(data_path / 'customers.csv')\n",
    "articles = pd.read_csv(\n",
    "    '../input/h-and-m-personalized-fashion-recommendations/articles.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "\n",
    "t_max = transactions['t_dat'].max()\n",
    "transactions['t_diff'] = (t_max - transactions['t_dat']).dt.days\n",
    "transactions['week'] = transactions['t_diff'] // 7\n",
    "\n",
    "# Noneの表記不揃い対策\n",
    "customers.loc[~customers['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "\n",
    "# メモリ削減\n",
    "id_to_index_dict = dict(zip(customers[\"customer_id\"], customers.index))\n",
    "index_to_id_dict = dict(zip(customers.index, customers[\"customer_id\"]))\n",
    "transactions[\"customer_id\"] = transactions[\"customer_id\"].map(id_to_index_dict).astype('int32')\n",
    "customers['customer_id'] = customers['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "\n",
    "print(transactions.shape)\n",
    "display(transactions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_trending(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    # 以下のロジックはtrendingの公開カーネル参照\n",
    "    weekly_sales = df.groupby(['week', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    df = df.merge(weekly_sales, on=['week', 'article_id'], how='left')    \n",
    "    weekly_sales = weekly_sales.reset_index().set_index('article_id')\n",
    "    df = df.join(weekly_sales.loc[weekly_sales['week']==1, ['count']], on='article_id', rsuffix='_targ')\n",
    "    df['count_targ'].fillna(0, inplace=True)\n",
    "    df['quotient'] = df['count_targ'] / df['count']\n",
    "    \n",
    "    t_max = df['t_dat'].max()\n",
    "    df['x'] = ((t_max - df['t_dat']) / np.timedelta64(1, 'D')).astype(int)\n",
    "    df['dummy_1'] = 1\n",
    "    df['x'] = df[['x', 'dummy_1']].max(axis=1)    \n",
    "    a, b, c, d = 2.5e4, 1.5e5, 2e-1, 1e3\n",
    "    df['y'] = a / np.sqrt(df['x']) + b * np.exp(-c*df['x']) - d\n",
    "    df['dummy_0'] = 0 \n",
    "    df['y'] = df[[\"y\", \"dummy_0\"]].max(axis=1)\n",
    "    df['trending_value'] = df['quotient'] * df['y']\n",
    "    df = df.groupby(['customer_id', 'article_id']).agg({'trending_value': 'sum'}).reset_index()\n",
    "    df = df.loc[df['trending_value'] > 0]\n",
    "    # df['rank'] = df.groupby(\"customer_id\")[\"value\"].rank(\"dense\", ascending=False)\n",
    "    # df = df.loc[df['rank'] <= 12]\n",
    "    \n",
    "    df['isin_trending'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'trending_value', 'isin_trending']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_recently(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''直近の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 12\")  # 12週分の履歴を使用\n",
    "    \n",
    "    # 各週の購入個数から特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = df.query('week == @w').groupby(['customer_id', 'article_id'])['article_id'].count().rename(f'count_{w}w').reset_index().copy()\n",
    "        if w == 1:\n",
    "            purchase_df = tmp\n",
    "            continue\n",
    "        purchase_df = purchase_df.merge(tmp, how='outer', on=['customer_id', 'article_id'])\n",
    "        \n",
    "    purchase_df['isin_recently'] = 1\n",
    "    \n",
    "    return purchase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_popular(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''販売数の多い商品から候補生成'''\n",
    "    # make_articles_featureが販売数から特徴量を作ってくれるから、この関数は候補のペアだけ返す\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 4\")  # 4週分の履歴を使用\n",
    "\n",
    "    # 各週の販売数上位12個\n",
    "    dummy_count_df = df.groupby(['article_id', 'week'])['week'].count().rename('dummy_count').reset_index().copy()\n",
    "    dummy_count_df['rank_in_week'] = dummy_count_df.groupby('week')['dummy_count'].rank(method='min', ascending=False)\n",
    "    dummy_articles = dummy_count_df.query('rank_in_week <= 12')['article_id'].unique()\n",
    "\n",
    "    dummy_df = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [np.repeat(customers, repeats=len(dummy_articles)).reshape(-1, 1),\n",
    "            np.repeat(dummy_articles[None, :], repeats=len(customers), axis=0).reshape(-1, 1)],\n",
    "            axis=1),\n",
    "        columns = ['customer_id', 'article_id']\n",
    "    )\n",
    "    \n",
    "    dummy_df['isin_popular'] = 1\n",
    "    \n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_lastw(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''customer毎の最後の購入から1週間の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\").copy()\n",
    "\n",
    "    # 最後の購入から1週間に絞る\n",
    "    df['max_dat'] = df['customer_id'].map(df.groupby('customer_id')['t_dat'].max())\n",
    "    df['max_diff'] = (df['max_dat'] - df['t_dat']).dt.days\n",
    "    df = df.query('max_diff <= 6')\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count_lastw').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.sort_values('t_dat', ascending=True)\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    df = df.rename(columns={'t_dat': 'last_dat'})\n",
    "    df['last_dat'] = df['last_dat'].view(np.int64) // 10 ** 9\n",
    "    \n",
    "    df['isin_lastw'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'count_lastw', 'last_dat', 'isin_lastw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_pair(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''ルールベース協調フィルタリングから候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    pairs = pd.read_parquet(f'../input/hmitempairs/pairs_fold{target_week}_5items.parquet')\n",
    "    df = df.merge(pairs, on='article_id', how='inner')\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'pair_article_id': 'article_id'})\n",
    "    df['count_pair'] = df['count'] * df['pair_ratio']\n",
    "\n",
    "    df['isin_pair'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'count_pair', 'isin_pair']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_collabo(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''コラボ商品の購入履歴から候補生成'''\n",
    "    df = pd.read_csv('../input/ranking_features/collabo_candidate_with_nseries.csv', dtype={'article_id': 'int32'})\n",
    "    # FIXME: ほんとはスコープ外変数の参照は良くない\n",
    "    df['customer_id'] = df['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "    df = df.query('customer_id in @customers').copy()\n",
    "    df = df.query('target_week == @target_week').copy()\n",
    "    \n",
    "    df['isin_collabo'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_candidates_desc_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int, encoder: str):\n",
    "    '''説明文の埋め込みベクトル（次元削減済み）から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    \n",
    "    desc_knn = pd.read_csv(f'../input/ranking_features/item_features_{encoder}_class.csv', dtype={'article_id': 'int32'})\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        dfs.append(df.merge(desc_knn[['article_id', f'knn_article_id_{i}', f'knn_distance_{i}']])\n",
    "                        .rename(columns={f'knn_article_id_{i}': 'knn_article_id', f'knn_distance_{i}': f'{encoder}_distance'}))\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article_id': 'article_id'})\n",
    "    df[f'{encoder}_count'] = df['count'] * abs(1-df[f'{encoder}_distance'])\n",
    "    \n",
    "    # # 重複を削除、item1とitem2それぞれの類似として同じitemが出てくることがある\n",
    "    # df = df.sort_values(f'{encoder}_count', ascending=True).drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "    \n",
    "    df[f'isin_{encoder}_knn'] = 1\n",
    "                   \n",
    "    return df[['customer_id', 'article_id', f'{encoder}_count', f'{encoder}_distance', f'isin_{encoder}_knn']]\n",
    "\n",
    "def _generate_candidates_albert_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "\n",
    "    albert_knn = pd.read_csv('../input/ranking_features/Albert_cos.csv', dtype={'article': 'int32', 'knn_article': 'int32'})\n",
    "    albert_knn = albert_knn.rename(columns={'article': 'article_id'})\n",
    "    \n",
    "    df = df.merge(albert_knn)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article': 'article_id', 'distances': 'albert_distance'})\n",
    "    df['albert_count'] = df['count'] * abs(1-df['albert_distance'])\n",
    "    \n",
    "    df['isin_albert_knn'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'albert_count', 'albert_distance', 'isin_albert_knn']]\n",
    "\n",
    "def _generate_candidates_bert_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "\n",
    "    usecols=['article_id', 'knn_article_id_0', 'knn_distance_0', 'knn_article_id_1', 'knn_distance_1', \n",
    "             'knn_article_id_2', 'knn_distance_2', 'knn_article_id_3', 'knn_distance_3', 'knn_article_id_4', 'knn_distance_4']\n",
    "    bert_knn = pd.read_csv('../input/ranking_features/article_bert_pairs.csv', dtype={'article_id': 'int32'}, usecols=usecols)\n",
    "\n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        dfs.append(df.merge(bert_knn[['article_id', f'knn_article_id_{i}', f'knn_distance_{i}']])\n",
    "                        .rename(columns={f'knn_article_id_{i}': 'knn_article_id', f'knn_distance_{i}': 'bert_distance'}))\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article_id': 'article_id'})\n",
    "    df['bert_count'] = df['count'] * abs(1-df['bert_distance'])\n",
    "    \n",
    "    df['isin_bert_knn'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'bert_count', 'bert_distance', 'isin_bert_knn']]    \n",
    "    \n",
    "def generate_candidates_desc_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''商品説明文から候補生成'''\n",
    "    desc_knns = []\n",
    "    \n",
    "    desc_knns.append(_generate_candidates_desc_knn(transactions, customers, target_week, encoder='tsne'))\n",
    "    # desc_knns.append(_generate_candidates_desc_knn(transactions, customers, target_week, encoder='umap'))\n",
    "    desc_knns.append(_generate_candidates_albert_knn(transactions, customers, target_week))\n",
    "    desc_knns.append(_generate_candidates_bert_knn(transactions, customers, target_week))\n",
    "    \n",
    "    df = reduce(lambda left, right: pd.merge(left, right, how='outer', on=['customer_id', 'article_id']), desc_knns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_im_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''画像の埋め込みベクトルから候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    \n",
    "    im_knn = pd.read_parquet(f'../input/ranking_features/candidates_knn_vgg.parquet')\n",
    "    im_knn['article_id'] = im_knn['article_id'].astype('int32')\n",
    "    im_knn['knn_im_article_id'] = im_knn['knn_im_article_id'].astype('int32')\n",
    "    \n",
    "    df = df.merge(im_knn)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_im_article_id': 'article_id', 'knn_im_distance': 'vgg_distance'})\n",
    "    df[f'vgg_count'] = df['count'] * abs(1-df['vgg_distance'])\n",
    "    \n",
    "    # # 重複を削除、item1とitem2それぞれの類似として同じitemが出てくることがある\n",
    "    # df = df.sort_values(f'{encoder}_count', ascending=True).drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "    \n",
    "    df[f'isin_vgg_knn'] = 1\n",
    "                   \n",
    "    return df[['customer_id', 'article_id', 'vgg_count', 'vgg_distance', 'isin_vgg_knn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_sequence_word2vec(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''購入履歴をシーケンスと見立て、article_idをword2vec'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    \n",
    "    desc_knn = pd.read_csv(f'../input/ranking_features/item_features_tsne_word2vec_date.csv', dtype={'article_id': 'int32'})\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        dfs.append(df.merge(desc_knn[['article_id', f'knn_article_id_{i}', f'knn_distance_{i}']])\n",
    "                        .rename(columns={f'knn_article_id_{i}': 'knn_article_id', f'knn_distance_{i}': 'sequence_word2vec_distance'}))\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article_id': 'article_id'})\n",
    "    df['sequence_word2vec_count'] = df['count'] * df['sequence_word2vec_distance']\n",
    "    \n",
    "    # # 重複を削除、item1とitem2それぞれの類似として同じitemが出てくることがある\n",
    "    # df = df.sort_values(f'{encoder}_count', ascending=True).drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "    \n",
    "    df['isin_sequence_word2vec_knn'] = 1\n",
    "                   \n",
    "    return df[['customer_id', 'article_id', 'sequence_word2vec_count', 'sequence_word2vec_distance', 'isin_sequence_word2vec_knn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_customers_feature(customers: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''customer毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    customers_feature = customers.drop(['postal_code'], axis=1).copy()\n",
    "    customers_feature.loc[~customers_feature['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None  # Noneの表記揃え\n",
    "    customers_feature[['FN', 'Active']] = customers_feature[['FN', 'Active']].fillna(0)\n",
    "\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_purchase = df.groupby(['customer_id', 'week'])['week'].count().rename('purchase').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_purchase.groupby('customer_id')['purchase'].agg(agg_name)\n",
    "        customers_feature[f'purchase_{agg_name}_groupby_customer'] = customers_feature['customer_id'].map(agg_sr)\n",
    "    \n",
    "    # 各週の購入数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_purchase[weekly_purchase['week']==w]\n",
    "        tmp = tmp[['customer_id', 'purchase']].set_index('customer_id')['purchase']\n",
    "        customers_feature[f'purchase_{w}w'] = customers_feature['customer_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_ratio_{w}w'] = customers_feature[f'purchase_{w}w'] / customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_diff_{w}w'] = customers_feature[f'purchase_{w}w'] - customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "\n",
    "    # --- 一意の(article_id, week)を購入単位とみなす ---\n",
    "    # ※あるarticleを1個買うことを、ふつうは購入単位とみなしている\n",
    "    # rank: 何回目の購入か\n",
    "    unique_transactions = df[['customer_id', 'article_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['customer_id', 'article_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再購入したarticleの割合\n",
    "    customers_feature['repurchase_article'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article'] = customers_feature['customer_id'].map(unique_transactions.drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count())\n",
    "    customers_feature['repurchase_article_percent'] = customers_feature['repurchase_article'] / customers_feature['purchase_article']\n",
    "\n",
    "    # 再購入を含む週の割合\n",
    "    customers_feature['repurchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count()).fillna(0)\n",
    "    customers_feature['purchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count())\n",
    "    customers_feature['repurchase_week_percent'] = customers_feature['repurchase_week'] / customers_feature['purchase_week']\n",
    "    \n",
    "    # 再購入の割合\n",
    "    customers_feature['repurchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('customer_id')['customer_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.groupby('customer_id')['customer_id'].count())\n",
    "    customers_feature['repurchase_article_and_week_percent'] = customers_feature['repurchase_article_and_week'] / customers_feature['purchase_article_and_week']\n",
    "    # --- おわり ---\n",
    "    \n",
    "    return customers_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_articles_feature(articles: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''article毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    articles_feature = articles.drop(\n",
    "        ['prod_name', 'product_type_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'prod_name', 'department_name', 'detail_desc'], \n",
    "        axis=1).copy()\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_sale = df.groupby(['article_id', 'week'])['week'].count().rename('sale').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_sale.groupby('article_id')['sale'].agg(agg_name)\n",
    "        articles_feature[f'sale_{agg_name}_groupby_article'] = articles_feature['article_id'].map(agg_sr)\n",
    "\n",
    "    # 各週の販売数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_sale[weekly_sale['week']==w]\n",
    "        tmp = tmp[['article_id', 'sale']].set_index('article_id')['sale']\n",
    "        articles_feature[f'sale_{w}w'] = articles_feature['article_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_ratio_{w}w'] = articles_feature[f'sale_{w}w'] / articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_diff_{w}w'] = articles_feature[f'sale_{w}w'] - articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "\n",
    "    # --- 一意の(customer_id, week)を販売単位とみなす ---\n",
    "    # ※あるcustomerに1つ売れることを、ふつうは販売単位とみなしている\n",
    "    # rank: 何回目の販売か\n",
    "    unique_transactions = df[['article_id', 'customer_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['article_id', 'customer_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再販売したcustomerの割合\n",
    "    articles_feature['resale_customer'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer'] = articles_feature['article_id'].map(unique_transactions.drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count())\n",
    "    articles_feature['resale_customer_percent'] = articles_feature['resale_customer'] / articles_feature['sale_customer']\n",
    "\n",
    "    # 再販売を含む週の割合\n",
    "    articles_feature['resale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count()).fillna(0)\n",
    "    articles_feature['sale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count())\n",
    "    articles_feature['resale_week_percent'] = articles_feature['resale_week'] / articles_feature['sale_week']\n",
    "\n",
    "    # 再販売の割合\n",
    "    articles_feature['resale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('article_id')['article_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.groupby('article_id')['article_id'].count())\n",
    "    articles_feature['resale_customer_and_week_percent'] = articles_feature['resale_customer_and_week'] / articles_feature['sale_customer_and_week']\n",
    "    # --- 終わり ---\n",
    "    \n",
    "    return articles_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_df(transactions: pd.DataFrame, week: int, is_labeled: bool, use_customers: np.ndarray = None, metric_verbose: bool = True, compress_verbose: bool = False):\n",
    "    '''ランク学習モデルへの入力データ作成'''\n",
    "    # WARNING: 戦略を追加した時に書き換え忘れ注意！\n",
    "    strategy_flags = [\n",
    "        'isin_trending', \n",
    "        'isin_recently', \n",
    "        'isin_popular', \n",
    "        'isin_lastw', \n",
    "        'isin_pair', \n",
    "        'isin_collabo', \n",
    "        'isin_tsne_knn', \n",
    "        'isin_albert_knn',\n",
    "        'isin_bert_knn',\n",
    "        'isin_vgg_knn',\n",
    "        'isin_sequence_word2vec_knn',\n",
    "    ]\n",
    "    kwargs = {'how': 'outer', 'on': ['customer_id', 'article_id'], 'copy': True}\n",
    "    \n",
    "    # 入力データに含むcustomersの絞り込み\n",
    "    if use_customers is not None:\n",
    "        data_customers = use_customers\n",
    "    elif week >= 0:\n",
    "        data_customers = transactions.query(\"week == @week\")['customer_id'].unique()\n",
    "    else:\n",
    "        raise ValueError('set use_customers as something when week=-1.')\n",
    "    \n",
    "    # 候補作り\n",
    "    data_dfs = []\n",
    "    data_dfs.append(generate_candidates_trending(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_recently(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_popular(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_lastw(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_pair(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_collabo(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_desc_knn(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_im_knn(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_sequence_word2vec(transactions, data_customers, week))\n",
    "    \n",
    "    data_df = reduce(\n",
    "        lambda  left,right: pd.merge(left, right, on=['customer_id', 'article_id'], how='outer'), \n",
    "        data_dfs)\n",
    "\n",
    "    data_df[strategy_flags] = data_df[strategy_flags].fillna(0)\n",
    "    \n",
    "    # 正解ラベル付け\n",
    "    if is_labeled:\n",
    "        if week < 0:\n",
    "            raise ValueError(f\"can't label when week={week}.\")\n",
    "        data_actual = transactions.query(\"week == @week\")[['customer_id', 'article_id']].drop_duplicates()\n",
    "        data_actual['label'] = 1\n",
    "        data_df = data_df.merge(data_actual, how='left', on=['customer_id', 'article_id'])\n",
    "        data_df['label'] = data_df['label'].fillna(0)\n",
    "    \n",
    "    # customers, articles の特徴量\n",
    "    data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    data_customers_feature = compress_df(\n",
    "        make_customers_feature(customers, transactions, target_week=week, debug=True), \n",
    "        verbose=compress_verbose)\n",
    "    data_articles_feature = compress_df(\n",
    "        make_articles_feature(articles, transactions, target_week=week, debug=True), \n",
    "        verbose=compress_verbose)\n",
    "    data_df = data_df.merge(data_customers_feature, how='left', on=['customer_id'])\n",
    "    data_df = data_df.merge(data_articles_feature, how='left', on=['article_id'])\n",
    "    # data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    \n",
    "    # 候補のスコア\n",
    "    if metric_verbose:\n",
    "        print(f\"[Info] shape     : {data_df.shape}\")\n",
    "        print(f\"[Info] mem       : {data_df.memory_usage().sum() / 1024**2 :5.2f} Mb\")\n",
    "        print(f\"[Info] candidates: {len(data_df) / len(data_customers):.1f} 個 / customer\")\n",
    "        display(data_df[strategy_flags].astype(float).mean())\n",
    "        if is_labeled:\n",
    "            print(f\"[Info] Precision: {data_df['label'].sum() / len(data_df):.5f}\")\n",
    "            print(f\"[Info] Recall   : {data_df['label'].sum() / len(data_actual):.5f}\")\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_df(\n",
    "    df: pd.DataFrame, \n",
    "    category_columns: list =['club_member_status', 'fashion_news_frequency', 'product_group_name', 'index_code'], \n",
    "    verbose: bool =True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    '''DataFrameのデータ型を適切に選び圧縮'''\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        bar = tqdm(df.columns, leave=False)\n",
    "    else:\n",
    "        bar = df.columns\n",
    "    for col in bar:\n",
    "        col_type = df[col].dtypes\n",
    "        if col in category_columns:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(category)\")\n",
    "            df[col] = df[col].astype('category')\n",
    "        elif col_type in numerics:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(num)\")\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params: dict, cols: list, tr_df: pd.DataFrame, val_df: pd.DataFrame = None, early_stopping: bool = True):\n",
    "    if val_df is None:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dtrain], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    else:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "        if early_stopping:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.early_stopping(10, first_metric_only=True), lgb.log_evaluation(10)])\n",
    "        else:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_df: pd.DataFrame, model_folds: list):\n",
    "    pred = np.zeros(len(test_df))\n",
    "    for w in model_folds:\n",
    "        with open(f\"../models/lgb_rank/{EXP}_model_fold{w}.pkl\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        pred += model.predict(test_df[cols], num_iteration=model.best_iteration)    \n",
    "    pred = pred/len(model_folds)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_sr(test_df: pd.DataFrame, pred: np.ndarray):\n",
    "    '''モデルのスコアを用いて並び替え＆上位12個抽出'''\n",
    "    test_df['predict_score'] = pred\n",
    "    test_df = test_df.sort_values('predict_score', ascending=False).drop_duplicates(['customer_id', 'article_id'], keep='first').reset_index(drop=True)\n",
    "    test_df['rank'] = test_df.groupby('customer_id')['predict_score'].rank('min', ascending=False)\n",
    "    test_df = test_df[test_df['rank'] <= 12]\n",
    "    \n",
    "    # test_df['article_id'] = le.inverse_transform(test_df['article_id'])\n",
    "    test_df['article_id'] = ' 0' + test_df['article_id'].astype(str)\n",
    "    \n",
    "    top_sr = test_df.groupby('customer_id')['article_id'].sum()\n",
    "    \n",
    "    return top_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optunaによるチューニング\n",
    "# import optuna.integration.lightgbm as lgb_optuna\n",
    "# from optuna.logging import set_verbosity\n",
    "# import numpy as np\n",
    "# import random as rn\n",
    "\n",
    "# set_verbosity(-1)\n",
    "# np.random.seed(71)\n",
    "# rn.seed(71)\n",
    "\n",
    "# params = {\n",
    "#     'objective': 'lambdarank',\n",
    "#     'boosting': 'gbdt',  # default: 'gbdt', 'gbdt' or 'dart'\n",
    "#     'num_iterations': 1000,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'metric': ['ndcg'],\n",
    "#     'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "#     'random_state': 71,  # 訓練用とは違う値にする\n",
    "#     'verbosity': -1,  # -1: ignore, 0: warnings, 1: info\n",
    "#     'deterministic': True,  # 再現性確保\n",
    "#     'force_row_wise': True  # 再現性確保\n",
    "# }\n",
    "\n",
    "# tr_df = make_data_df(transactions, 2, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "# cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "# tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "# train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "\n",
    "# val_df = make_data_df(transactions, 1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "# val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "\n",
    "# model = lgb_optuna.LightGBMTuner(params, dtrain, valid_sets=[dtrain, dval], early_stopping_rounds=10, verbose_eval=-1, optuna_seed=71)\n",
    "# model.run()\n",
    "\n",
    "# best_params = model.params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "params = {\n",
    "    # Core Parameters\n",
    "    'objective': 'lambdarank',\n",
    "    'boosting': 'gbdt',  # default: 'gbdt', ['gbdt', 'dart', 'goss'], dart: 超遅いけど高精度\n",
    "    'num_iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,  # default: 31, large for accuracy\n",
    "    'num_threads': cpu_count(logical=False),\n",
    "    'random_state': 41,\n",
    "\n",
    "    # Learning Control Parameters\n",
    "    'force_col_wise': True,\n",
    "    # 'histogram_pool_size': -1.0,  # default: -1.0, max cache size in MB for historical histogram, (histogram_pool_size + dataset size) = approximately RAM used\n",
    "    'min_data_in_leaf': 20,  # default: 20, large dealing with over-fitting\n",
    "    'min_sum_hessian_in_leaf': 1e-3,  # default: 1e-3, large dealing with over-fitting\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    # 'drop_rate': 0.1,  # used only in dart, a fraction of previous trees to drop during the dropout\n",
    "    'verbosity': 0,  # 0: warnings, 1: info\n",
    "\n",
    "    # Dataset Parameters\n",
    "    'max_bin': 255,  # default: 255, large for accuracy\n",
    "    'min_data_in_bin': 3,  # default: 3, \n",
    "    'bin_construct_sample_cnt': 200000,  # default: 200000, larger for accuracy\n",
    "\n",
    "    # Objective Parameters\n",
    "    'lambdarank_truncation_level': 30,\n",
    "    'lambdarank_norm': True,\n",
    "    # 'label_gain': [0, 1],\n",
    "\n",
    "    # Metric Parameters\n",
    "    'metric': ['ndcg'],\n",
    "    'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c449c7960e4a93ba84f45c1b29b437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_week(fold): 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 989.38 Mb (71.2% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 614.96 Mb (74.9% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 48.62 Mb (75.0% reduction)\n",
      "[Info] shape     : (9787213, 520)\n",
      "[Info] mem       : 9809.84 Mb\n",
      "[Info] candidates: 135.9 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending                 0.129098\n",
       "isin_recently                 0.070616\n",
       "isin_popular                  0.324147\n",
       "isin_lastw                    0.032522\n",
       "isin_pair                     0.109818\n",
       "isin_collabo                  0.000194\n",
       "isin_tsne_knn                 0.105414\n",
       "isin_albert_knn               0.241267\n",
       "isin_bert_knn                 0.114106\n",
       "isin_vgg_knn                  0.094727\n",
       "isin_sequence_word2vec_knn    0.106523\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00280\n",
      "[Info] Recall   : 0.11874\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.854674\tvalid_1's ndcg@12: 0.861179\n",
      "[20]\ttraining's ndcg@12: 0.856998\tvalid_1's ndcg@12: 0.862994\n",
      "[30]\ttraining's ndcg@12: 0.858921\tvalid_1's ndcg@12: 0.863106\n",
      "[40]\ttraining's ndcg@12: 0.860518\tvalid_1's ndcg@12: 0.863131\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's ndcg@12: 0.860142\tvalid_1's ndcg@12: 0.863353\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 3\n",
      "[Info] shape     : (10548831, 520)\n",
      "[Info] mem       : 10573.22 Mb\n",
      "[Info] candidates: 131.4 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending                 0.131084\n",
       "isin_recently                 0.068452\n",
       "isin_popular                  0.348348\n",
       "isin_lastw                    0.032199\n",
       "isin_pair                     0.103561\n",
       "isin_collabo                  0.000182\n",
       "isin_tsne_knn                 0.099484\n",
       "isin_albert_knn               0.229666\n",
       "isin_bert_knn                 0.107928\n",
       "isin_vgg_knn                  0.089466\n",
       "isin_sequence_word2vec_knn    0.100477\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00246\n",
      "[Info] Recall   : 0.10177\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.870968\tvalid_1's ndcg@12: 0.85778\n",
      "[20]\ttraining's ndcg@12: 0.873107\tvalid_1's ndcg@12: 0.858806\n",
      "[30]\ttraining's ndcg@12: 0.87466\tvalid_1's ndcg@12: 0.858898\n",
      "[40]\ttraining's ndcg@12: 0.876122\tvalid_1's ndcg@12: 0.859375\n",
      "[50]\ttraining's ndcg@12: 0.877326\tvalid_1's ndcg@12: 0.859528\n",
      "[60]\ttraining's ndcg@12: 0.878853\tvalid_1's ndcg@12: 0.859521\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's ndcg@12: 0.878496\tvalid_1's ndcg@12: 0.859607\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 2\n",
      "[Info] shape     : (9554211, 520)\n",
      "[Info] mem       : 9576.30 Mb\n",
      "[Info] candidates: 126.0 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending                 0.129556\n",
       "isin_recently                 0.071349\n",
       "isin_popular                  0.320720\n",
       "isin_lastw                    0.033452\n",
       "isin_pair                     0.106787\n",
       "isin_collabo                  0.000202\n",
       "isin_tsne_knn                 0.102836\n",
       "isin_albert_knn               0.238702\n",
       "isin_bert_knn                 0.111433\n",
       "isin_vgg_knn                  0.092151\n",
       "isin_sequence_word2vec_knn    0.103996\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00284\n",
      "[Info] Recall   : 0.11391\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.864991\tvalid_1's ndcg@12: 0.855512\n",
      "[20]\ttraining's ndcg@12: 0.867055\tvalid_1's ndcg@12: 0.856951\n",
      "[30]\ttraining's ndcg@12: 0.868934\tvalid_1's ndcg@12: 0.856928\n",
      "[40]\ttraining's ndcg@12: 0.870527\tvalid_1's ndcg@12: 0.857207\n",
      "[50]\ttraining's ndcg@12: 0.871772\tvalid_1's ndcg@12: 0.8576\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's ndcg@12: 0.871615\tvalid_1's ndcg@12: 0.857772\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 1\n",
      "[Info] shape     : (8584435, 520)\n",
      "[Info] mem       : 8604.28 Mb\n",
      "[Info] candidates: 119.2 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending                 0.135442\n",
       "isin_recently                 0.076858\n",
       "isin_popular                  0.273729\n",
       "isin_lastw                    0.035344\n",
       "isin_pair                     0.114309\n",
       "isin_collabo                  0.000215\n",
       "isin_tsne_knn                 0.110510\n",
       "isin_albert_knn               0.258326\n",
       "isin_bert_knn                 0.119864\n",
       "isin_vgg_knn                  0.098906\n",
       "isin_sequence_word2vec_knn    0.111656\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00314\n",
      "[Info] Recall   : 0.11822\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.862068\tvalid_1's ndcg@12: 0.839019\n",
      "[20]\ttraining's ndcg@12: 0.864738\tvalid_1's ndcg@12: 0.840069\n",
      "[30]\ttraining's ndcg@12: 0.866981\tvalid_1's ndcg@12: 0.840781\n",
      "[40]\ttraining's ndcg@12: 0.869013\tvalid_1's ndcg@12: 0.839853\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttraining's ndcg@12: 0.867462\tvalid_1's ndcg@12: 0.841293\n",
      "Evaluated only: ndcg@12\n"
     ]
    }
   ],
   "source": [
    "# train lgb ranker\n",
    "best_iterations = []\n",
    "feature_importance_dfs = []\n",
    "oof_weeks = [4, 3, 2, 1]\n",
    "\n",
    "for i, w in enumerate(tqdm(oof_weeks)):\n",
    "    print(f\"\\ntarget_week(fold): {w}\")\n",
    "    if i == 0:\n",
    "        compress_verbose=True\n",
    "    else:\n",
    "        compress_verbose=False\n",
    "    \n",
    "    tr_df = make_data_df(transactions, w, is_labeled=True, metric_verbose=True, compress_verbose=compress_verbose)\n",
    "    val_df = make_data_df(transactions, w-1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    if i == 0:\n",
    "        exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "        cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "        with open(f'../models/lgb_rank/{EXP}_cols.pkl', 'wb') as f:\n",
    "            pickle.dump(cols, f)\n",
    "\n",
    "    model = train(params, cols, tr_df, val_df)\n",
    "    with open(f'../models/lgb_rank/{EXP}_model_fold{w}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    best_iterations.append(model.best_iteration)\n",
    "    feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': w}))\n",
    "    \n",
    "send_line_notification(f'[EXP{EXP}]\\ntrain finished exceped for last week.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] shape     : (8146935, 519)\n",
      "[Info] mem       : 8150.23 Mb\n",
      "[Info] candidates: 118.1 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending                 0.126067\n",
       "isin_recently                 0.071981\n",
       "isin_popular                  0.283381\n",
       "isin_lastw                    0.035404\n",
       "isin_pair                     0.113696\n",
       "isin_collabo                  0.000198\n",
       "isin_tsne_knn                 0.110230\n",
       "isin_albert_knn               0.258757\n",
       "isin_bert_knn                 0.119811\n",
       "isin_vgg_knn                  0.098272\n",
       "isin_sequence_word2vec_knn    0.111271\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.25654581 -4.24466881 -4.23387059 ...  2.76626312  2.7672224\n",
      "  2.7763691 ]\n"
     ]
    }
   ],
   "source": [
    "# predict val data\n",
    "val_df = make_data_df(transactions, 0, is_labeled=False, metric_verbose=True, compress_verbose=False)\n",
    "val_pred = predict(val_df, oof_weeks)\n",
    "print(np.sort(val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "80      0671607001 0436261001 0568601007 0754751001 0...\n",
       "86      0621381012 0889036004 0640021012 0880017001 0...\n",
       "107     0556255001 0399136061 0732842014 0732842021 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val top rank articles\n",
    "val_df2 = val_df.copy()\n",
    "val_pred_sr = extract_top_sr(val_df2, val_pred)\n",
    "display(val_pred_sr.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 popular items:\n",
      " 0909370001 0865799006 0918522001 0924243001 0448509014 0751471001 0809238001 0918292001 0762846027 0809238005 0673677002 0923758001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0     0685814003 0448509014 0918522001 0715624001 0...\n",
       "1.0     0909370001 0865799006 0924243001 0809238001 0...\n",
       "2.0     0909370001 0865799006 0918525001 0909371001 0...\n",
       "3.0     0909370001 0751471001 0673677002 0910601003 0...\n",
       "4.0     0918522001 0751471001 0751471043 0910601003 0...\n",
       "5.0     0918522001 0908799002 0896152002 0924243001 0...\n",
       "6.0     0736870001 0796210001 0908799002 0865799006 0...\n",
       "Name: top_12_popular_items, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 1]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "display(popular_items_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_lgb</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>prediction_popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 07...</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "\n",
       "                                          prediction prediction_lgb  age_bin  \\\n",
       "0  0909370001 0751471001 0673677002 0910601003 07...                     3.0   \n",
       "1  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "2  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "\n",
       "                                  prediction_popular  \n",
       "0   0909370001 0751471001 0673677002 0910601003 0...  \n",
       "1   0909370001 0865799006 0924243001 0809238001 0...  \n",
       "2   0909370001 0865799006 0924243001 0809238001 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(val_pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission_fold0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del val_df, model\n",
    "del transactions_last_week, top12, popular_items, popular_items_dict, popular_items_sr\n",
    "del val_df2, val_pred_sr, \n",
    "del submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] shape     : (8146935, 521)\n",
      "[Info] mem       : 8181.31 Mb\n",
      "[Info] candidates: 118.1 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending                 0.126067\n",
       "isin_recently                 0.071981\n",
       "isin_popular                  0.283381\n",
       "isin_lastw                    0.035404\n",
       "isin_pair                     0.113696\n",
       "isin_collabo                  0.000198\n",
       "isin_tsne_knn                 0.110230\n",
       "isin_albert_knn               0.258757\n",
       "isin_bert_knn                 0.119811\n",
       "isin_vgg_knn                  0.098272\n",
       "isin_sequence_word2vec_knn    0.111271\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00336\n",
      "[Info] Recall   : 0.12824\n",
      "[10]\ttraining's ndcg@12: 0.847166\n",
      "[20]\ttraining's ndcg@12: 0.849942\n",
      "[30]\ttraining's ndcg@12: 0.85209\n",
      "[40]\ttraining's ndcg@12: 0.853833\n"
     ]
    }
   ],
   "source": [
    "# train last target_week(=0) data\n",
    "tr_df = make_data_df(transactions, week=0, is_labeled=True, metric_verbose=True, compress_verbose=False)\n",
    "\n",
    "# valがないのでアーリーストッピングが使えない\n",
    "params['num_iterations'] = int(np.mean(best_iterations))\n",
    "model = train(params, cols, tr_df)\n",
    "with open(f\"../models/lgb_rank/{EXP}_model_fold0.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance(gain)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sale_1w</th>\n",
       "      <td>30184.772183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trending_value</th>\n",
       "      <td>25224.696420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_max_groupby_article_ratio_1w</th>\n",
       "      <td>17888.819389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_dat</th>\n",
       "      <td>14206.291987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_mean_groupby_article_ratio_1w</th>\n",
       "      <td>10399.240804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_sum_groupby_article_ratio_1w</th>\n",
       "      <td>7386.917978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_lastw</th>\n",
       "      <td>6428.327465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_pair</th>\n",
       "      <td>5667.737917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert_distance</th>\n",
       "      <td>3793.644107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_distance</th>\n",
       "      <td>3736.375615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_1w</th>\n",
       "      <td>2952.754849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_sum_groupby_customer_ratio_1w</th>\n",
       "      <td>2694.533138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isin_recently</th>\n",
       "      <td>2577.950109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_code</th>\n",
       "      <td>2265.523888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_group_name</th>\n",
       "      <td>2254.021255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_1w</th>\n",
       "      <td>2141.357914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>2124.967919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resale_customer_and_week_percent</th>\n",
       "      <td>1839.569919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_article</th>\n",
       "      <td>1754.773316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_mean_groupby_article_diff_1w</th>\n",
       "      <td>1714.742148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        importance(gain)\n",
       "feature                                                 \n",
       "sale_1w                                     30184.772183\n",
       "trending_value                              25224.696420\n",
       "sale_max_groupby_article_ratio_1w           17888.819389\n",
       "last_dat                                    14206.291987\n",
       "sale_mean_groupby_article_ratio_1w          10399.240804\n",
       "sale_sum_groupby_article_ratio_1w            7386.917978\n",
       "count_lastw                                  6428.327465\n",
       "count_pair                                   5667.737917\n",
       "albert_distance                              3793.644107\n",
       "bert_distance                                3736.375615\n",
       "count_1w                                     2952.754849\n",
       "purchase_sum_groupby_customer_ratio_1w       2694.533138\n",
       "isin_recently                                2577.950109\n",
       "product_code                                 2265.523888\n",
       "product_group_name                           2254.021255\n",
       "purchase_1w                                  2141.357914\n",
       "age                                          2124.967919\n",
       "resale_customer_and_week_percent             1839.569919\n",
       "purchase_article                             1754.773316\n",
       "sale_mean_groupby_article_diff_1w            1714.742148"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance_df = pd.concat(feature_importance_dfs, ignore_index=True, axis=0)\n",
    "display(feature_importance_df.groupby(['feature'])[['importance(gain)']].mean().sort_values('importance(gain)', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tr_df, params, model, best_iterations, feature_importance_dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e75e67ddcd46c9a97463beeec472d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] shape     : (5660162, 520)\n",
      "[Info] mem       : 5673.25 Mb\n",
      "[Info] candidates: 56.6 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending                 0.068150\n",
       "isin_recently                 0.044265\n",
       "isin_popular                  0.536875\n",
       "isin_lastw                    0.063165\n",
       "isin_pair                     0.068032\n",
       "isin_collabo                  0.000284\n",
       "isin_tsne_knn                 0.065808\n",
       "isin_albert_knn               0.156204\n",
       "isin_bert_knn                 0.071292\n",
       "isin_vgg_knn                  0.059663\n",
       "isin_sequence_word2vec_knn    0.066400\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "0     0568601043 0568601006 0568601044 0779781015 0...\n",
       "1     0924243001 0866731001 0915529005 0924243002 0...\n",
       "2     0794321007 0791587001 0924243001 0805000001 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict test data\n",
    "BATCH_SIZE = 100_000\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "test_customers = submission['customer_id'].map(id_to_index_dict).unique()\n",
    "\n",
    "# バッチ処理\n",
    "def process(i):\n",
    "    if i == (len(test_customers)//BATCH_SIZE):\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : ]\n",
    "    else:\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
    "    \n",
    "    if i == 0:  # meric_verbose=True\n",
    "        test_df = make_data_df(transactions, week=-1, is_labeled=False, use_customers=test_customers_batch, metric_verbose=True, compress_verbose=False)\n",
    "    else:       # metric_verbose=False\n",
    "        test_df = make_data_df(transactions, week=-1, is_labeled=False, use_customers=test_customers_batch, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    all_weeks = oof_weeks + [0]\n",
    "    pred = predict(test_df, all_weeks)\n",
    "    return extract_top_sr(test_df, pred)\n",
    "\n",
    "# single process execution\n",
    "preds = []\n",
    "for i in tqdm(range(len(test_customers)//BATCH_SIZE + 1)):\n",
    "    preds.append(process(i))\n",
    "\n",
    "# # multi process execution\n",
    "# # cpus = cpu_count(logical=False)\n",
    "# cpus = 4\n",
    "# print('cpu(core): ', cpus)\n",
    "# preds = Parallel(n_jobs=cpus, verbose=0)( [delayed(process)(i) for i in range(len(test_customers)//BATCH_SIZE + 1)] )\n",
    "\n",
    "pred_sr = pd.concat(preds, axis=0)\n",
    "display(pred_sr.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # most popular items\n",
    "# transactions_last_week = transactions.loc[transactions.week == 0]\n",
    "# top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "# print(\"Top 12 popular items:\")\n",
    "# print( top12 )\n",
    "\n",
    "# customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "# transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "# popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "# popular_items_dict = {}\n",
    "# for index in popular_items.index.levels[0]:\n",
    "#     popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "# popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "# popular_items_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0568601043 0568601006 0568601044 0779781015 08...</td>\n",
       "      <td>0568601043 0568601006 0568601044 0779781015 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0924243001 0866731001 0915529005 0924243002 06...</td>\n",
       "      <td>0924243001 0866731001 0915529005 0924243002 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0794321007 0791587001 0924243001 0805000001 09...</td>\n",
       "      <td>0794321007 0791587001 0924243001 0805000001 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "\n",
       "                                          prediction  \\\n",
       "0  0568601043 0568601006 0568601044 0779781015 08...   \n",
       "1  0924243001 0866731001 0915529005 0924243002 06...   \n",
       "2  0794321007 0791587001 0924243001 0805000001 09...   \n",
       "\n",
       "                                      prediction_lgb  \n",
       "0   0568601043 0568601006 0568601044 0779781015 0...  \n",
       "1   0924243001 0866731001 0915529005 0924243002 0...  \n",
       "2   0794321007 0791587001 0924243001 0805000001 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "# submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "# submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "# submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission.csv', index=False)\n",
    "\n",
    "send_line_notification(f'[EXP{EXP}]\\nprediction finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f804528fa1bcda80b2057737773baa2134c5be7e013153f88e69abab752260b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
