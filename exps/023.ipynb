{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "# 学習データ5週分(検証データ1週含む)\n",
    "# 候補作り12週分\n",
    "# trendingで候補作り\n",
    "# articlesとcustomersの特徴量をtarget_weekごとに作る\n",
    "# 候補の良さを測る\n",
    "# pairで候補作り\n",
    "# 顧客毎の最終週で候補作り\n",
    "# New: アンダーフィッティング気味なので、より学習させる✅\n",
    "# New: Optunaによるパラメータチューニング❌ カーネルが落ちる\n",
    "\n",
    "# ベース\n",
    "# MAP@12 (all): 0.030636\n",
    "# MAP@12 (cold start): 0.006791\n",
    "# num_leaves: 31 -> 80\n",
    "# MAP@12 (all): 0.030241\n",
    "# MAP@12 (cold start): 0.005878\n",
    "# bagging_fraction: 0.75 -> 0.80, feature_fraction: 1 -> 0.8\n",
    "# MAP@12 (all): 0.031079（再実行したら0.030450、要検証）\n",
    "# MAP@12 (cold start): 0.007570\n",
    "\n",
    "EXP = '023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from psutil import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = None\n",
    "warnings.simplefilter('ignore', pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "data_path = Path('../input/h-and-m-personalized-fashion-recommendations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>t_diff</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31788319</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>929511001</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788320</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>891322004</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788321</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371721</td>\n",
       "      <td>918325001</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788322</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371747</td>\n",
       "      <td>833459002</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788323</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371960</td>\n",
       "      <td>898573003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_dat  customer_id  article_id     price  sales_channel_id  \\\n",
       "31788319 2020-09-22      1371691   929511001  0.059305                 2   \n",
       "31788320 2020-09-22      1371691   891322004  0.042356                 2   \n",
       "31788321 2020-09-22      1371721   918325001  0.043203                 1   \n",
       "31788322 2020-09-22      1371747   833459002  0.006763                 1   \n",
       "31788323 2020-09-22      1371960   898573003  0.033881                 2   \n",
       "\n",
       "          t_diff  week  \n",
       "31788319       0     0  \n",
       "31788320       0     0  \n",
       "31788321       0     0  \n",
       "31788322       0     0  \n",
       "31788323       0     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transactions = pd.read_csv(\n",
    "    data_path / f'transactions_train.csv',\n",
    "    # set dtype or pandas will drop the leading '0' and convert to int\n",
    "    dtype={'article_id': 'int32'},\n",
    "    parse_dates=['t_dat'])\n",
    "customers = pd.read_csv(data_path / 'customers.csv')\n",
    "articles = pd.read_csv(\n",
    "    '../input/h-and-m-personalized-fashion-recommendations/articles.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "\n",
    "t_max = transactions['t_dat'].max()\n",
    "transactions['t_diff'] = (t_max - transactions['t_dat']).dt.days\n",
    "transactions['week'] = transactions['t_diff'] // 7\n",
    "\n",
    "# Noneの表記不揃い対策\n",
    "customers.loc[~customers['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "\n",
    "# メモリ削減\n",
    "id_to_index_dict = dict(zip(customers[\"customer_id\"], customers.index))\n",
    "index_to_id_dict = dict(zip(customers.index, customers[\"customer_id\"]))\n",
    "transactions[\"customer_id\"] = transactions[\"customer_id\"].map(id_to_index_dict).astype('int32')\n",
    "customers['customer_id'] = customers['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "\n",
    "print(transactions.shape)\n",
    "display(transactions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_trending(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    # 以下のロジックはtrendingの公開カーネル参照\n",
    "    weekly_sales = df.groupby(['week', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    df = df.merge(weekly_sales, on=['week', 'article_id'], how='left')    \n",
    "    weekly_sales = weekly_sales.reset_index().set_index('article_id')\n",
    "    df = df.join(weekly_sales.loc[weekly_sales['week']==1, ['count']], on='article_id', rsuffix='_targ')\n",
    "    df['count_targ'].fillna(0, inplace=True)\n",
    "    df['quotient'] = df['count_targ'] / df['count']\n",
    "    \n",
    "    t_max = df['t_dat'].max()\n",
    "    df['x'] = ((t_max - df['t_dat']) / np.timedelta64(1, 'D')).astype(int)\n",
    "    df['dummy_1'] = 1\n",
    "    df['x'] = df[['x', 'dummy_1']].max(axis=1)    \n",
    "    a, b, c, d = 2.5e4, 1.5e5, 2e-1, 1e3\n",
    "    df['y'] = a / np.sqrt(df['x']) + b * np.exp(-c*df['x']) - d\n",
    "    df['dummy_0'] = 0 \n",
    "    df['y'] = df[[\"y\", \"dummy_0\"]].max(axis=1)\n",
    "    df['trending_value'] = df['quotient'] * df['y']\n",
    "    df = df.groupby(['customer_id', 'article_id']).agg({'trending_value': 'sum'}).reset_index()\n",
    "    df = df.loc[df['trending_value'] > 0]\n",
    "    # df['rank'] = df.groupby(\"customer_id\")[\"value\"].rank(\"dense\", ascending=False)\n",
    "    # df = df.loc[df['rank'] <= 12]\n",
    "    \n",
    "    df['isin_trending'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'trending_value', 'isin_trending']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_recently(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''直近の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 12\")  # 12週分の履歴を使用\n",
    "    \n",
    "    # 各週の購入個数から特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = df.query('week == @w').groupby(['customer_id', 'article_id'])['article_id'].count().rename(f'count_{w}w').reset_index().copy()\n",
    "        if w == 1:\n",
    "            purchase_df = tmp\n",
    "            continue\n",
    "        purchase_df = purchase_df.merge(tmp, how='outer', on=['customer_id', 'article_id'])\n",
    "        \n",
    "    purchase_df['isin_recently'] = 1\n",
    "    \n",
    "    return purchase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_popular(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''販売数の多い商品から候補生成'''\n",
    "    # make_articles_featureが販売数から特徴量を作ってくれるから、この関数は候補のペアだけ返す\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 4\")  # 4週分の履歴を使用\n",
    "\n",
    "    # 各週の販売数上位12個\n",
    "    dummy_count_df = df.groupby(['article_id', 'week'])['week'].count().rename('dummy_count').reset_index().copy()\n",
    "    dummy_count_df['rank_in_week'] = dummy_count_df.groupby('week')['dummy_count'].rank(method='min', ascending=False)\n",
    "    dummy_articles = dummy_count_df.query('rank_in_week <= 12')['article_id'].unique()\n",
    "\n",
    "    dummy_df = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [np.repeat(customers, repeats=len(dummy_articles)).reshape(-1, 1),\n",
    "            np.repeat(dummy_articles[None, :], repeats=len(customers), axis=0).reshape(-1, 1)],\n",
    "            axis=1),\n",
    "        columns = ['customer_id', 'article_id']\n",
    "    )\n",
    "    \n",
    "    dummy_df['isin_popular'] = 1\n",
    "    \n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_lastw(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''customer毎の最後の購入から1週間の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\").copy()\n",
    "\n",
    "    # 最後の購入から1週間に絞る\n",
    "    df['max_dat'] = df['customer_id'].map(df.groupby('customer_id')['t_dat'].max())\n",
    "    df['max_diff'] = (df['max_dat'] - df['t_dat']).dt.days\n",
    "    df = df.query('max_diff <= 6')\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count_lastw').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.sort_values('t_dat', ascending=True)\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    df = df.rename(columns={'t_dat': 'last_dat'})\n",
    "    df['last_dat'] = df['last_dat'].astype(np.int64) // 10 ** 9\n",
    "    \n",
    "    df['isin_lastw'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'count_lastw', 'last_dat', 'isin_lastw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_pairs(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''ルールベース協調フィルタリングから候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 12').copy()  # 12週分の履歴を使用\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    pairs = pd.read_parquet(f'../input/hmitempairs/pairs_fold{target_week}.parquet', dtype='int32')\n",
    "    df = df.merge(pairs, on='article_id', how='inner')\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'pair_article_id': 'article_id'})\n",
    "    df['count_pair'] = df['count'] * df['pair_ratio']\n",
    "\n",
    "    df['isin_pair'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'count_pair', 'isin_pair']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_collabo(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''コラボ商品の購入履歴から候補生成'''\n",
    "    df = pd.read_csv('../input/ranking_features/collabo_candidate.csv', dtypes={'customer_id': 'int32'})\n",
    "    # TODO: ほんとはスコープ外変数の参照は良くない\n",
    "    df['customer_id'] = df['customer_id'].map(id_to_index_dict)\n",
    "    df = df.query('target_week == @target_week')\n",
    "    \n",
    "    df['isin_collabo'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_customers_feature(customers: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''customer毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    customers_feature = customers.drop(['postal_code'], axis=1).copy()\n",
    "    customers_feature.loc[~customers_feature['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None  # Noneの表記揃え\n",
    "    customers_feature[['FN', 'Active']] = customers_feature[['FN', 'Active']].fillna(0)\n",
    "\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_purchase = df.groupby(['customer_id', 'week'])['week'].count().rename('purchase').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_purchase.groupby('customer_id')['purchase'].agg(agg_name)\n",
    "        customers_feature[f'purchase_{agg_name}_groupby_customer'] = customers_feature['customer_id'].map(agg_sr)\n",
    "    \n",
    "    # 各週の購入数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_purchase[weekly_purchase['week']==w]\n",
    "        tmp = tmp[['customer_id', 'purchase']].set_index('customer_id')['purchase']\n",
    "        customers_feature[f'purchase_{w}w'] = customers_feature['customer_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_ratio_{w}w'] = customers_feature[f'purchase_{w}w'] / customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_diff_{w}w'] = customers_feature[f'purchase_{w}w'] - customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "\n",
    "    # --- 一意の(article_id, week)を購入単位とみなす ---\n",
    "    # ※あるarticleを1個買うことを、ふつうは購入単位とみなしている\n",
    "    # rank: 何回目の購入か\n",
    "    unique_transactions = df[['customer_id', 'article_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['customer_id', 'article_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再購入したarticleの割合\n",
    "    customers_feature['repurchase_article'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article'] = customers_feature['customer_id'].map(unique_transactions.drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count())\n",
    "    customers_feature['repurchase_article_percent'] = customers_feature['repurchase_article'] / customers_feature['purchase_article']\n",
    "\n",
    "    # 再購入を含む週の割合\n",
    "    customers_feature['repurchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count()).fillna(0)\n",
    "    customers_feature['purchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count())\n",
    "    customers_feature['repurchase_week_percent'] = customers_feature['repurchase_week'] / customers_feature['purchase_week']\n",
    "    \n",
    "    # 再購入の割合\n",
    "    customers_feature['repurchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('customer_id')['customer_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.groupby('customer_id')['customer_id'].count())\n",
    "    customers_feature['repurchase_article_and_week_percent'] = customers_feature['repurchase_article_and_week'] / customers_feature['purchase_article_and_week']\n",
    "    # --- おわり ---\n",
    "    \n",
    "    return customers_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_articles_feature(articles: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''article毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    articles_feature = articles.drop(\n",
    "        ['prod_name', 'product_type_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'prod_name', 'department_name', 'detail_desc'], \n",
    "        axis=1).copy()\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_sale = df.groupby(['article_id', 'week'])['week'].count().rename('sale').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_sale.groupby('article_id')['sale'].agg(agg_name)\n",
    "        articles_feature[f'sale_{agg_name}_groupby_article'] = articles_feature['article_id'].map(agg_sr)\n",
    "\n",
    "    # 各週の販売数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_sale[weekly_sale['week']==w]\n",
    "        tmp = tmp[['article_id', 'sale']].set_index('article_id')['sale']\n",
    "        articles_feature[f'sale_{w}w'] = articles_feature['article_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_ratio_{w}w'] = articles_feature[f'sale_{w}w'] / articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_diff_{w}w'] = articles_feature[f'sale_{w}w'] - articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "\n",
    "    # --- 一意の(customer_id, week)を販売単位とみなす ---\n",
    "    # ※あるcustomerに1つ売れることを、ふつうは販売単位とみなしている\n",
    "    # rank: 何回目の販売か\n",
    "    unique_transactions = df[['article_id', 'customer_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['article_id', 'customer_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再販売したcustomerの割合\n",
    "    articles_feature['resale_customer'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer'] = articles_feature['article_id'].map(unique_transactions.drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count())\n",
    "    articles_feature['resale_customer_percent'] = articles_feature['resale_customer'] / articles_feature['sale_customer']\n",
    "\n",
    "    # 再販売を含む週の割合\n",
    "    articles_feature['resale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count()).fillna(0)\n",
    "    articles_feature['sale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count())\n",
    "    articles_feature['resale_week_percent'] = articles_feature['resale_week'] / articles_feature['sale_week']\n",
    "\n",
    "    # 再販売の割合\n",
    "    articles_feature['resale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('article_id')['article_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.groupby('article_id')['article_id'].count())\n",
    "    articles_feature['resale_customer_and_week_percent'] = articles_feature['resale_customer_and_week'] / articles_feature['sale_customer_and_week']\n",
    "    # --- 終わり ---\n",
    "    \n",
    "    return articles_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_df(transactions: pd.DataFrame, week: int, is_labeled: bool, use_customers: np.ndarray = None, metric_verbose: bool = True, compress_verbose: bool = False):\n",
    "    '''ランク学習モデルへの入力データ作成'''\n",
    "    strategy_flags = ['isin_trending', 'isin_recently', 'isin_popular', 'isin_lastw']\n",
    "    kwargs = {'how': 'outer', 'on': ['customer_id', 'article_id'], 'copy': True}\n",
    "    \n",
    "    # 入力データに含むcustomersの絞り込み\n",
    "    if use_customers is not None:\n",
    "        data_customers = use_customers\n",
    "    elif week >= 0:\n",
    "        data_customers = transactions.query(\"week == @week\")['customer_id'].unique()\n",
    "    else:\n",
    "        raise ValueError('set use_customers as something when week=-1.')\n",
    "    \n",
    "    # 候補作り\n",
    "    data_candidates_trending = generate_candidates_trending(transactions, data_customers, week)\n",
    "    data_candidates_recently = generate_candidates_recently(transactions, data_customers, week)\n",
    "    data_candidates_popular = generate_candidates_popular(transactions, data_customers, week)    \n",
    "    data_candidates_lastw = generate_candidates_lastw(transactions, data_customers, week)\n",
    "    # data_candidates_collabo = generate_candidates_collabo(transactions, data_customers, week)\n",
    "    data_df = data_candidates_trending.merge(\n",
    "        data_candidates_recently.merge(\n",
    "        data_candidates_popular.merge(\n",
    "        data_candidates_lastw, **kwargs), **kwargs), **kwargs)\n",
    "    data_df[strategy_flags] = data_df[strategy_flags].fillna(0)\n",
    "    \n",
    "    # 正解ラベル付け\n",
    "    if is_labeled:\n",
    "        if week < 0:\n",
    "            raise ValueError(f\"can't label when week={week}.\")\n",
    "        data_actual = transactions.query(\"week == @week\")[['customer_id', 'article_id']].drop_duplicates()\n",
    "        data_actual['label'] = 1\n",
    "        data_df = data_df.merge(data_actual, how='left', on=['customer_id', 'article_id'])\n",
    "        data_df['label'] = data_df['label'].fillna(0)\n",
    "    \n",
    "    # customers, articles の特徴量\n",
    "    data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    data_customers_feature = compress_df(make_customers_feature(customers, transactions, target_week=week, debug=True), verbose=compress_verbose)\n",
    "    data_articles_feature = compress_df(make_articles_feature(articles, transactions, target_week=week, debug=True), verbose=compress_verbose)\n",
    "    data_df = data_df.merge(data_customers_feature, how='left', on=['customer_id'], copy=True)\n",
    "    data_df = data_df.merge(data_articles_feature, how='left', on=['article_id'], copy=True)\n",
    "    # data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    \n",
    "    # 候補のスコア\n",
    "    if metric_verbose:\n",
    "        print(f\"[Info] data shape: {data_df.shape}\")\n",
    "        print(f\"[Info] candidates per a customer: {len(data_df) / len(data_customers):.1f}\")\n",
    "        display(data_df[strategy_flags].astype(float).mean())\n",
    "        if is_labeled:\n",
    "            print(f\"[Info] Precision: {data_df['label'].sum() / len(data_df):.5f}\")\n",
    "            print(f\"[Info] Recall: {data_df['label'].sum() / len(data_actual):.5f}\")\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_df(\n",
    "    df: pd.DataFrame, \n",
    "    category_columns: list =['club_member_status', 'fashion_news_frequency', 'product_group_name', 'index_code'], \n",
    "    verbose: bool =True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    '''DataFrameのデータ型を適切に選び圧縮'''\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        bar = tqdm(df.columns, leave=False)\n",
    "    else:\n",
    "        bar = df.columns\n",
    "    for col in bar:\n",
    "        col_type = df[col].dtypes\n",
    "        if col in category_columns:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(category)\")\n",
    "            df[col] = df[col].astype('category')\n",
    "        elif col_type in numerics:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(num)\")\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params: dict, cols: list, tr_df: pd.DataFrame, val_df: pd.DataFrame = None, early_stopping: bool = True):\n",
    "    if val_df is None:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dtrain], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    else:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "        if early_stopping:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.early_stopping(10, first_metric_only=True), lgb.log_evaluation(10)])\n",
    "        else:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_df: pd.DataFrame, model_folds: list):\n",
    "    pred = np.zeros(len(test_df))\n",
    "    for w in model_folds:\n",
    "        with open(f\"../models/lgb_rank/{EXP}_model_fold{w}.pkl\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        pred += model.predict(test_df[cols], num_iteration=model.best_iteration)    \n",
    "    pred = pred/len(model_folds)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_sr(test_df: pd.DataFrame, pred: np.ndarray):\n",
    "    '''モデルのスコアを用いて並び替え＆上位12個抽出'''\n",
    "    test_df['predict_score'] = pred\n",
    "    test_df = test_df.sort_values('predict_score', ascending=False).drop_duplicates(['customer_id', 'article_id'], keep='first').reset_index(drop=True)\n",
    "    test_df['rank'] = test_df.groupby('customer_id')['predict_score'].rank('min', ascending=False)\n",
    "    test_df = test_df[test_df['rank'] <= 12]\n",
    "    \n",
    "    # test_df['article_id'] = le.inverse_transform(test_df['article_id'])\n",
    "    test_df['article_id'] = ' 0' + test_df['article_id'].astype(str)\n",
    "    \n",
    "    top_sr = test_df.groupby('customer_id')['article_id'].sum()\n",
    "    \n",
    "    return top_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optunaによるチューニング\n",
    "# import optuna.integration.lightgbm as lgb_optuna\n",
    "# from optuna.logging import set_verbosity\n",
    "# import numpy as np\n",
    "# import random as rn\n",
    "\n",
    "# set_verbosity(-1)\n",
    "# np.random.seed(71)\n",
    "# rn.seed(71)\n",
    "\n",
    "# params = {\n",
    "#     'objective': 'lambdarank',\n",
    "#     'boosting': 'gbdt',  # default: 'gbdt', 'gbdt' or 'dart'\n",
    "#     'num_iterations': 1000,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'metric': ['ndcg'],\n",
    "#     'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "#     'random_state': 71,  # 訓練用とは違う値にする\n",
    "#     'verbosity': -1,  # -1: ignore, 0: warnings, 1: info\n",
    "#     'deterministic': True,  # 再現性確保\n",
    "#     'force_row_wise': True  # 再現性確保\n",
    "# }\n",
    "\n",
    "# tr_df = make_data_df(transactions, 2, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "# cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "# tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "# train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "\n",
    "# val_df = make_data_df(transactions, 1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "# val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "\n",
    "# model = lgb_optuna.LightGBMTuner(params, dtrain, valid_sets=[dtrain, dval], early_stopping_rounds=10, verbose_eval=-1, optuna_seed=71)\n",
    "# model.run()\n",
    "\n",
    "# best_params = model.params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "params = {\n",
    "    # Core Parameters\n",
    "    'objective': 'lambdarank',\n",
    "    'boosting': 'gbdt',  # default: 'gbdt', ['gbdt', 'dart', 'goss'], dart: 超遅いけど高精度\n",
    "    'num_iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,  # default: 31, large for accuracy\n",
    "    'num_threads': cpu_count(logical=False),\n",
    "    'random_state': 41,\n",
    "\n",
    "    # Learning Control Parameters\n",
    "    'force_col_wise'=True,\n",
    "    'min_data_in_leaf': 20,  # default: 20, large dealing with over-fitting\n",
    "    'min_sum_hessian_in_leaf': 1e-3,  # default: 1e-3, large dealing with over-fitting\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    # 'drop_rate': 0.1,  # used only in dart, a fraction of previous trees to drop during the dropout\n",
    "    'verbosity': 0  # 0: warnings, 1: info\n",
    "\n",
    "    # Dataset Parameters\n",
    "    'max_bin': 255,  # default: 255, large for accuracy\n",
    "\n",
    "    # Objective Parameters\n",
    "    'lambdarank_truncation_level': 30,\n",
    "    'lambdarank_norm': True,\n",
    "    # 'label_gain': [0, 1],\n",
    "\n",
    "    # Metric Parameters\n",
    "    'metric': ['ndcg'],\n",
    "    'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_week(fold): 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 245.65 Mb (67.4% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 614.96 Mb (74.9% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 48.62 Mb (75.0% reduction)\n",
      "[Info] data shape: (4292962, 497)\n",
      "[Info] candidates per a customer: 59.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.268328\n",
       "isin_recently    0.138038\n",
       "isin_popular     0.704751\n",
       "isin_lastw       0.060702\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00375\n",
      "[Info] Recall: 0.06973\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 10.676796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.888299\tvalid_1's ndcg@12: 0.890095\n",
      "[20]\ttraining's ndcg@12: 0.890325\tvalid_1's ndcg@12: 0.891354\n",
      "[30]\ttraining's ndcg@12: 0.89195\tvalid_1's ndcg@12: 0.8912\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's ndcg@12: 0.891302\tvalid_1's ndcg@12: 0.891532\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 3\n",
      "[Info] data shape: (4935164, 497)\n",
      "[Info] candidates per a customer: 61.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.257627\n",
       "isin_recently    0.126501\n",
       "isin_popular     0.715504\n",
       "isin_lastw       0.057123\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00326\n",
      "[Info] Recall: 0.06302\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.420481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.899397\tvalid_1's ndcg@12: 0.888397\n",
      "[20]\ttraining's ndcg@12: 0.901469\tvalid_1's ndcg@12: 0.889507\n",
      "[30]\ttraining's ndcg@12: 0.903034\tvalid_1's ndcg@12: 0.889791\n",
      "[40]\ttraining's ndcg@12: 0.904829\tvalid_1's ndcg@12: 0.889608\n",
      "[50]\ttraining's ndcg@12: 0.906116\tvalid_1's ndcg@12: 0.889843\n",
      "[60]\ttraining's ndcg@12: 0.907391\tvalid_1's ndcg@12: 0.889879\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's ndcg@12: 0.906995\tvalid_1's ndcg@12: 0.889913\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 2\n",
      "[Info] data shape: (4256778, 497)\n",
      "[Info] candidates per a customer: 56.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.265981\n",
       "isin_recently    0.138183\n",
       "isin_popular     0.694670\n",
       "isin_lastw       0.061846\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00386\n",
      "[Info] Recall: 0.06909\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 11.624155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.896861\tvalid_1's ndcg@12: 0.892511\n",
      "[20]\ttraining's ndcg@12: 0.898792\tvalid_1's ndcg@12: 0.892859\n",
      "[30]\ttraining's ndcg@12: 0.900729\tvalid_1's ndcg@12: 0.893345\n",
      "[40]\ttraining's ndcg@12: 0.902203\tvalid_1's ndcg@12: 0.89346\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's ndcg@12: 0.902098\tvalid_1's ndcg@12: 0.893488\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 1\n",
      "[Info] data shape: (3465389, 497)\n",
      "[Info] candidates per a customer: 48.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.305411\n",
       "isin_recently    0.163816\n",
       "isin_popular     0.644254\n",
       "isin_lastw       0.072046\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00438\n",
      "[Info] Recall: 0.06655\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.974432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.898621\tvalid_1's ndcg@12: 0.871333\n",
      "[20]\ttraining's ndcg@12: 0.900807\tvalid_1's ndcg@12: 0.872061\n",
      "[30]\ttraining's ndcg@12: 0.902772\tvalid_1's ndcg@12: 0.87299\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's ndcg@12: 0.902422\tvalid_1's ndcg@12: 0.873328\n",
      "Evaluated only: ndcg@12\n"
     ]
    }
   ],
   "source": [
    "# train lgb ranker\n",
    "best_iterations = []\n",
    "feature_importance_dfs = []\n",
    "oof_weeks = [4, 3, 2, 1]\n",
    "\n",
    "for i, w in enumerate(tqdm(oof_weeks)):\n",
    "    print(f\"\\ntarget_week(fold): {w}\")\n",
    "    if i == 0:\n",
    "        compress_verbose=True\n",
    "    else:\n",
    "        compress_verbose=False\n",
    "    \n",
    "    tr_df = make_data_df(transactions, w, is_labeled=True, metric_verbose=True, compress_verbose=compress_verbose)\n",
    "    val_df = make_data_df(transactions, w-1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    if i == 0:\n",
    "        exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "        cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "        with open(f'../models/lgb_rank/{EXP}_cols.pkl', 'wb') as f:\n",
    "            pickle.dump(cols, f)\n",
    "\n",
    "    model = train(params, cols, tr_df, val_df)\n",
    "    with open(f'../models/lgb_rank/{EXP}_model_fold{w}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    best_iterations.append(model.best_iteration)\n",
    "    feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': w}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] data shape: (3297798, 496)\n",
      "[Info] candidates per a customer: 47.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.280896\n",
       "isin_recently    0.150857\n",
       "isin_popular     0.669382\n",
       "isin_lastw       0.070936\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.46390671 -2.45062473 -2.45062473 ...  2.33291388  2.38539609\n",
      "  2.38539609]\n"
     ]
    }
   ],
   "source": [
    "# predict val data\n",
    "val_df = make_data_df(transactions, 0, is_labeled=False, metric_verbose=True, compress_verbose=False)\n",
    "val_pred = predict(val_df, oof_weeks)\n",
    "print(np.sort(val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "80      0671607001 0436261001 0568601007 0448509014 0...\n",
       "86      0621381012 0889036004 0640021012 0880017001 0...\n",
       "107     0556255001 0732842014 0399136061 0732842021 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val top rank articles\n",
    "val_df2 = val_df.copy()\n",
    "val_pred_sr = extract_top_sr(val_df2, val_pred)\n",
    "display(val_pred_sr.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 popular items:\n",
      " 0909370001 0865799006 0918522001 0924243001 0448509014 0751471001 0809238001 0918292001 0762846027 0809238005 0673677002 0923758001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0     0685814003 0448509014 0918522001 0715624001 0...\n",
       "1.0     0909370001 0865799006 0924243001 0809238001 0...\n",
       "2.0     0909370001 0865799006 0918525001 0909371001 0...\n",
       "3.0     0909370001 0751471001 0673677002 0910601003 0...\n",
       "4.0     0918522001 0751471001 0751471043 0910601003 0...\n",
       "5.0     0918522001 0908799002 0896152002 0924243001 0...\n",
       "6.0     0736870001 0796210001 0908799002 0865799006 0...\n",
       "Name: top_12_popular_items, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 1]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "display(popular_items_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_lgb</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>prediction_popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 07...</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "\n",
       "                                          prediction prediction_lgb  age_bin  \\\n",
       "0  0909370001 0751471001 0673677002 0910601003 07...                     3.0   \n",
       "1  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "2  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "\n",
       "                                  prediction_popular  \n",
       "0   0909370001 0751471001 0673677002 0910601003 0...  \n",
       "1   0909370001 0865799006 0924243001 0809238001 0...  \n",
       "2   0909370001 0865799006 0924243001 0809238001 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(val_pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission_fold0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9398"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del val_df, model\n",
    "del transactions_last_week, top12, popular_items, popular_items_dict, popular_items_sr\n",
    "del val_df2, val_pred_sr, \n",
    "del submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] data shape: (3297798, 498)\n",
      "[Info] candidates per a customer: 47.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.280896\n",
       "isin_recently    0.150857\n",
       "isin_popular     0.669382\n",
       "isin_lastw       0.070936\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00497\n",
      "[Info] Recall: 0.07673\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.359769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[10]\ttraining's ndcg@12: 0.882872\n",
      "[20]\ttraining's ndcg@12: 0.885015\n",
      "[30]\ttraining's ndcg@12: 0.887278\n"
     ]
    }
   ],
   "source": [
    "# train last target_week(=0) data\n",
    "tr_df = make_data_df(transactions, week=0, is_labeled=True, metric_verbose=True, compress_verbose=False)\n",
    "\n",
    "# valがないのでアーリーストッピングが使えない\n",
    "params['num_iterations'] = int(np.mean(best_iterations))\n",
    "model = train(params, cols, tr_df)\n",
    "with open(f\"../models/lgb_rank/{EXP}_model_fold0.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance(gain)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trending_value</th>\n",
       "      <td>26368.648350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_max_groupby_article_ratio_1w</th>\n",
       "      <td>11135.143252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_dat</th>\n",
       "      <td>10252.860617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_max_groupby_customer_ratio_1w</th>\n",
       "      <td>4222.934516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_sum_groupby_customer_ratio_1w</th>\n",
       "      <td>4035.318266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_lastw</th>\n",
       "      <td>3694.982594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_mean_groupby_customer_ratio_1w</th>\n",
       "      <td>2670.511423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_1w</th>\n",
       "      <td>2427.720008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_group_name</th>\n",
       "      <td>2117.508558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>2097.721853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_mean_groupby_article_diff_1w</th>\n",
       "      <td>1851.569324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_min_groupby_article_diff_1w</th>\n",
       "      <td>1810.619224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resale_customer_and_week_percent</th>\n",
       "      <td>1804.871950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_mean_groupby_article_ratio_1w</th>\n",
       "      <td>1798.109354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_1w</th>\n",
       "      <td>1664.611891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resale_customer_percent</th>\n",
       "      <td>1632.151441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_code</th>\n",
       "      <td>1489.781799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_article</th>\n",
       "      <td>1444.574720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_max_groupby_article_ratio_2w</th>\n",
       "      <td>882.253519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_type_no</th>\n",
       "      <td>707.116057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         importance(gain)\n",
       "feature                                                  \n",
       "trending_value                               26368.648350\n",
       "sale_max_groupby_article_ratio_1w            11135.143252\n",
       "last_dat                                     10252.860617\n",
       "purchase_max_groupby_customer_ratio_1w        4222.934516\n",
       "purchase_sum_groupby_customer_ratio_1w        4035.318266\n",
       "count_lastw                                   3694.982594\n",
       "purchase_mean_groupby_customer_ratio_1w       2670.511423\n",
       "sale_1w                                       2427.720008\n",
       "product_group_name                            2117.508558\n",
       "age                                           2097.721853\n",
       "sale_mean_groupby_article_diff_1w             1851.569324\n",
       "sale_min_groupby_article_diff_1w              1810.619224\n",
       "resale_customer_and_week_percent              1804.871950\n",
       "sale_mean_groupby_article_ratio_1w            1798.109354\n",
       "purchase_1w                                   1664.611891\n",
       "resale_customer_percent                       1632.151441\n",
       "product_code                                  1489.781799\n",
       "purchase_article                              1444.574720\n",
       "sale_max_groupby_article_ratio_2w              882.253519\n",
       "product_type_no                                707.116057"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance_df = pd.concat(feature_importance_dfs, ignore_index=True, axis=0)\n",
    "display(feature_importance_df.groupby(['feature'])[['importance(gain)']].mean().sort_values('importance(gain)', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5629"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tr_df, params, model, best_iterations, feature_importance_dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "0     0568601043 0568601006 0866731001 0924243002 0...\n",
       "1     0866731001 0924243001 0915529005 0714790020 0...\n",
       "2     0794321007 0924243001 0866731001 0924243002 0...\n",
       "3     0866731001 0915529005 0924243002 0924243001 0...\n",
       "4     0730683050 0791587015 0896152002 0818320001 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict test data\n",
    "BATCH_SIZE = 100_000\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "test_customers = submission['customer_id'].map(id_to_index_dict).unique()\n",
    "\n",
    "# バッチ処理\n",
    "def process(i):\n",
    "    if i == (len(test_customers)//BATCH_SIZE):\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : ]\n",
    "    else:\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
    "\n",
    "    test_df = make_data_df(transactions, week=-1, is_labeled=False, use_customers=test_customers_batch, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    all_weeks = oof_weeks + [0]\n",
    "    pred = predict(test_df, all_weeks)\n",
    "    return extract_top_sr(test_df, pred)\n",
    "\n",
    "# single process execution\n",
    "preds = []\n",
    "for i in tqdm(range(len(test_customers)//BATCH_SIZE + 1)):\n",
    "    preds.append(process(i))\n",
    "\n",
    "# # multi process execution\n",
    "# # cpus = cpu_count(logical=False)\n",
    "# cpus = 4\n",
    "# print('cpu(core): ', cpus)\n",
    "# preds = Parallel(n_jobs=cpus, verbose=0)( [delayed(process)(i) for i in range(len(test_customers)//BATCH_SIZE + 1)] )\n",
    "\n",
    "pred_sr = pd.concat(preds, axis=0)\n",
    "display(pred_sr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6503"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # most popular items\n",
    "# transactions_last_week = transactions.loc[transactions.week == 0]\n",
    "# top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "# print(\"Top 12 popular items:\")\n",
    "# print( top12 )\n",
    "\n",
    "# customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "# transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "# popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "# popular_items_dict = {}\n",
    "# for index in popular_items.index.levels[0]:\n",
    "#     popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "# popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "# popular_items_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0568601043 0568601006 0866731001 0924243002 09...</td>\n",
       "      <td>0568601043 0568601006 0866731001 0924243002 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0866731001 0924243001 0915529005 0714790020 09...</td>\n",
       "      <td>0866731001 0924243001 0915529005 0714790020 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0794321007 0924243001 0866731001 0924243002 07...</td>\n",
       "      <td>0794321007 0924243001 0866731001 0924243002 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "\n",
       "                                          prediction  \\\n",
       "0  0568601043 0568601006 0866731001 0924243002 09...   \n",
       "1  0866731001 0924243001 0915529005 0714790020 09...   \n",
       "2  0794321007 0924243001 0866731001 0924243002 07...   \n",
       "\n",
       "                                      prediction_lgb  \n",
       "0   0568601043 0568601006 0866731001 0924243002 0...  \n",
       "1   0866731001 0924243001 0915529005 0714790020 0...  \n",
       "2   0794321007 0924243001 0866731001 0924243002 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "# submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "# submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "# submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f804528fa1bcda80b2057737773baa2134c5be7e013153f88e69abab752260b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
