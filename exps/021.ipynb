{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "# 学習データ5週分(検証データ1週含む)\n",
    "# 候補作り12週分\n",
    "# trendingで候補作り\n",
    "# articlesとcustomersの特徴量をtarget_weekごとに作る\n",
    "# 候補の良さを測る\n",
    "EXP = '021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = None\n",
    "warnings.simplefilter('ignore', pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "data_path = Path('../input/h-and-m-personalized-fashion-recommendations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>t_diff</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31788319</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>929511001</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788320</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>891322004</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788321</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371721</td>\n",
       "      <td>918325001</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788322</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371747</td>\n",
       "      <td>833459002</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788323</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371960</td>\n",
       "      <td>898573003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_dat  customer_id  article_id     price  sales_channel_id  \\\n",
       "31788319 2020-09-22      1371691   929511001  0.059305                 2   \n",
       "31788320 2020-09-22      1371691   891322004  0.042356                 2   \n",
       "31788321 2020-09-22      1371721   918325001  0.043203                 1   \n",
       "31788322 2020-09-22      1371747   833459002  0.006763                 1   \n",
       "31788323 2020-09-22      1371960   898573003  0.033881                 2   \n",
       "\n",
       "          t_diff  week  \n",
       "31788319       0     0  \n",
       "31788320       0     0  \n",
       "31788321       0     0  \n",
       "31788322       0     0  \n",
       "31788323       0     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transactions = pd.read_csv(\n",
    "    data_path / f'transactions_train.csv',\n",
    "    # set dtype or pandas will drop the leading '0' and convert to int\n",
    "    dtype={'article_id': 'int32'},\n",
    "    parse_dates=['t_dat'])\n",
    "customers = pd.read_csv(data_path / 'customers.csv')\n",
    "articles = pd.read_csv(\n",
    "    '../input/h-and-m-personalized-fashion-recommendations/articles.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "\n",
    "t_max = transactions['t_dat'].max()\n",
    "transactions['t_diff'] = (t_max - transactions['t_dat']).dt.days\n",
    "transactions['week'] = transactions['t_diff'] // 7\n",
    "\n",
    "customers.loc[~customers['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "\n",
    "id_to_index_dict = dict(zip(customers[\"customer_id\"], customers.index))\n",
    "index_to_id_dict = dict(zip(customers.index, customers[\"customer_id\"]))\n",
    "transactions[\"customer_id\"] = transactions[\"customer_id\"].map(id_to_index_dict).astype('int32')\n",
    "customers['customer_id'] = customers['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "\n",
    "print(transactions.shape)\n",
    "display(transactions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_trending(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    weekly_sales = df.groupby(['week', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    df = df.merge(weekly_sales, on=['week', 'article_id'], how='left')\n",
    "    \n",
    "    weekly_sales = weekly_sales.reset_index().set_index('article_id')\n",
    "    df = df.join(weekly_sales.loc[weekly_sales['week']==1, ['count']], on='article_id', rsuffix='_targ')\n",
    "    df['count_targ'].fillna(0, inplace=True)\n",
    "    df['quotient'] = df['count_targ'] / df['count']\n",
    "    \n",
    "    t_max = df['t_dat'].max()\n",
    "    df['x'] = ((t_max - df['t_dat']) / np.timedelta64(1, 'D')).astype(int)\n",
    "    df['dummy_1'] = 1\n",
    "    df['x'] = df[['x', 'dummy_1']].max(axis=1)    \n",
    "    a, b, c, d = 2.5e4, 1.5e5, 2e-1, 1e3\n",
    "    df['y'] = a / np.sqrt(df['x']) + b * np.exp(-c*df['x']) - d\n",
    "    df['dummy_0'] = 0 \n",
    "    df['y'] = df[[\"y\", \"dummy_0\"]].max(axis=1)\n",
    "    df['trending_value'] = df['quotient'] * df['y']\n",
    "    df = df.groupby(['customer_id', 'article_id']).agg({'trending_value': 'sum'}).reset_index()\n",
    "    df = df.loc[df['trending_value'] > 0]\n",
    "    # df['rank'] = df.groupby(\"customer_id\")[\"value\"].rank(\"dense\", ascending=False)\n",
    "    # df = df.loc[df['rank'] <= 12]\n",
    "    df = df[['customer_id', 'article_id', 'trending_value']]\n",
    "    df['isin_trending'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_recently(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 12\")\n",
    "    \n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = df.query('week == @w').groupby(['customer_id', 'article_id'])['article_id'].count().rename(f'count_{w}w').reset_index().copy()\n",
    "        if w == 1:\n",
    "            purchase_df = tmp\n",
    "            continue\n",
    "        purchase_df = purchase_df.merge(tmp, how='outer', on=['customer_id', 'article_id'])\n",
    "        \n",
    "    purchase_df['isin_recently'] = 1\n",
    "    \n",
    "    return purchase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_popular(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''\n",
    "    make_customers_featureが購入数から特徴量を作ってくれるから、この関数は候補のペアだけ返す\n",
    "    '''\n",
    "    df = transactions.copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 4\")\n",
    "\n",
    "    dummy_count_df = df.groupby(['article_id', 'week'])['week'].count().rename('dummy_count').reset_index().copy()\n",
    "    dummy_count_df['rank_in_week'] = dummy_count_df.groupby('week')['dummy_count'].rank(method='min', ascending=False)\n",
    "    dummy_articles = dummy_count_df.query('rank_in_week <= 12')['article_id'].unique()\n",
    "\n",
    "    dummy_df = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [np.repeat(customers, repeats=len(dummy_articles)).reshape(-1, 1),\n",
    "            np.repeat(dummy_articles[None, :], repeats=len(customers), axis=0).reshape(-1, 1)],\n",
    "            axis=1),\n",
    "        columns = ['customer_id', 'article_id']\n",
    "    )\n",
    "    \n",
    "    dummy_df['isin_popular'] = 1\n",
    "    \n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_customers_feature(customers: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    df = transactions.copy()\n",
    "    customers_feature = customers.drop(['postal_code'], axis=1).copy()\n",
    "    customers_feature.loc[~customers_feature['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "    customers_feature[['FN', 'Active']] = customers_feature[['FN', 'Active']].fillna(0)\n",
    "\n",
    "    # リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_purchase = df.groupby(['customer_id', 'week'])['week'].count().rename('purchase').reset_index()\n",
    "    \n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_purchase.groupby('customer_id')['purchase'].agg(agg_name)\n",
    "        customers_feature[f'purchase_{agg_name}_groupby_customer'] = customers_feature['customer_id'].map(agg_sr)\n",
    "    \n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_purchase[weekly_purchase['week']==w]\n",
    "        tmp = tmp[['customer_id', 'purchase']].set_index('customer_id')['purchase']\n",
    "        customers_feature[f'purchase_{w}w'] = customers_feature['customer_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_ratio_{w}w'] = customers_feature[f'purchase_{w}w'] / customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_diff_{w}w'] = customers_feature[f'purchase_{w}w'] - customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "\n",
    "    unique_transactions = df[['customer_id', 'article_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['customer_id', 'article_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    customers_feature['repurchase_article'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article'] = customers_feature['customer_id'].map(unique_transactions.drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count())\n",
    "    customers_feature['repurchase_article_percent'] = customers_feature['repurchase_article'] / customers_feature['purchase_article']\n",
    "\n",
    "    customers_feature['repurchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count()).fillna(0)\n",
    "    customers_feature['purchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count())\n",
    "    customers_feature['repurchase_week_percent'] = customers_feature['repurchase_week'] / customers_feature['purchase_week']\n",
    "\n",
    "    customers_feature['repurchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('customer_id')['customer_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.groupby('customer_id')['customer_id'].count())\n",
    "    customers_feature['repurchase_article_and_week_percent'] = customers_feature['repurchase_article_and_week'] / customers_feature['purchase_article_and_week']\n",
    "        \n",
    "    return customers_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_articles_feature(articles: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    df = transactions.copy()\n",
    "    articles_feature = articles.drop(\n",
    "        ['prod_name', 'product_type_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'prod_name', 'department_name', 'detail_desc'], \n",
    "        axis=1).copy()\n",
    "    \n",
    "    # リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_sale = df.groupby(['article_id', 'week'])['week'].count().rename('sale').reset_index()\n",
    "    \n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_sale.groupby('article_id')['sale'].agg(agg_name)\n",
    "        articles_feature[f'sale_{agg_name}_groupby_article'] = articles_feature['article_id'].map(agg_sr)\n",
    "    \n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_sale[weekly_sale['week']==w]\n",
    "        tmp = tmp[['article_id', 'sale']].set_index('article_id')['sale']\n",
    "        articles_feature[f'sale_{w}w'] = articles_feature['article_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_ratio_{w}w'] = articles_feature[f'sale_{w}w'] / articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_diff_{w}w'] = articles_feature[f'sale_{w}w'] - articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "\n",
    "    unique_transactions = df[['article_id', 'customer_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['article_id', 'customer_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    articles_feature['resale_customer'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer'] = articles_feature['article_id'].map(unique_transactions.drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count())\n",
    "    articles_feature['resale_customer_percent'] = articles_feature['resale_customer'] / articles_feature['sale_customer']\n",
    "\n",
    "    articles_feature['resale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count()).fillna(0)\n",
    "    articles_feature['sale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count())\n",
    "    articles_feature['resale_week_percent'] = articles_feature['resale_week'] / articles_feature['sale_week']\n",
    "\n",
    "    articles_feature['resale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('article_id')['article_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.groupby('article_id')['article_id'].count())\n",
    "    articles_feature['resale_customer_and_week_percent'] = articles_feature['resale_customer_and_week'] / articles_feature['sale_customer_and_week']\n",
    "    \n",
    "    return articles_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_df(\n",
    "    df: pd.DataFrame, \n",
    "    category_columns: list =['club_member_status', 'fashion_news_frequency', 'product_group_name', 'index_code'], \n",
    "    verbose: bool =True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        bar = tqdm(df.columns, leave=False)\n",
    "    else:\n",
    "        bar = df.columns\n",
    "    for col in bar:\n",
    "        col_type = df[col].dtypes\n",
    "        if col in category_columns:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(category)\")\n",
    "            df[col] = df[col].astype('category')\n",
    "        elif col_type in numerics:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(num)\")\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'num_threads': 4,  # for M1 Mac\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'metric': ['ndcg'],\n",
    "    'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "    'random_state': 41,\n",
    "    'verbosity': 1  # 0: warnings, 1: info\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_weeks = [4, 3, 2, 1]\n",
    "strategy_flags = ['isin_trending', 'isin_recently', 'isin_popular']\n",
    "kwargs = {'how': 'outer', 'on': ['customer_id', 'article_id'], 'copy': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f795c55d245c4fcd9a763abb2b66654b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_week(fold): 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 215.71 Mb (68.5% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 614.96 Mb (74.9% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 48.62 Mb (75.0% reduction)\n",
      "\n",
      "target_week(fold): 3\n",
      "\n",
      "target_week(fold): 2\n",
      "\n",
      "target_week(fold): 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0523ebec704ee096c88c3199dfced0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train lgb ranker\n",
    "tr_dfs = []\n",
    "feature_importance_dfs = []\n",
    "for i, w in enumerate(tqdm(oof_weeks)):\n",
    "    print(f\"\\ntarget_week(fold): {w}\")\n",
    "    if i == 0:\n",
    "        verbose=True\n",
    "    else:\n",
    "        verbose=False\n",
    "    \n",
    "    tr_actual = transactions.query(\"week == @w\")[['customer_id', 'article_id']].drop_duplicates()\n",
    "    tr_actual['label'] = 1\n",
    "    tr_customers = transactions.query(\"week == @w\")['customer_id'].unique()\n",
    "    tr_candidates_trending = generate_candidates_trending(transactions, tr_customers, w)\n",
    "    tr_candidates_recently = generate_candidates_recently(transactions, tr_customers, w)\n",
    "    tr_candidates_popular = generate_candidates_popular(transactions, tr_customers, w)    \n",
    "    tr_df = tr_candidates_trending.merge(tr_candidates_recently.merge(tr_candidates_popular, **kwargs), **kwargs)\n",
    "    tr_df[strategy_flags] = tr_df[strategy_flags].fillna(0)\n",
    "    tr_df = tr_df.merge(tr_actual, how='left', on=['customer_id', 'article_id'])\n",
    "    tr_df['label'] = tr_df['label'].fillna(0)\n",
    "    tr_df['target_week'] = w\n",
    "    tr_df = compress_df(tr_df, verbose=verbose)\n",
    "    tr_customers_feature = compress_df(make_customers_feature(customers, transactions, target_week=w, debug=True), verbose=verbose)\n",
    "    tr_articles_feature = compress_df(make_articles_feature(articles, transactions, target_week=w, debug=True), verbose=verbose)\n",
    "    tr_df = tr_df.merge(tr_customers_feature, how='left', on=['customer_id'], copy=False)\n",
    "    tr_df = tr_df.merge(tr_articles_feature, how='left', on=['article_id'], copy=False)\n",
    "    tr_dfs.append(tr_df)\n",
    "\n",
    "tr_df = pd.concat(tr_dfs, axis=0)\n",
    "tr_df = compress_df(tr_df, verbose=True)\n",
    "print(f\"[Info] candidates per a customer: {tr_df.groupby('target_week', 'customer_id')['customer_id'].count().mean():.1f}\")\n",
    "display(tr_df[strategy_flags].astype(float).mean())\n",
    "print(f\"[Info] Precision: {tr_df['label'].astype(float).mean():.5f}\")\n",
    "print(f\"[Info] Recall(fold1): {tr_df.query('target_week == 1')['label'].sum() / len(tr_actual):.5f}\")\n",
    "\n",
    "val_actual = transactions.query(\"week == 0\")[['customer_id', 'article_id']].drop_duplicates()\n",
    "val_actual['label'] = 1\n",
    "val_customers = transactions.query(\"week == 0\")['customer_id'].unique()\n",
    "val_candidates_trending = generate_candidates_trending(transactions, val_customers, 0)\n",
    "val_candidates_recently = generate_candidates_recently(transactions, val_customers, 0)\n",
    "val_candidates_popular = generate_candidates_popular(transactions, val_customers, 0)\n",
    "val_df = val_candidates_trending.merge(val_candidates_recently.merge(val_candidates_popular, **kwargs), **kwargs)\n",
    "val_df[strategy_flags] = val_df[strategy_flags].fillna(0)\n",
    "val_df = val_df.merge(val_actual, how='left', on=['customer_id', 'article_id'])\n",
    "val_df['label'] = val_df['label'].fillna(0)\n",
    "val_df = compress_df(val_df, verbose=False)\n",
    "val_customers_feature = compress_df(make_customers_feature(customers, transactions, target_week=0, debug=True), verbose=False)\n",
    "val_articles_feature = compress_df(make_articles_feature(articles, transactions, target_week=0, debug=True), verbose=False)\n",
    "val_df = val_df.merge(val_customers_feature, how='left', on=['customer_id'], copy=False)\n",
    "val_df = val_df.merge(val_articles_feature, how='left', on=['article_id'], copy=False)\n",
    "\n",
    "exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "with open(f'../models/lgb_rank/{EXP}_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(cols, f)\n",
    "\n",
    "tr_df = tr_df.sort_values(['target_week', 'customer_id']).reset_index(drop=True)\n",
    "train_query = tr_df.groupby(['target_week', 'customer_id'])['customer_id'].count().to_list()\n",
    "dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain, valid_sets=[dtrain, dval], \n",
    "    callbacks=[lgb.early_stopping(10, first_metric_only=True), lgb.log_evaluation(10)])\n",
    "with open(f'../models/lgb_rank/{EXP}_model_fold{w}.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': w}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0c8cf5e024e92a1fbb4703c26e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.72599849 -0.72573982 -0.72318887 ...  0.62941608  0.62941608\n",
      "  0.63092939]\n"
     ]
    }
   ],
   "source": [
    "val_pred = np.zeros(len(val_df))\n",
    "with tqdm(oof_weeks) as pbar:\n",
    "    for w in pbar:\n",
    "        pbar.set_description(f\"model's target_week(fold): {w}\")\n",
    "        with open(f\"../models/lgb_rank/{EXP}_model_fold{w}.pkl\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        val_pred += model.predict(val_df[cols], num_iteration=model.best_iteration)\n",
    "val_pred = val_pred/len(oof_weeks)\n",
    "print(np.sort(val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 popular items:\n",
      " 0909370001 0865799006 0918522001 0924243001 0448509014 0751471001 0809238001 0918292001 0762846027 0809238005 0673677002 0923758001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0     0685814003 0448509014 0918522001 0715624001 0...\n",
       "1.0     0909370001 0865799006 0924243001 0809238001 0...\n",
       "2.0     0909370001 0865799006 0918525001 0909371001 0...\n",
       "3.0     0909370001 0751471001 0673677002 0910601003 0...\n",
       "4.0     0918522001 0751471001 0751471043 0910601003 0...\n",
       "5.0     0918522001 0908799002 0896152002 0924243001 0...\n",
       "6.0     0736870001 0796210001 0908799002 0865799006 0...\n",
       "Name: top_12_popular_items, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 1]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "display(popular_items_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "80      0671607001 0436261001 0918292001 0448509014 0...\n",
       "86      0913367001 0905914002 0904026001 0920012001 0...\n",
       "107     0706016001 0372860068 0706016002 0762846031 0...\n",
       "117     0448509014 0918292001 0866731001 0915529003 0...\n",
       "179     0806388001 0776179001 0448509014 0806388002 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict val data\n",
    "val_df2 = val_df.copy()\n",
    "val_df2['predict_score'] = val_pred\n",
    "val_df2 = val_df2.sort_values('predict_score', ascending=False).drop_duplicates(['customer_id', 'article_id'], keep='first').reset_index(drop=True)\n",
    "val_df2['rank'] = val_df2.groupby('customer_id')['predict_score'].rank('dense', ascending=False)\n",
    "val_df2 = val_df2[val_df2['rank'] <= 12]\n",
    "# val_df2['article_id'] = le.inverse_transform(val_df2['article_id'])\n",
    "val_df2['article_id'] = ' 0' + val_df2['article_id'].astype(str)\n",
    "val_pred_sr = val_df2.groupby('customer_id')['article_id'].sum()\n",
    "display(val_pred_sr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_lgb</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>prediction_popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 07...</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>0918522001 0751471001 0751471043 0910601003 09...</td>\n",
       "      <td></td>\n",
       "      <td>4.0</td>\n",
       "      <td>0918522001 0751471001 0751471043 0910601003 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>0918522001 0751471001 0751471043 0910601003 09...</td>\n",
       "      <td></td>\n",
       "      <td>4.0</td>\n",
       "      <td>0918522001 0751471001 0751471043 0910601003 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
       "\n",
       "                                          prediction prediction_lgb  age_bin  \\\n",
       "0  0909370001 0751471001 0673677002 0910601003 07...                     3.0   \n",
       "1  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "2  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "3  0918522001 0751471001 0751471043 0910601003 09...                     4.0   \n",
       "4  0918522001 0751471001 0751471043 0910601003 09...                     4.0   \n",
       "\n",
       "                                  prediction_popular  \n",
       "0   0909370001 0751471001 0673677002 0910601003 0...  \n",
       "1   0909370001 0865799006 0924243001 0809238001 0...  \n",
       "2   0909370001 0865799006 0924243001 0809238001 0...  \n",
       "3   0918522001 0751471001 0751471043 0910601003 0...  \n",
       "4   0918522001 0751471001 0751471043 0910601003 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(val_pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head())\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission_fold1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_actual, val_customers, val_candidates_trending, val_candidates_recently, val_candidates_popular, val_df, val_customers_feature, val_articles_feature\n",
    "del val_query, dval, val_pred\n",
    "del transactions_last_week, top12, popular_items, popular_items_dict, popular_items_sr\n",
    "del val_df2, val_pred_sr, submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] candidates per a customer: 38.5\n",
      "[Info] Precision: 0.003\n",
      "[Info] Recall: 0.042\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tr_purchase_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Info] Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(tr_actual)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m tr_df \u001b[38;5;241m=\u001b[39m tr_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m train_query \u001b[38;5;241m=\u001b[39m \u001b[43mtr_purchase_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m     25\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(tr_purchase_df[cols], label\u001b[38;5;241m=\u001b[39mtr_purchase_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], group\u001b[38;5;241m=\u001b[39mtrain_query)\n\u001b[1;32m     27\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tr_purchase_df' is not defined"
     ]
    }
   ],
   "source": [
    "# train last target_week data\n",
    "tr_actual = transactions.query(\"week == 0\")[['customer_id', 'article_id']].drop_duplicates()\n",
    "tr_actual['label'] = 1\n",
    "tr_customers = transactions.query(\"week == 0\")['customer_id'].unique()\n",
    "tr_candidates_trending = generate_candidates_trending(transactions, tr_customers, 0)\n",
    "tr_candidates_recently = generate_candidates_recently(transactions, tr_customers, 0)\n",
    "tr_candidates_popular = generate_candidates_popular(transactions, tr_customers, 0)\n",
    "tr_df = tr_candidates_trending.merge(tr_candidates_recently.merge(tr_candidates_popular, **kwargs), **kwargs)\n",
    "tr_df[strategy_flags] = tr_df[strategy_flags].fillna(0)\n",
    "tr_df = tr_df.merge(tr_actual, how='left', on=['customer_id', 'article_id'])\n",
    "tr_df['label'] = tr_df['label'].fillna(0)\n",
    "tr_df = compress_df(tr_df, verbose=False)\n",
    "tr_cusotmers_feature = compress_df(make_customers_feature(customers, transactions, target_week=0, debug=True), verbose=False)\n",
    "tr_articles_feature = compress_df(make_articles_feature(articles, transactions, target_week=0, debug=True), verbose=False)\n",
    "tr_df = tr_df.merge(tr_customers_feature, how='left', on=['customer_id'], copy=False)\n",
    "tr_df = tr_df.merge(tr_articles_feature, how='left', on=['article_id'], copy=False)\n",
    "tr_df = compress_df(tr_df, verbose=False)\n",
    "print(f\"[Info] candidates per a customer: {len(tr_df) / len(tr_customers):.1f}\")\n",
    "print(f\"[Info] Precision: {tr_df['label'].sum() / len(tr_df):.5f}\")\n",
    "print(f\"[Info] Recall: {tr_df['label'].sum() / len(tr_actual):.5f}\")\n",
    "\n",
    "tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "\n",
    "params['num_iterations'] = model.best_iteration\n",
    "model = lgb.train(\n",
    "    params, dtrain, valid_sets=[dtrain], callbacks=[lgb.log_evaluation(10)])\n",
    "with open(f\"../models/lgb_rank/{EXP}_model_fold0.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': last_week}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.concat(feature_importance_dfs, ignore_index=True, axis=0)\n",
    "display(feature_importance_df.groupby(['feature'])[['importance(gain)']].mean().sort_values('importance(gain)', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tr_actual, tr_customers, tr_candidates_trending, tr_candidates_recently, tr_candidates_popular, tr_df, tr_customers_feature, tr_articles_feature\n",
    "del train_query, dtrain, params, model\n",
    "del feature_importance_dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test data\n",
    "BATCH_SIZE = 1000\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "test_customers = submission['customer_id'].map(id_to_index_dict).unique()\n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(len(test_customers)//BATCH_SIZE + 1), desc=\"Mini Batch\"):\n",
    "    if i == (len(test_customers)//BATCH_SIZE):\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : ]\n",
    "    else:\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
    "\n",
    "    test_candidates_trending = generate_candidates_trending(transactions, test_customers_batch, -1)\n",
    "    test_candidates_recently = generate_candidates_recently(transactions, test_customers_batch, -1)\n",
    "    test_candidates_popular = generate_candidates_popular(transactions, test_customers_batch, -1)    \n",
    "    test_df = compress_df(test_candidates_trending.merge(test_candidates_recently.merge(test_candidates_popular, **kwargs), **kwargs), verbose=False)\n",
    "    test_df[strategy_flags] = test_df[strategy_flags].fillna(0)    \n",
    "    test_customers_feature = compress_df(make_customers_feature(customers, transactions, target_week=-1 debug=True), verbose=False)\n",
    "    test_articles_feature = compress_df(make_articles_feature(articles, transactions, target_week=-1, debug=True), verbose=False)\n",
    "    test_df = test_df.merge(test_customers_feature, how='left', on=['customer_id'], copy=False)\n",
    "    test_df = test_df.merge(test_articles_feature, how='left', on=['article_id'], copy=False)\n",
    "    test_df = compress_df(test_df, verbose=False)\n",
    "    \n",
    "    pred = np.zeros(len(test_df))\n",
    "    all_weeks = oof_weeks + [0]\n",
    "    for w in all_weeks:\n",
    "        with open(f\"../models/lgb_rank/{EXP}_model_fold{w}.pkl\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        pred += model.predict(test_df[cols], num_iteration=model.best_iteration)    \n",
    "    pred = pred/len(all_weeks)\n",
    "    \n",
    "    test_df['predict_score'] = pred\n",
    "    test_df = test_df.sort_values('predict_score', ascending=False).drop_duplicates(['customer_id', 'article_id'], keep='first').reset_index(drop=True)\n",
    "    test_df['rank'] = test_df.groupby('customer_id')['predict_score'].rank('min', ascending=False)\n",
    "    test_df = test_df[test_df['rank'] <= 12]\n",
    "    \n",
    "    # test_df['article_id'] = le.inverse_transform(test_df['article_id'])\n",
    "    test_df['article_id'] = ' 0' + test_df['article_id'].astype(str)\n",
    "    preds.append(test_df.groupby('customer_id')['article_id'].sum())\n",
    "    \n",
    "pred_sr = pd.concat(preds, axis=0)\n",
    "display(pred_sr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e9ed82346646c38add97d7ba2e475d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mini Batch:   0%|          | 0/1372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del test_candidates_trending, test_candidates_recently, test_candidates_popular, test_df, test_customers_feature, test_articles_feature, pred, preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 0]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "popular_items_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "submission = submission[['customer_id', 'prediction']]\n",
    "display(submission.head())\n",
    "submission.to_csv(f'../submissions/{EXP}_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f804528fa1bcda80b2057737773baa2134c5be7e013153f88e69abab752260b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
