{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "# 学習データ5週分(検証データ1週含む)\n",
    "# 候補作り12週分\n",
    "# trendingで候補作り\n",
    "# articlesとcustomersの特徴量をtarget_weekごとに作る\n",
    "# 候補の良さを測る\n",
    "# 顧客毎の最終週で候補作り\n",
    "# コラボ商品で候補作り\n",
    "# 説明文KNN(tsne, 重複除外なし)\n",
    "# ペア商品\n",
    "# コラボのtarget_week=-1修正したやつ\n",
    "# albertでKNN\n",
    "# 説明文KNNのtsneのcos距離の掛け算を修正する\n",
    "# BertでKNN\n",
    "# New: 画像KNN\n",
    "# MAP@12 (all): 0.032035\n",
    "# MAP@12 (cold start): 0.006125\n",
    "EXP = '036'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from psutil import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "from time import time\n",
    "import warnings\n",
    "from functools import reduce\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = None\n",
    "warnings.simplefilter('ignore', pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "data_path = Path('../input/h-and-m-personalized-fashion-recommendations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>t_diff</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31788319</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>929511001</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788320</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>891322004</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788321</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371721</td>\n",
       "      <td>918325001</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788322</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371747</td>\n",
       "      <td>833459002</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788323</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371960</td>\n",
       "      <td>898573003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_dat  customer_id  article_id     price  sales_channel_id  \\\n",
       "31788319 2020-09-22      1371691   929511001  0.059305                 2   \n",
       "31788320 2020-09-22      1371691   891322004  0.042356                 2   \n",
       "31788321 2020-09-22      1371721   918325001  0.043203                 1   \n",
       "31788322 2020-09-22      1371747   833459002  0.006763                 1   \n",
       "31788323 2020-09-22      1371960   898573003  0.033881                 2   \n",
       "\n",
       "          t_diff  week  \n",
       "31788319       0     0  \n",
       "31788320       0     0  \n",
       "31788321       0     0  \n",
       "31788322       0     0  \n",
       "31788323       0     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transactions = pd.read_csv(\n",
    "    data_path / f'transactions_train.csv',\n",
    "    # set dtype or pandas will drop the leading '0' and convert to int\n",
    "    dtype={'article_id': 'int32'},\n",
    "    parse_dates=['t_dat'])\n",
    "customers = pd.read_csv(data_path / 'customers.csv')\n",
    "articles = pd.read_csv(\n",
    "    '../input/h-and-m-personalized-fashion-recommendations/articles.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "\n",
    "t_max = transactions['t_dat'].max()\n",
    "transactions['t_diff'] = (t_max - transactions['t_dat']).dt.days\n",
    "transactions['week'] = transactions['t_diff'] // 7\n",
    "\n",
    "# Noneの表記不揃い対策\n",
    "customers.loc[~customers['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "\n",
    "# メモリ削減\n",
    "id_to_index_dict = dict(zip(customers[\"customer_id\"], customers.index))\n",
    "index_to_id_dict = dict(zip(customers.index, customers[\"customer_id\"]))\n",
    "transactions[\"customer_id\"] = transactions[\"customer_id\"].map(id_to_index_dict).astype('int32')\n",
    "customers['customer_id'] = customers['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "\n",
    "print(transactions.shape)\n",
    "display(transactions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_trending(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    # 以下のロジックはtrendingの公開カーネル参照\n",
    "    weekly_sales = df.groupby(['week', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    df = df.merge(weekly_sales, on=['week', 'article_id'], how='left')    \n",
    "    weekly_sales = weekly_sales.reset_index().set_index('article_id')\n",
    "    df = df.join(weekly_sales.loc[weekly_sales['week']==1, ['count']], on='article_id', rsuffix='_targ')\n",
    "    df['count_targ'].fillna(0, inplace=True)\n",
    "    df['quotient'] = df['count_targ'] / df['count']\n",
    "    \n",
    "    t_max = df['t_dat'].max()\n",
    "    df['x'] = ((t_max - df['t_dat']) / np.timedelta64(1, 'D')).astype(int)\n",
    "    df['dummy_1'] = 1\n",
    "    df['x'] = df[['x', 'dummy_1']].max(axis=1)    \n",
    "    a, b, c, d = 2.5e4, 1.5e5, 2e-1, 1e3\n",
    "    df['y'] = a / np.sqrt(df['x']) + b * np.exp(-c*df['x']) - d\n",
    "    df['dummy_0'] = 0 \n",
    "    df['y'] = df[[\"y\", \"dummy_0\"]].max(axis=1)\n",
    "    df['trending_value'] = df['quotient'] * df['y']\n",
    "    df = df.groupby(['customer_id', 'article_id']).agg({'trending_value': 'sum'}).reset_index()\n",
    "    df = df.loc[df['trending_value'] > 0]\n",
    "    # df['rank'] = df.groupby(\"customer_id\")[\"value\"].rank(\"dense\", ascending=False)\n",
    "    # df = df.loc[df['rank'] <= 12]\n",
    "    \n",
    "    df['isin_trending'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'trending_value', 'isin_trending']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_recently(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''直近の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 12\")  # 12週分の履歴を使用\n",
    "    \n",
    "    # 各週の購入個数から特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = df.query('week == @w').groupby(['customer_id', 'article_id'])['article_id'].count().rename(f'count_{w}w').reset_index().copy()\n",
    "        if w == 1:\n",
    "            purchase_df = tmp\n",
    "            continue\n",
    "        purchase_df = purchase_df.merge(tmp, how='outer', on=['customer_id', 'article_id'])\n",
    "        \n",
    "    purchase_df['isin_recently'] = 1\n",
    "    \n",
    "    return purchase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_popular(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''販売数の多い商品から候補生成'''\n",
    "    # make_articles_featureが販売数から特徴量を作ってくれるから、この関数は候補のペアだけ返す\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 4\")  # 4週分の履歴を使用\n",
    "\n",
    "    # 各週の販売数上位12個\n",
    "    dummy_count_df = df.groupby(['article_id', 'week'])['week'].count().rename('dummy_count').reset_index().copy()\n",
    "    dummy_count_df['rank_in_week'] = dummy_count_df.groupby('week')['dummy_count'].rank(method='min', ascending=False)\n",
    "    dummy_articles = dummy_count_df.query('rank_in_week <= 12')['article_id'].unique()\n",
    "\n",
    "    dummy_df = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [np.repeat(customers, repeats=len(dummy_articles)).reshape(-1, 1),\n",
    "            np.repeat(dummy_articles[None, :], repeats=len(customers), axis=0).reshape(-1, 1)],\n",
    "            axis=1),\n",
    "        columns = ['customer_id', 'article_id']\n",
    "    )\n",
    "    \n",
    "    dummy_df['isin_popular'] = 1\n",
    "    \n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_lastw(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''customer毎の最後の購入から1週間の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\").copy()\n",
    "\n",
    "    # 最後の購入から1週間に絞る\n",
    "    df['max_dat'] = df['customer_id'].map(df.groupby('customer_id')['t_dat'].max())\n",
    "    df['max_diff'] = (df['max_dat'] - df['t_dat']).dt.days\n",
    "    df = df.query('max_diff <= 6')\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count_lastw').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.sort_values('t_dat', ascending=True)\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    df = df.rename(columns={'t_dat': 'last_dat'})\n",
    "    df['last_dat'] = df['last_dat'].view(np.int64) // 10 ** 9\n",
    "    \n",
    "    df['isin_lastw'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'count_lastw', 'last_dat', 'isin_lastw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_pair(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''ルールベース協調フィルタリングから候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    pairs = pd.read_parquet(f'../input/hmitempairs/pairs_fold{target_week}_5items.parquet')\n",
    "    df = df.merge(pairs, on='article_id', how='inner')\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'pair_article_id': 'article_id'})\n",
    "    df['count_pair'] = df['count'] * df['pair_ratio']\n",
    "\n",
    "    df['isin_pair'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'count_pair', 'isin_pair']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_collabo(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''コラボ商品の購入履歴から候補生成'''\n",
    "    df = pd.read_csv('../input/ranking_features/collabo_candidate_with_nseries.csv', dtype={'article_id': 'int32'})\n",
    "    # FIXME: ほんとはスコープ外変数の参照は良くない\n",
    "    df['customer_id'] = df['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "    df = df.query('customer_id in @customers').copy()\n",
    "    df = df.query('target_week == @target_week').copy()\n",
    "    \n",
    "    df['isin_collabo'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_candidates_desc_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int, encoder: str):\n",
    "    '''説明文の埋め込みベクトル（次元削減済み）から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    \n",
    "    desc_knn = pd.read_csv(f'../input/ranking_features/item_features_{encoder}_class.csv', dtype={'article_id': 'int32'})\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        dfs.append(df.merge(desc_knn[['article_id', f'knn_article_id_{i}', f'knn_distance_{i}']])\n",
    "                        .rename(columns={f'knn_article_id_{i}': 'knn_article_id', f'knn_distance_{i}': f'{encoder}_distance'}))\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article_id': 'article_id'})\n",
    "    df[f'{encoder}_count'] = df['count'] * abs(1-df[f'{encoder}_distance'])\n",
    "    \n",
    "    # # 重複を削除、item1とitem2それぞれの類似として同じitemが出てくることがある\n",
    "    # df = df.sort_values(f'{encoder}_count', ascending=True).drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "    \n",
    "    df[f'isin_{encoder}_knn'] = 1\n",
    "                   \n",
    "    return df[['customer_id', 'article_id', f'{encoder}_count', f'{encoder}_distance', f'isin_{encoder}_knn']]\n",
    "\n",
    "def _generate_candidates_albert_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "\n",
    "    albert_knn = pd.read_csv('../input/ranking_features/Albert_cos.csv', dtype={'article': 'int32', 'knn_article': 'int32'})\n",
    "    albert_knn = albert_knn.rename(columns={'article': 'article_id'})\n",
    "    \n",
    "    df = df.merge(albert_knn)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article': 'article_id', 'distances': 'albert_distance'})\n",
    "    df['albert_count'] = df['count'] * abs(1-df['albert_distance'])\n",
    "    \n",
    "    df['isin_albert_knn'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'albert_count', 'albert_distance', 'isin_albert_knn']]\n",
    "\n",
    "def _generate_candidates_bert_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "\n",
    "    usecols=['article_id', 'knn_article_id_0', 'knn_distance_0', 'knn_article_id_1', 'knn_distance_1', \n",
    "             'knn_article_id_2', 'knn_distance_2', 'knn_article_id_3', 'knn_distance_3', 'knn_article_id_4', 'knn_distance_4']\n",
    "    bert_knn = pd.read_csv('../input/ranking_features/article_bert_pairs.csv', dtype={'article_id': 'int32'}, usecols=usecols)\n",
    "\n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        dfs.append(df.merge(bert_knn[['article_id', f'knn_article_id_{i}', f'knn_distance_{i}']])\n",
    "                        .rename(columns={f'knn_article_id_{i}': 'knn_article_id', f'knn_distance_{i}': 'bert_distance'}))\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article_id': 'article_id'})\n",
    "    df['bert_count'] = df['count'] * abs(1-df['bert_distance'])\n",
    "    \n",
    "    df['isin_bert_knn'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'bert_count', 'bert_distance', 'isin_bert_knn']]    \n",
    "    \n",
    "def generate_candidates_desc_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''商品説明文から候補生成'''\n",
    "    desc_knns = []\n",
    "    \n",
    "    desc_knns.append(_generate_candidates_desc_knn(transactions, customers, target_week, encoder='tsne'))\n",
    "    # desc_knns.append(_generate_candidates_desc_knn(transactions, customers, target_week, encoder='umap'))\n",
    "    desc_knns.append(_generate_candidates_albert_knn(transactions, customers, target_week))\n",
    "    desc_knns.append(_generate_candidates_bert_knn(transactions, customers, target_week))\n",
    "    \n",
    "    df = reduce(lambda left, right: pd.merge(left, right, how='outer', on=['customer_id', 'article_id']), desc_knns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_im_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''画像の埋め込みベクトル（次元削減済み）から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    \n",
    "    im_knn = pd.read_parquet(f'../input/ranking_features/candidates_knn_vgg.parquet')\n",
    "    im_knn['article_id'] = im_knn['article_id'].astype('int32')\n",
    "    im_knn['knn_im_article_id'] = im_knn['knn_im_article_id'].astype('int32')\n",
    "    \n",
    "    df = df.merge(im_knn)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_im_article_id': 'article_id', 'knn_im_distance': 'vgg_distance'})\n",
    "    df[f'vgg_count'] = df['count'] * abs(1-df['vgg_distance'])\n",
    "    \n",
    "    # # 重複を削除、item1とitem2それぞれの類似として同じitemが出てくることがある\n",
    "    # df = df.sort_values(f'{encoder}_count', ascending=True).drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "    \n",
    "    df[f'isin_vgg_knn'] = 1\n",
    "                   \n",
    "    return df[['customer_id', 'article_id', 'vgg_count', 'vgg_distance', 'isin_vgg_knn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_sequence_word2vec(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''購入履歴をシーケンスと見立て、article_idをword2vec'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    \n",
    "    desc_knn = pd.read_csv(f'../input/ranking_features/item_features_tsne_word2vec.csv', dtype={'article_id': 'int32'})\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        dfs.append(df.merge(desc_knn[['article_id', f'knn_article_id_{i}', f'knn_distance_{i}']])\n",
    "                        .rename(columns={f'knn_article_id_{i}': 'knn_article_id', f'knn_distance_{i}': 'sequence_word2vec_distance'}))\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article_id': 'article_id'})\n",
    "    df['sequence_word2vec_count'] = df['count'] * df['sequence_word2vec_distance']\n",
    "    \n",
    "    # # 重複を削除、item1とitem2それぞれの類似として同じitemが出てくることがある\n",
    "    # df = df.sort_values(f'{encoder}_count', ascending=True).drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "    \n",
    "    df['isin_sequence_word2vec_knn'] = 1\n",
    "                   \n",
    "    return df[['customer_id', 'article_id', 'sequence_word2vec_count', 'sequence_word2vec_distance', 'isin_sequence_word2vec_knn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_customers_feature(customers: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''customer毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    customers_feature = customers.drop(['postal_code'], axis=1).copy()\n",
    "    customers_feature.loc[~customers_feature['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None  # Noneの表記揃え\n",
    "    customers_feature[['FN', 'Active']] = customers_feature[['FN', 'Active']].fillna(0)\n",
    "\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_purchase = df.groupby(['customer_id', 'week'])['week'].count().rename('purchase').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_purchase.groupby('customer_id')['purchase'].agg(agg_name)\n",
    "        customers_feature[f'purchase_{agg_name}_groupby_customer'] = customers_feature['customer_id'].map(agg_sr)\n",
    "    \n",
    "    # 各週の購入数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_purchase[weekly_purchase['week']==w]\n",
    "        tmp = tmp[['customer_id', 'purchase']].set_index('customer_id')['purchase']\n",
    "        customers_feature[f'purchase_{w}w'] = customers_feature['customer_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_ratio_{w}w'] = customers_feature[f'purchase_{w}w'] / customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_diff_{w}w'] = customers_feature[f'purchase_{w}w'] - customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "\n",
    "    # --- 一意の(article_id, week)を購入単位とみなす ---\n",
    "    # ※あるarticleを1個買うことを、ふつうは購入単位とみなしている\n",
    "    # rank: 何回目の購入か\n",
    "    unique_transactions = df[['customer_id', 'article_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['customer_id', 'article_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再購入したarticleの割合\n",
    "    customers_feature['repurchase_article'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article'] = customers_feature['customer_id'].map(unique_transactions.drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count())\n",
    "    customers_feature['repurchase_article_percent'] = customers_feature['repurchase_article'] / customers_feature['purchase_article']\n",
    "\n",
    "    # 再購入を含む週の割合\n",
    "    customers_feature['repurchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count()).fillna(0)\n",
    "    customers_feature['purchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count())\n",
    "    customers_feature['repurchase_week_percent'] = customers_feature['repurchase_week'] / customers_feature['purchase_week']\n",
    "    \n",
    "    # 再購入の割合\n",
    "    customers_feature['repurchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('customer_id')['customer_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.groupby('customer_id')['customer_id'].count())\n",
    "    customers_feature['repurchase_article_and_week_percent'] = customers_feature['repurchase_article_and_week'] / customers_feature['purchase_article_and_week']\n",
    "    # --- おわり ---\n",
    "    \n",
    "    return customers_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_articles_feature(articles: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''article毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    articles_feature = articles.drop(\n",
    "        ['prod_name', 'product_type_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'prod_name', 'department_name', 'detail_desc'], \n",
    "        axis=1).copy()\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_sale = df.groupby(['article_id', 'week'])['week'].count().rename('sale').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_sale.groupby('article_id')['sale'].agg(agg_name)\n",
    "        articles_feature[f'sale_{agg_name}_groupby_article'] = articles_feature['article_id'].map(agg_sr)\n",
    "\n",
    "    # 各週の販売数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_sale[weekly_sale['week']==w]\n",
    "        tmp = tmp[['article_id', 'sale']].set_index('article_id')['sale']\n",
    "        articles_feature[f'sale_{w}w'] = articles_feature['article_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_ratio_{w}w'] = articles_feature[f'sale_{w}w'] / articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_diff_{w}w'] = articles_feature[f'sale_{w}w'] - articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "\n",
    "    # --- 一意の(customer_id, week)を販売単位とみなす ---\n",
    "    # ※あるcustomerに1つ売れることを、ふつうは販売単位とみなしている\n",
    "    # rank: 何回目の販売か\n",
    "    unique_transactions = df[['article_id', 'customer_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['article_id', 'customer_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再販売したcustomerの割合\n",
    "    articles_feature['resale_customer'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer'] = articles_feature['article_id'].map(unique_transactions.drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count())\n",
    "    articles_feature['resale_customer_percent'] = articles_feature['resale_customer'] / articles_feature['sale_customer']\n",
    "\n",
    "    # 再販売を含む週の割合\n",
    "    articles_feature['resale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count()).fillna(0)\n",
    "    articles_feature['sale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count())\n",
    "    articles_feature['resale_week_percent'] = articles_feature['resale_week'] / articles_feature['sale_week']\n",
    "\n",
    "    # 再販売の割合\n",
    "    articles_feature['resale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('article_id')['article_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.groupby('article_id')['article_id'].count())\n",
    "    articles_feature['resale_customer_and_week_percent'] = articles_feature['resale_customer_and_week'] / articles_feature['sale_customer_and_week']\n",
    "    # --- 終わり ---\n",
    "    \n",
    "    return articles_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_df(transactions: pd.DataFrame, week: int, is_labeled: bool, use_customers: np.ndarray = None, metric_verbose: bool = True, compress_verbose: bool = False):\n",
    "    '''ランク学習モデルへの入力データ作成'''\n",
    "    # WARNING: 戦略を追加した時に書き換え忘れ注意！\n",
    "    strategy_flags = [\n",
    "        'isin_trending', \n",
    "        'isin_recently', \n",
    "        'isin_popular', \n",
    "        'isin_lastw', \n",
    "        'isin_pair', \n",
    "        'isin_collabo', \n",
    "        'isin_tsne_knn', \n",
    "        'isin_albert_knn',\n",
    "        'isin_bert_knn',\n",
    "        'isin_vgg_knn',\n",
    "        # 'isin_sequence_word2vec_knn',\n",
    "    ]\n",
    "    kwargs = {'how': 'outer', 'on': ['customer_id', 'article_id'], 'copy': True}\n",
    "    \n",
    "    # 入力データに含むcustomersの絞り込み\n",
    "    if use_customers is not None:\n",
    "        data_customers = use_customers\n",
    "    elif week >= 0:\n",
    "        data_customers = transactions.query(\"week == @week\")['customer_id'].unique()\n",
    "    else:\n",
    "        raise ValueError('set use_customers as something when week=-1.')\n",
    "    \n",
    "    # 候補作り\n",
    "    data_dfs = []\n",
    "    data_dfs.append(generate_candidates_trending(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_recently(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_popular(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_lastw(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_pair(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_collabo(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_desc_knn(transactions, data_customers, week))\n",
    "    data_dfs.append(generate_candidates_im_knn(transactions, data_customers, week))\n",
    "    # data_dfs.append(generate_candidates_sequence_word2vec(transactions, data_customers, week))\n",
    "    \n",
    "    data_df = reduce(\n",
    "        lambda  left,right: pd.merge(left, right, on=['customer_id', 'article_id'], how='outer'), \n",
    "        data_dfs)\n",
    "\n",
    "    data_df[strategy_flags] = data_df[strategy_flags].fillna(0)\n",
    "    \n",
    "    # 正解ラベル付け\n",
    "    if is_labeled:\n",
    "        if week < 0:\n",
    "            raise ValueError(f\"can't label when week={week}.\")\n",
    "        data_actual = transactions.query(\"week == @week\")[['customer_id', 'article_id']].drop_duplicates()\n",
    "        data_actual['label'] = 1\n",
    "        data_df = data_df.merge(data_actual, how='left', on=['customer_id', 'article_id'])\n",
    "        data_df['label'] = data_df['label'].fillna(0)\n",
    "    \n",
    "    # customers, articles の特徴量\n",
    "    data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    data_customers_feature = compress_df(\n",
    "        make_customers_feature(customers, transactions, target_week=week, debug=True), \n",
    "        verbose=compress_verbose)\n",
    "    data_articles_feature = compress_df(\n",
    "        make_articles_feature(articles, transactions, target_week=week, debug=True), \n",
    "        verbose=compress_verbose)\n",
    "    data_df = data_df.merge(data_customers_feature, how='left', on=['customer_id'])\n",
    "    data_df = data_df.merge(data_articles_feature, how='left', on=['article_id'])\n",
    "    # data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    \n",
    "    # 候補のスコア\n",
    "    if metric_verbose:\n",
    "        print(f\"[Info] shape     : {data_df.shape}\")\n",
    "        print(f\"[Info] mem       : {data_df.memory_usage().sum() / 1024**2 :5.2f} Mb\")\n",
    "        print(f\"[Info] candidates: {len(data_df) / len(data_customers):.1f} 個 / customer\")\n",
    "        display(data_df[strategy_flags].astype(float).mean())\n",
    "        if is_labeled:\n",
    "            print(f\"[Info] Precision: {data_df['label'].sum() / len(data_df):.5f}\")\n",
    "            print(f\"[Info] Recall   : {data_df['label'].sum() / len(data_actual):.5f}\")\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_df(\n",
    "    df: pd.DataFrame, \n",
    "    category_columns: list =['club_member_status', 'fashion_news_frequency', 'product_group_name', 'index_code'], \n",
    "    verbose: bool =True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    '''DataFrameのデータ型を適切に選び圧縮'''\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        bar = tqdm(df.columns, leave=False)\n",
    "    else:\n",
    "        bar = df.columns\n",
    "    for col in bar:\n",
    "        col_type = df[col].dtypes\n",
    "        if col in category_columns:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(category)\")\n",
    "            df[col] = df[col].astype('category')\n",
    "        elif col_type in numerics:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(num)\")\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params: dict, cols: list, tr_df: pd.DataFrame, val_df: pd.DataFrame = None, early_stopping: bool = True):\n",
    "    if val_df is None:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dtrain], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    else:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "        if early_stopping:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.early_stopping(10, first_metric_only=True), lgb.log_evaluation(10)])\n",
    "        else:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_df: pd.DataFrame, model_folds: list):\n",
    "    pred = np.zeros(len(test_df))\n",
    "    for w in model_folds:\n",
    "        with open(f\"../models/lgb_rank/{EXP}_model_fold{w}.pkl\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        pred += model.predict(test_df[cols], num_iteration=model.best_iteration)    \n",
    "    pred = pred/len(model_folds)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_sr(test_df: pd.DataFrame, pred: np.ndarray):\n",
    "    '''モデルのスコアを用いて並び替え＆上位12個抽出'''\n",
    "    test_df['predict_score'] = pred\n",
    "    test_df = test_df.sort_values('predict_score', ascending=False).drop_duplicates(['customer_id', 'article_id'], keep='first').reset_index(drop=True)\n",
    "    test_df['rank'] = test_df.groupby('customer_id')['predict_score'].rank('min', ascending=False)\n",
    "    test_df = test_df[test_df['rank'] <= 12]\n",
    "    \n",
    "    # test_df['article_id'] = le.inverse_transform(test_df['article_id'])\n",
    "    test_df['article_id'] = ' 0' + test_df['article_id'].astype(str)\n",
    "    \n",
    "    top_sr = test_df.groupby('customer_id')['article_id'].sum()\n",
    "    \n",
    "    return top_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optunaによるチューニング\n",
    "# import optuna.integration.lightgbm as lgb_optuna\n",
    "# from optuna.logging import set_verbosity\n",
    "# import numpy as np\n",
    "# import random as rn\n",
    "\n",
    "# set_verbosity(-1)\n",
    "# np.random.seed(71)\n",
    "# rn.seed(71)\n",
    "\n",
    "# params = {\n",
    "#     'objective': 'lambdarank',\n",
    "#     'boosting': 'gbdt',  # default: 'gbdt', 'gbdt' or 'dart'\n",
    "#     'num_iterations': 1000,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'metric': ['ndcg'],\n",
    "#     'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "#     'random_state': 71,  # 訓練用とは違う値にする\n",
    "#     'verbosity': -1,  # -1: ignore, 0: warnings, 1: info\n",
    "#     'deterministic': True,  # 再現性確保\n",
    "#     'force_row_wise': True  # 再現性確保\n",
    "# }\n",
    "\n",
    "# tr_df = make_data_df(transactions, 2, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "# cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "# tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "# train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "\n",
    "# val_df = make_data_df(transactions, 1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "# val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "\n",
    "# model = lgb_optuna.LightGBMTuner(params, dtrain, valid_sets=[dtrain, dval], early_stopping_rounds=10, verbose_eval=-1, optuna_seed=71)\n",
    "# model.run()\n",
    "\n",
    "# best_params = model.params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "params = {\n",
    "    # Core Parameters\n",
    "    'objective': 'lambdarank',\n",
    "    'boosting': 'gbdt',  # default: 'gbdt', ['gbdt', 'dart', 'goss'], dart: 超遅いけど高精度\n",
    "    'num_iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,  # default: 31, large for accuracy\n",
    "    'num_threads': cpu_count(logical=False),\n",
    "    'random_state': 41,\n",
    "\n",
    "    # Learning Control Parameters\n",
    "    'force_col_wise': True,\n",
    "    # 'histogram_pool_size': -1.0,  # default: -1.0, max cache size in MB for historical histogram, (histogram_pool_size + dataset size) = approximately RAM used\n",
    "    'min_data_in_leaf': 20,  # default: 20, large dealing with over-fitting\n",
    "    'min_sum_hessian_in_leaf': 1e-3,  # default: 1e-3, large dealing with over-fitting\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    # 'drop_rate': 0.1,  # used only in dart, a fraction of previous trees to drop during the dropout\n",
    "    'verbosity': 0,  # 0: warnings, 1: info\n",
    "\n",
    "    # Dataset Parameters\n",
    "    'max_bin': 255,  # default: 255, large for accuracy\n",
    "    'min_data_in_bin': 3,  # default: 3, \n",
    "    'bin_construct_sample_cnt': 200000,  # default: 200000, larger for accuracy\n",
    "\n",
    "    # Objective Parameters\n",
    "    'lambdarank_truncation_level': 30,\n",
    "    'lambdarank_norm': True,\n",
    "    # 'label_gain': [0, 1],\n",
    "\n",
    "    # Metric Parameters\n",
    "    'metric': ['ndcg'],\n",
    "    'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abbe6d2f3724514978dbe66b6088494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_week(fold): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 729.80 Mb (70.6% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 614.96 Mb (74.9% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 48.62 Mb (75.0% reduction)\n",
      "[Info] shape     : (8140955, 514)\n",
      "[Info] mem       : 8066.61 Mb\n",
      "[Info] candidates: 113.0 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending      0.154077\n",
       "isin_recently      0.083734\n",
       "isin_popular       0.389538\n",
       "isin_lastw         0.038412\n",
       "isin_pair          0.130725\n",
       "isin_collabo       0.000234\n",
       "isin_tsne_knn      0.125528\n",
       "isin_albert_knn    0.288695\n",
       "isin_bert_knn      0.136119\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00333\n",
      "[Info] Recall   : 0.11735\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.856316\tvalid_1's ndcg@12: 0.863438\n",
      "[20]\ttraining's ndcg@12: 0.858241\tvalid_1's ndcg@12: 0.86358\n",
      "[30]\ttraining's ndcg@12: 0.859873\tvalid_1's ndcg@12: 0.864097\n",
      "[40]\ttraining's ndcg@12: 0.86138\tvalid_1's ndcg@12: 0.864323\n",
      "[50]\ttraining's ndcg@12: 0.862953\tvalid_1's ndcg@12: 0.864293\n",
      "[60]\ttraining's ndcg@12: 0.864473\tvalid_1's ndcg@12: 0.864902\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's ndcg@12: 0.864312\tvalid_1's ndcg@12: 0.864974\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 3\n"
     ]
    }
   ],
   "source": [
    "# train lgb ranker\n",
    "best_iterations = []\n",
    "feature_importance_dfs = []\n",
    "oof_weeks = [4, 3, 2, 1]\n",
    "\n",
    "for i, w in enumerate(tqdm(oof_weeks)):\n",
    "    print(f\"\\ntarget_week(fold): {w}\")\n",
    "    if i == 0:\n",
    "        compress_verbose=True\n",
    "    else:\n",
    "        compress_verbose=False\n",
    "    \n",
    "    tr_df = make_data_df(transactions, w, is_labeled=True, metric_verbose=True, compress_verbose=compress_verbose)\n",
    "    val_df = make_data_df(transactions, w-1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    if i == 0:\n",
    "        exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "        cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "        with open(f'../models/lgb_rank/{EXP}_cols.pkl', 'wb') as f:\n",
    "            pickle.dump(cols, f)\n",
    "\n",
    "    model = train(params, cols, tr_df, val_df)\n",
    "    with open(f'../models/lgb_rank/{EXP}_model_fold{w}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    best_iterations.append(model.best_iteration)\n",
    "    feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': w}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict val data\n",
    "val_df = make_data_df(transactions, 0, is_labeled=False, metric_verbose=True, compress_verbose=False)\n",
    "val_pred = predict(val_df, oof_weeks)\n",
    "print(np.sort(val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val top rank articles\n",
    "val_df2 = val_df.copy()\n",
    "val_pred_sr = extract_top_sr(val_df2, val_pred)\n",
    "display(val_pred_sr.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 1]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "display(popular_items_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(val_pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission_fold0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_df, model\n",
    "del transactions_last_week, top12, popular_items, popular_items_dict, popular_items_sr\n",
    "del val_df2, val_pred_sr, \n",
    "del submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train last target_week(=0) data\n",
    "tr_df = make_data_df(transactions, week=0, is_labeled=True, metric_verbose=True, compress_verbose=False)\n",
    "\n",
    "# valがないのでアーリーストッピングが使えない\n",
    "params['num_iterations'] = int(np.mean(best_iterations))\n",
    "model = train(params, cols, tr_df)\n",
    "with open(f\"../models/lgb_rank/{EXP}_model_fold0.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.concat(feature_importance_dfs, ignore_index=True, axis=0)\n",
    "display(feature_importance_df.groupby(['feature'])[['importance(gain)']].mean().sort_values('importance(gain)', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tr_df, params, model, best_iterations, feature_importance_dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test data\n",
    "BATCH_SIZE = 100_000\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "test_customers = submission['customer_id'].map(id_to_index_dict).unique()\n",
    "\n",
    "# バッチ処理\n",
    "def process(i):\n",
    "    if i == (len(test_customers)//BATCH_SIZE):\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : ]\n",
    "    else:\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
    "    \n",
    "    if i == 0:  # meric_verbose=True\n",
    "        test_df = make_data_df(transactions, week=-1, is_labeled=False, use_customers=test_customers_batch, metric_verbose=True, compress_verbose=False)\n",
    "    else:       # metric_verbose=False\n",
    "        test_df = make_data_df(transactions, week=-1, is_labeled=False, use_customers=test_customers_batch, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    all_weeks = oof_weeks + [0]\n",
    "    pred = predict(test_df, all_weeks)\n",
    "    return extract_top_sr(test_df, pred)\n",
    "\n",
    "# single process execution\n",
    "preds = []\n",
    "for i in tqdm(range(len(test_customers)//BATCH_SIZE + 1)):\n",
    "    preds.append(process(i))\n",
    "\n",
    "# # multi process execution\n",
    "# # cpus = cpu_count(logical=False)\n",
    "# cpus = 4\n",
    "# print('cpu(core): ', cpus)\n",
    "# preds = Parallel(n_jobs=cpus, verbose=0)( [delayed(process)(i) for i in range(len(test_customers)//BATCH_SIZE + 1)] )\n",
    "\n",
    "pred_sr = pd.concat(preds, axis=0)\n",
    "display(pred_sr.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # most popular items\n",
    "# transactions_last_week = transactions.loc[transactions.week == 0]\n",
    "# top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "# print(\"Top 12 popular items:\")\n",
    "# print( top12 )\n",
    "\n",
    "# customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "# transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "# popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "# popular_items_dict = {}\n",
    "# for index in popular_items.index.levels[0]:\n",
    "#     popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "# popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "# popular_items_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "# submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "# submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "# submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f804528fa1bcda80b2057737773baa2134c5be7e013153f88e69abab752260b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
