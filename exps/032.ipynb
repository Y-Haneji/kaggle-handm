{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "# 学習データ5週分(検証データ1週含む)\n",
    "# 候補作り12週分\n",
    "# trendingで候補作り\n",
    "# articlesとcustomersの特徴量をtarget_weekごとに作る\n",
    "# 候補の良さを測る\n",
    "# 顧客毎の最終週で候補作り\n",
    "# コラボ商品で候補作り\n",
    "# 説明文KNN(tsne, 重複除外なし)\n",
    "# ペア商品\n",
    "# New: bin_construct_sample_cnt を減らす影響を調べる ✅0.0003下がった\n",
    "# EXP=030, bin_construct_sample_cnt=200000\n",
    "# MAP@12 (all): 0.032142\n",
    "# MAP@12 (cold start): 0.006950\n",
    "# bin_construct_sample_cnt=100000にしてみた\n",
    "# MAP@12 (all): 0.031699\n",
    "# MAP@12 (cold start): 0.005937\n",
    "# LB: 0.0265 (<- 0.0268: EXP030)\n",
    "EXP = '032'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from psutil import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns = None\n",
    "warnings.simplefilter('ignore', pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "data_path = Path('../input/h-and-m-personalized-fashion-recommendations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>t_diff</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31788319</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>929511001</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788320</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>891322004</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788321</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371721</td>\n",
       "      <td>918325001</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788322</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371747</td>\n",
       "      <td>833459002</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788323</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371960</td>\n",
       "      <td>898573003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_dat  customer_id  article_id     price  sales_channel_id  \\\n",
       "31788319 2020-09-22      1371691   929511001  0.059305                 2   \n",
       "31788320 2020-09-22      1371691   891322004  0.042356                 2   \n",
       "31788321 2020-09-22      1371721   918325001  0.043203                 1   \n",
       "31788322 2020-09-22      1371747   833459002  0.006763                 1   \n",
       "31788323 2020-09-22      1371960   898573003  0.033881                 2   \n",
       "\n",
       "          t_diff  week  \n",
       "31788319       0     0  \n",
       "31788320       0     0  \n",
       "31788321       0     0  \n",
       "31788322       0     0  \n",
       "31788323       0     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transactions = pd.read_csv(\n",
    "    data_path / f'transactions_train.csv',\n",
    "    # set dtype or pandas will drop the leading '0' and convert to int\n",
    "    dtype={'article_id': 'int32'},\n",
    "    parse_dates=['t_dat'])\n",
    "customers = pd.read_csv(data_path / 'customers.csv')\n",
    "articles = pd.read_csv(\n",
    "    '../input/h-and-m-personalized-fashion-recommendations/articles.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "\n",
    "t_max = transactions['t_dat'].max()\n",
    "transactions['t_diff'] = (t_max - transactions['t_dat']).dt.days\n",
    "transactions['week'] = transactions['t_diff'] // 7\n",
    "\n",
    "# Noneの表記不揃い対策\n",
    "customers.loc[~customers['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "\n",
    "# メモリ削減\n",
    "id_to_index_dict = dict(zip(customers[\"customer_id\"], customers.index))\n",
    "index_to_id_dict = dict(zip(customers.index, customers[\"customer_id\"]))\n",
    "transactions[\"customer_id\"] = transactions[\"customer_id\"].map(id_to_index_dict).astype('int32')\n",
    "customers['customer_id'] = customers['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "\n",
    "print(transactions.shape)\n",
    "display(transactions.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_trending(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    # 以下のロジックはtrendingの公開カーネル参照\n",
    "    weekly_sales = df.groupby(['week', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    df = df.merge(weekly_sales, on=['week', 'article_id'], how='left')    \n",
    "    weekly_sales = weekly_sales.reset_index().set_index('article_id')\n",
    "    df = df.join(weekly_sales.loc[weekly_sales['week']==1, ['count']], on='article_id', rsuffix='_targ')\n",
    "    df['count_targ'].fillna(0, inplace=True)\n",
    "    df['quotient'] = df['count_targ'] / df['count']\n",
    "    \n",
    "    t_max = df['t_dat'].max()\n",
    "    df['x'] = ((t_max - df['t_dat']) / np.timedelta64(1, 'D')).astype(int)\n",
    "    df['dummy_1'] = 1\n",
    "    df['x'] = df[['x', 'dummy_1']].max(axis=1)    \n",
    "    a, b, c, d = 2.5e4, 1.5e5, 2e-1, 1e3\n",
    "    df['y'] = a / np.sqrt(df['x']) + b * np.exp(-c*df['x']) - d\n",
    "    df['dummy_0'] = 0 \n",
    "    df['y'] = df[[\"y\", \"dummy_0\"]].max(axis=1)\n",
    "    df['trending_value'] = df['quotient'] * df['y']\n",
    "    df = df.groupby(['customer_id', 'article_id']).agg({'trending_value': 'sum'}).reset_index()\n",
    "    df = df.loc[df['trending_value'] > 0]\n",
    "    # df['rank'] = df.groupby(\"customer_id\")[\"value\"].rank(\"dense\", ascending=False)\n",
    "    # df = df.loc[df['rank'] <= 12]\n",
    "    \n",
    "    df['isin_trending'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'trending_value', 'isin_trending']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_recently(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''直近の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 12\")  # 12週分の履歴を使用\n",
    "    \n",
    "    # 各週の購入個数から特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = df.query('week == @w').groupby(['customer_id', 'article_id'])['article_id'].count().rename(f'count_{w}w').reset_index().copy()\n",
    "        if w == 1:\n",
    "            purchase_df = tmp\n",
    "            continue\n",
    "        purchase_df = purchase_df.merge(tmp, how='outer', on=['customer_id', 'article_id'])\n",
    "        \n",
    "    purchase_df['isin_recently'] = 1\n",
    "    \n",
    "    return purchase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_popular(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''販売数の多い商品から候補生成'''\n",
    "    # make_articles_featureが販売数から特徴量を作ってくれるから、この関数は候補のペアだけ返す\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query(\"week <= 4\")  # 4週分の履歴を使用\n",
    "\n",
    "    # 各週の販売数上位12個\n",
    "    dummy_count_df = df.groupby(['article_id', 'week'])['week'].count().rename('dummy_count').reset_index().copy()\n",
    "    dummy_count_df['rank_in_week'] = dummy_count_df.groupby('week')['dummy_count'].rank(method='min', ascending=False)\n",
    "    dummy_articles = dummy_count_df.query('rank_in_week <= 12')['article_id'].unique()\n",
    "\n",
    "    dummy_df = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [np.repeat(customers, repeats=len(dummy_articles)).reshape(-1, 1),\n",
    "            np.repeat(dummy_articles[None, :], repeats=len(customers), axis=0).reshape(-1, 1)],\n",
    "            axis=1),\n",
    "        columns = ['customer_id', 'article_id']\n",
    "    )\n",
    "    \n",
    "    dummy_df['isin_popular'] = 1\n",
    "    \n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_lastw(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''customer毎の最後の購入から1週間の購入履歴から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\").copy()\n",
    "\n",
    "    # 最後の購入から1週間に絞る\n",
    "    df['max_dat'] = df['customer_id'].map(df.groupby('customer_id')['t_dat'].max())\n",
    "    df['max_diff'] = (df['max_dat'] - df['t_dat']).dt.days\n",
    "    df = df.query('max_diff <= 6')\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count_lastw').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.sort_values('t_dat', ascending=True)\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    df = df.rename(columns={'t_dat': 'last_dat'})\n",
    "    df['last_dat'] = df['last_dat'].astype(np.int64) // 10 ** 9\n",
    "    \n",
    "    df['isin_lastw'] = 1\n",
    "    \n",
    "    return df[['customer_id', 'article_id', 'count_lastw', 'last_dat', 'isin_lastw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_pair(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''ルールベース協調フィルタリングから候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    \n",
    "    df = df.merge(df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index(), on=['customer_id', 'article_id'])\n",
    "    df = df.drop_duplicates(['customer_id', 'article_id'], keep='last')\n",
    "    pairs = pd.read_parquet(f'../input/hmitempairs/pairs_fold{target_week}_5items.parquet')\n",
    "    df = df.merge(pairs, on='article_id', how='inner')\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'pair_article_id': 'article_id'})\n",
    "    df['count_pair'] = df['count'] * df['pair_ratio']\n",
    "\n",
    "    df['isin_pair'] = 1\n",
    "\n",
    "    return df[['customer_id', 'article_id', 'count_pair', 'isin_pair']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates_collabo(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''コラボ商品の購入履歴から候補生成'''\n",
    "    df = pd.read_csv('../input/ranking_features/collabo_candidate.csv', dtype={'article_id': 'int32'})\n",
    "    # TODO: ほんとはスコープ外変数の参照は良くない\n",
    "    df['customer_id'] = df['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "    df = df.query('target_week == @target_week').copy()\n",
    "    \n",
    "    df['isin_collabo'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_candidates_desc_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int, encoder: str):\n",
    "    '''説明文の埋め込みベクトル（次元削減済み）から候補生成'''\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df = transactions.query(\"customer_id in @customers\").copy()\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query(\"week >= 1\")\n",
    "    df = df.query('week <= 4').copy()  # 4週分の履歴を使用\n",
    "    df = df.groupby(['customer_id', 'article_id'])['article_id'].count().rename('count').reset_index()\n",
    "    \n",
    "    desc_knn = pd.read_csv(f'../input/ranking_features/item_features_{encoder}_class.csv', dtype={'article_id': 'int32'})\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(5):\n",
    "        dfs.append(df.merge(desc_knn[['article_id', f'knn_article_id_{i}', f'knn_distance_{i}']])\n",
    "                        .rename(columns={f'knn_article_id_{i}': 'knn_article_id', f'knn_distance_{i}': f'{encoder}_distance'}))\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.drop('article_id', axis=1).rename(columns={'knn_article_id': 'article_id'})\n",
    "    df[f'{encoder}_count'] = df['count'] * df[f'{encoder}_distance']\n",
    "    \n",
    "    # # 重複を削除、item1とitem2それぞれの類似として同じitemが出てくることがある\n",
    "    # df = df.sort_values(f'{encoder}_count', ascending=True).drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "    \n",
    "    df[f'isin_{encoder}_knn'] = 1\n",
    "                   \n",
    "    return df[['customer_id', 'article_id', f'{encoder}_count', f'{encoder}_distance', f'isin_{encoder}_knn']]\n",
    "\n",
    "def generate_candidates_desc_knn(transactions: pd.DataFrame, customers: np.ndarray, target_week: int):\n",
    "    '''商品説明文から候補生成'''\n",
    "    tsne_df = _generate_candidates_desc_knn(transactions, customers, target_week, encoder='tsne')\n",
    "    # umap_df = _generate_candidates_desc_knn(transactions, customers, target_week, encoder='umap')\n",
    "    # df = pd.merge(tsne_df, umap_df, how='outer', on=['customer_id', 'article_id'])\n",
    "\n",
    "    return tsne_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_customers_feature(customers: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''customer毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    customers_feature = customers.drop(['postal_code'], axis=1).copy()\n",
    "    customers_feature.loc[~customers_feature['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None  # Noneの表記揃え\n",
    "    customers_feature[['FN', 'Active']] = customers_feature[['FN', 'Active']].fillna(0)\n",
    "\n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_purchase = df.groupby(['customer_id', 'week'])['week'].count().rename('purchase').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_purchase.groupby('customer_id')['purchase'].agg(agg_name)\n",
    "        customers_feature[f'purchase_{agg_name}_groupby_customer'] = customers_feature['customer_id'].map(agg_sr)\n",
    "    \n",
    "    # 各週の購入数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_purchase[weekly_purchase['week']==w]\n",
    "        tmp = tmp[['customer_id', 'purchase']].set_index('customer_id')['purchase']\n",
    "        customers_feature[f'purchase_{w}w'] = customers_feature['customer_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_ratio_{w}w'] = customers_feature[f'purchase_{w}w'] / customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_diff_{w}w'] = customers_feature[f'purchase_{w}w'] - customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "\n",
    "    # --- 一意の(article_id, week)を購入単位とみなす ---\n",
    "    # ※あるarticleを1個買うことを、ふつうは購入単位とみなしている\n",
    "    # rank: 何回目の購入か\n",
    "    unique_transactions = df[['customer_id', 'article_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['customer_id', 'article_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再購入したarticleの割合\n",
    "    customers_feature['repurchase_article'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article'] = customers_feature['customer_id'].map(unique_transactions.drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count())\n",
    "    customers_feature['repurchase_article_percent'] = customers_feature['repurchase_article'] / customers_feature['purchase_article']\n",
    "\n",
    "    # 再購入を含む週の割合\n",
    "    customers_feature['repurchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count()).fillna(0)\n",
    "    customers_feature['purchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count())\n",
    "    customers_feature['repurchase_week_percent'] = customers_feature['repurchase_week'] / customers_feature['purchase_week']\n",
    "    \n",
    "    # 再購入の割合\n",
    "    customers_feature['repurchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('customer_id')['customer_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.groupby('customer_id')['customer_id'].count())\n",
    "    customers_feature['repurchase_article_and_week_percent'] = customers_feature['repurchase_article_and_week'] / customers_feature['purchase_article_and_week']\n",
    "    # --- おわり ---\n",
    "    \n",
    "    return customers_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_articles_feature(articles: pd.DataFrame, transactions: pd.DataFrame, target_week: int, debug: bool = False):\n",
    "    '''article毎に特徴量エンジニアリング'''\n",
    "    df = transactions.copy()\n",
    "    articles_feature = articles.drop(\n",
    "        ['prod_name', 'product_type_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'prod_name', 'department_name', 'detail_desc'], \n",
    "        axis=1).copy()\n",
    "    \n",
    "    # week列のシフト(week=target_week => week=0)、リーク防止\n",
    "    df['week'] = df['week'] - target_week\n",
    "    df = df.query('week >= 1')\n",
    "    \n",
    "    if debug == True:\n",
    "        df = df.query('week <= 24')\n",
    "\n",
    "    weekly_sale = df.groupby(['article_id', 'week'])['week'].count().rename('sale').reset_index()\n",
    "    \n",
    "    # 統計量で特徴量\n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_sale.groupby('article_id')['sale'].agg(agg_name)\n",
    "        articles_feature[f'sale_{agg_name}_groupby_article'] = articles_feature['article_id'].map(agg_sr)\n",
    "\n",
    "    # 各週の販売数、統計量との差、比で特徴量\n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_sale[weekly_sale['week']==w]\n",
    "        tmp = tmp[['article_id', 'sale']].set_index('article_id')['sale']\n",
    "        articles_feature[f'sale_{w}w'] = articles_feature['article_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_ratio_{w}w'] = articles_feature[f'sale_{w}w'] / articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_diff_{w}w'] = articles_feature[f'sale_{w}w'] - articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "\n",
    "    # --- 一意の(customer_id, week)を販売単位とみなす ---\n",
    "    # ※あるcustomerに1つ売れることを、ふつうは販売単位とみなしている\n",
    "    # rank: 何回目の販売か\n",
    "    unique_transactions = df[['article_id', 'customer_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['article_id', 'customer_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    # 再販売したcustomerの割合\n",
    "    articles_feature['resale_customer'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer'] = articles_feature['article_id'].map(unique_transactions.drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count())\n",
    "    articles_feature['resale_customer_percent'] = articles_feature['resale_customer'] / articles_feature['sale_customer']\n",
    "\n",
    "    # 再販売を含む週の割合\n",
    "    articles_feature['resale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count()).fillna(0)\n",
    "    articles_feature['sale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count())\n",
    "    articles_feature['resale_week_percent'] = articles_feature['resale_week'] / articles_feature['sale_week']\n",
    "\n",
    "    # 再販売の割合\n",
    "    articles_feature['resale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('article_id')['article_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.groupby('article_id')['article_id'].count())\n",
    "    articles_feature['resale_customer_and_week_percent'] = articles_feature['resale_customer_and_week'] / articles_feature['sale_customer_and_week']\n",
    "    # --- 終わり ---\n",
    "    \n",
    "    return articles_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_df(transactions: pd.DataFrame, week: int, is_labeled: bool, use_customers: np.ndarray = None, metric_verbose: bool = True, compress_verbose: bool = False):\n",
    "    '''ランク学習モデルへの入力データ作成'''\n",
    "    # WARNING: 戦略を追加した時に書き換え忘れ注意！\n",
    "    strategy_flags = ['isin_trending', 'isin_recently', 'isin_popular', 'isin_lastw', 'isin_pair', 'isin_collabo', 'isin_tsne_knn']\n",
    "    kwargs = {'how': 'outer', 'on': ['customer_id', 'article_id'], 'copy': True}\n",
    "    \n",
    "    # 入力データに含むcustomersの絞り込み\n",
    "    if use_customers is not None:\n",
    "        data_customers = use_customers\n",
    "    elif week >= 0:\n",
    "        data_customers = transactions.query(\"week == @week\")['customer_id'].unique()\n",
    "    else:\n",
    "        raise ValueError('set use_customers as something when week=-1.')\n",
    "    \n",
    "    # 候補作り\n",
    "    data_candidates_trending = generate_candidates_trending(transactions, data_customers, week)\n",
    "    data_candidates_recently = generate_candidates_recently(transactions, data_customers, week)\n",
    "    data_candidates_popular = generate_candidates_popular(transactions, data_customers, week)    \n",
    "    data_candidates_lastw = generate_candidates_lastw(transactions, data_customers, week)\n",
    "    data_candidates_pair = generate_candidates_pair(transactions, data_customers, week)\n",
    "    data_candidates_collabo = generate_candidates_collabo(transactions, data_customers, week)\n",
    "    data_candidates_desc_knn = generate_candidates_desc_knn(transactions, data_customers, week)\n",
    "    \n",
    "    data_df = data_candidates_trending.merge(\n",
    "        data_candidates_recently.merge(\n",
    "        data_candidates_popular.merge(\n",
    "        data_candidates_lastw.merge(\n",
    "        data_candidates_pair.merge(\n",
    "        data_candidates_collabo.merge(\n",
    "        data_candidates_desc_knn, **kwargs), **kwargs), **kwargs), **kwargs), **kwargs), **kwargs)\n",
    "    \n",
    "    data_df[strategy_flags] = data_df[strategy_flags].fillna(0)\n",
    "    \n",
    "    # 正解ラベル付け\n",
    "    if is_labeled:\n",
    "        if week < 0:\n",
    "            raise ValueError(f\"can't label when week={week}.\")\n",
    "        data_actual = transactions.query(\"week == @week\")[['customer_id', 'article_id']].drop_duplicates()\n",
    "        data_actual['label'] = 1\n",
    "        data_df = data_df.merge(data_actual, how='left', on=['customer_id', 'article_id'])\n",
    "        data_df['label'] = data_df['label'].fillna(0)\n",
    "    \n",
    "    # customers, articles の特徴量\n",
    "    data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    data_customers_feature = compress_df(make_customers_feature(customers, transactions, target_week=week, debug=True), verbose=compress_verbose)\n",
    "    data_articles_feature = compress_df(make_articles_feature(articles, transactions, target_week=week, debug=True), verbose=compress_verbose)\n",
    "    data_df = data_df.merge(data_customers_feature, how='left', on=['customer_id'], copy=True)\n",
    "    data_df = data_df.merge(data_articles_feature, how='left', on=['article_id'], copy=True)\n",
    "    # data_df = compress_df(data_df, verbose=compress_verbose)\n",
    "    \n",
    "    # 候補のスコア\n",
    "    if metric_verbose:\n",
    "        print(f\"[Info] shape     : {data_df.shape}\")\n",
    "        print(f\"[Info] mem       : {data_df.memory_usage().sum() / 1024**2 :5.2f} Mb\")\n",
    "        print(f\"[Info] candidates: {len(data_df) / len(data_customers):.1f} 個 / customer\")\n",
    "        display(data_df[strategy_flags].astype(float).mean())\n",
    "        if is_labeled:\n",
    "            print(f\"[Info] Precision: {data_df['label'].sum() / len(data_df):.5f}\")\n",
    "            print(f\"[Info] Recall   : {data_df['label'].sum() / len(data_actual):.5f}\")\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_df(\n",
    "    df: pd.DataFrame, \n",
    "    category_columns: list =['club_member_status', 'fashion_news_frequency', 'product_group_name', 'index_code'], \n",
    "    verbose: bool =True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    '''DataFrameのデータ型を適切に選び圧縮'''\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        bar = tqdm(df.columns, leave=False)\n",
    "    else:\n",
    "        bar = df.columns\n",
    "    for col in bar:\n",
    "        col_type = df[col].dtypes\n",
    "        if col in category_columns:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(category)\")\n",
    "            df[col] = df[col].astype('category')\n",
    "        elif col_type in numerics:\n",
    "            if verbose:\n",
    "                bar.set_description(f\"{col}(num)\")\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params: dict, cols: list, tr_df: pd.DataFrame, val_df: pd.DataFrame = None, early_stopping: bool = True):\n",
    "    if val_df is None:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dtrain], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    else:\n",
    "        tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "        val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "        train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "        dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "        dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "        if early_stopping:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.early_stopping(10, first_metric_only=True), lgb.log_evaluation(10)])\n",
    "        else:\n",
    "            model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_df: pd.DataFrame, model_folds: list):\n",
    "    pred = np.zeros(len(test_df))\n",
    "    for w in model_folds:\n",
    "        with open(f\"../models/lgb_rank/{EXP}_model_fold{w}.pkl\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        pred += model.predict(test_df[cols], num_iteration=model.best_iteration)    \n",
    "    pred = pred/len(model_folds)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_sr(test_df: pd.DataFrame, pred: np.ndarray):\n",
    "    '''モデルのスコアを用いて並び替え＆上位12個抽出'''\n",
    "    test_df['predict_score'] = pred\n",
    "    test_df = test_df.sort_values('predict_score', ascending=False).drop_duplicates(['customer_id', 'article_id'], keep='first').reset_index(drop=True)\n",
    "    test_df['rank'] = test_df.groupby('customer_id')['predict_score'].rank('min', ascending=False)\n",
    "    test_df = test_df[test_df['rank'] <= 12]\n",
    "    \n",
    "    # test_df['article_id'] = le.inverse_transform(test_df['article_id'])\n",
    "    test_df['article_id'] = ' 0' + test_df['article_id'].astype(str)\n",
    "    \n",
    "    top_sr = test_df.groupby('customer_id')['article_id'].sum()\n",
    "    \n",
    "    return top_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optunaによるチューニング\n",
    "# import optuna.integration.lightgbm as lgb_optuna\n",
    "# from optuna.logging import set_verbosity\n",
    "# import numpy as np\n",
    "# import random as rn\n",
    "\n",
    "# set_verbosity(-1)\n",
    "# np.random.seed(71)\n",
    "# rn.seed(71)\n",
    "\n",
    "# params = {\n",
    "#     'objective': 'lambdarank',\n",
    "#     'boosting': 'gbdt',  # default: 'gbdt', 'gbdt' or 'dart'\n",
    "#     'num_iterations': 1000,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'metric': ['ndcg'],\n",
    "#     'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "#     'random_state': 71,  # 訓練用とは違う値にする\n",
    "#     'verbosity': -1,  # -1: ignore, 0: warnings, 1: info\n",
    "#     'deterministic': True,  # 再現性確保\n",
    "#     'force_row_wise': True  # 再現性確保\n",
    "# }\n",
    "\n",
    "# tr_df = make_data_df(transactions, 2, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "# cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "# tr_df = tr_df.sort_values('customer_id').reset_index(drop=True)\n",
    "# train_query = tr_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dtrain = lgb.Dataset(tr_df[cols], label=tr_df['label'], group=train_query)\n",
    "\n",
    "# val_df = make_data_df(transactions, 1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "# val_df = val_df.sort_values('customer_id').reset_index(drop=True)    \n",
    "# val_query = val_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "# dval = lgb.Dataset(val_df[cols], reference=dtrain, label=val_df['label'], group=val_query)\n",
    "\n",
    "# model = lgb_optuna.LightGBMTuner(params, dtrain, valid_sets=[dtrain, dval], early_stopping_rounds=10, verbose_eval=-1, optuna_seed=71)\n",
    "# model.run()\n",
    "\n",
    "# best_params = model.params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "params = {\n",
    "    # Core Parameters\n",
    "    'objective': 'lambdarank',\n",
    "    'boosting': 'gbdt',  # default: 'gbdt', ['gbdt', 'dart', 'goss'], dart: 超遅いけど高精度\n",
    "    'num_iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,  # default: 31, large for accuracy\n",
    "    'num_threads': cpu_count(logical=False),\n",
    "    'random_state': 41,\n",
    "\n",
    "    # Learning Control Parameters\n",
    "    'force_col_wise': True,\n",
    "    # 'histogram_pool_size': -1.0,  # default: -1.0, max cache size in MB for historical histogram, (histogram_pool_size + dataset size) = approximately RAM used\n",
    "    'min_data_in_leaf': 20,  # default: 20, large dealing with over-fitting\n",
    "    'min_sum_hessian_in_leaf': 1e-3,  # default: 1e-3, large dealing with over-fitting\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    # 'drop_rate': 0.1,  # used only in dart, a fraction of previous trees to drop during the dropout\n",
    "    'verbosity': 0,  # 0: warnings, 1: info\n",
    "\n",
    "    # Dataset Parameters\n",
    "    'max_bin': 255,  # default: 255, large for accuracy\n",
    "    'min_data_in_bin': 3,  # default: 3, \n",
    "    'bin_construct_sample_cnt': 100000,  # default: 200000, larger for accuracy\n",
    "\n",
    "    # Objective Parameters\n",
    "    'lambdarank_truncation_level': 30,\n",
    "    'lambdarank_norm': True,\n",
    "    # 'label_gain': [0, 1],\n",
    "\n",
    "    # Metric Parameters\n",
    "    'metric': ['ndcg'],\n",
    "    'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec92f72a0a88409ea4a39e24739dc4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_week(fold): 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 441.07 Mb (69.7% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 614.96 Mb (74.9% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 48.62 Mb (75.0% reduction)\n",
      "[Info] shape     : (5781201, 507)\n",
      "[Info] mem       : 5651.22 Mb\n",
      "[Info] candidates: 80.3 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.205088\n",
       "isin_recently    0.105854\n",
       "isin_popular     0.547571\n",
       "isin_lastw       0.046787\n",
       "isin_pair        0.161185\n",
       "isin_collabo     0.000246\n",
       "isin_tsne_knn    0.163490\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00356\n",
      "[Info] Recall   : 0.08921\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.868465\tvalid_1's ndcg@12: 0.871941\n",
      "[20]\ttraining's ndcg@12: 0.87082\tvalid_1's ndcg@12: 0.873854\n",
      "[30]\ttraining's ndcg@12: 0.872423\tvalid_1's ndcg@12: 0.874208\n",
      "[40]\ttraining's ndcg@12: 0.874025\tvalid_1's ndcg@12: 0.8745\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's ndcg@12: 0.873457\tvalid_1's ndcg@12: 0.874531\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 3\n",
      "[Info] shape     : (6454141, 507)\n",
      "[Info] mem       : 6309.03 Mb\n",
      "[Info] candidates: 80.4 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.202393\n",
       "isin_recently    0.099756\n",
       "isin_popular     0.568036\n",
       "isin_lastw       0.045232\n",
       "isin_pair        0.146941\n",
       "isin_collabo     0.000152\n",
       "isin_tsne_knn    0.149091\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00317\n",
      "[Info] Recall   : 0.08013\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.882237\tvalid_1's ndcg@12: 0.8709\n",
      "[20]\ttraining's ndcg@12: 0.884391\tvalid_1's ndcg@12: 0.871225\n",
      "[30]\ttraining's ndcg@12: 0.88595\tvalid_1's ndcg@12: 0.8723\n",
      "[40]\ttraining's ndcg@12: 0.887227\tvalid_1's ndcg@12: 0.872137\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's ndcg@12: 0.88595\tvalid_1's ndcg@12: 0.8723\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 2\n",
      "[Info] shape     : (5708460, 507)\n",
      "[Info] mem       : 5580.11 Mb\n",
      "[Info] candidates: 75.3 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.204172\n",
       "isin_recently    0.106291\n",
       "isin_popular     0.535408\n",
       "isin_lastw       0.047812\n",
       "isin_pair        0.155050\n",
       "isin_collabo     0.000130\n",
       "isin_tsne_knn    0.157320\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00369\n",
      "[Info] Recall   : 0.08844\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.877814\tvalid_1's ndcg@12: 0.872805\n",
      "[20]\ttraining's ndcg@12: 0.879547\tvalid_1's ndcg@12: 0.873233\n",
      "[30]\ttraining's ndcg@12: 0.881665\tvalid_1's ndcg@12: 0.873786\n",
      "[40]\ttraining's ndcg@12: 0.883102\tvalid_1's ndcg@12: 0.873988\n",
      "[50]\ttraining's ndcg@12: 0.884785\tvalid_1's ndcg@12: 0.873854\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's ndcg@12: 0.883315\tvalid_1's ndcg@12: 0.874094\n",
      "Evaluated only: ndcg@12\n",
      "\n",
      "target_week(fold): 1\n",
      "[Info] shape     : (4855011, 507)\n",
      "[Info] mem       : 4745.85 Mb\n",
      "[Info] candidates: 67.4 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.224767\n",
       "isin_recently    0.120649\n",
       "isin_popular     0.482560\n",
       "isin_lastw       0.053342\n",
       "isin_pair        0.175735\n",
       "isin_collabo     0.000133\n",
       "isin_tsne_knn    0.178294\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00405\n",
      "[Info] Recall   : 0.08621\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's ndcg@12: 0.878876\tvalid_1's ndcg@12: 0.85379\n",
      "[20]\ttraining's ndcg@12: 0.88171\tvalid_1's ndcg@12: 0.855048\n",
      "[30]\ttraining's ndcg@12: 0.883735\tvalid_1's ndcg@12: 0.855097\n",
      "[40]\ttraining's ndcg@12: 0.885465\tvalid_1's ndcg@12: 0.855455\n",
      "[50]\ttraining's ndcg@12: 0.886805\tvalid_1's ndcg@12: 0.855472\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's ndcg@12: 0.886171\tvalid_1's ndcg@12: 0.855642\n",
      "Evaluated only: ndcg@12\n"
     ]
    }
   ],
   "source": [
    "# train lgb ranker\n",
    "best_iterations = []\n",
    "feature_importance_dfs = []\n",
    "oof_weeks = [4, 3, 2, 1]\n",
    "\n",
    "for i, w in enumerate(tqdm(oof_weeks)):\n",
    "    print(f\"\\ntarget_week(fold): {w}\")\n",
    "    if i == 0:\n",
    "        compress_verbose=True\n",
    "    else:\n",
    "        compress_verbose=False\n",
    "    \n",
    "    tr_df = make_data_df(transactions, w, is_labeled=True, metric_verbose=True, compress_verbose=compress_verbose)\n",
    "    val_df = make_data_df(transactions, w-1, is_labeled=True, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    if i == 0:\n",
    "        exclude_columns = ['target_week', 'customer_id', 'article_id', 'label']\n",
    "        cols = [c for c in tr_df.columns.tolist() if c not in exclude_columns]\n",
    "        with open(f'../models/lgb_rank/{EXP}_cols.pkl', 'wb') as f:\n",
    "            pickle.dump(cols, f)\n",
    "\n",
    "    model = train(params, cols, tr_df, val_df)\n",
    "    with open(f'../models/lgb_rank/{EXP}_model_fold{w}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    best_iterations.append(model.best_iteration)\n",
    "    feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': w}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] shape     : (4613362, 506)\n",
      "[Info] mem       : 4500.84 Mb\n",
      "[Info] candidates: 66.9 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.207565\n",
       "isin_recently    0.111548\n",
       "isin_popular     0.499141\n",
       "isin_lastw       0.052680\n",
       "isin_pair        0.174341\n",
       "isin_collabo     0.000113\n",
       "isin_tsne_knn    0.176892\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.66625116 -3.66415851 -3.66212379 ...  2.42560599  2.42560599\n",
      "  2.42781042]\n"
     ]
    }
   ],
   "source": [
    "# predict val data\n",
    "val_df = make_data_df(transactions, 0, is_labeled=False, metric_verbose=True, compress_verbose=False)\n",
    "val_pred = predict(val_df, oof_weeks)\n",
    "print(np.sort(val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "80      0671607001 0436261001 0673677002 0754751001 0...\n",
       "86      0889036004 0621381012 0640021012 0880017001 0...\n",
       "107     0556255001 0399136061 0732842014 0732842021 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val top rank articles\n",
    "val_df2 = val_df.copy()\n",
    "val_pred_sr = extract_top_sr(val_df2, val_pred)\n",
    "display(val_pred_sr.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 popular items:\n",
      " 0909370001 0865799006 0918522001 0924243001 0448509014 0751471001 0809238001 0918292001 0762846027 0809238005 0673677002 0923758001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0     0685814003 0448509014 0918522001 0715624001 0...\n",
       "1.0     0909370001 0865799006 0924243001 0809238001 0...\n",
       "2.0     0909370001 0865799006 0918525001 0909371001 0...\n",
       "3.0     0909370001 0751471001 0673677002 0910601003 0...\n",
       "4.0     0918522001 0751471001 0751471043 0910601003 0...\n",
       "5.0     0918522001 0908799002 0896152002 0924243001 0...\n",
       "6.0     0736870001 0796210001 0908799002 0865799006 0...\n",
       "Name: top_12_popular_items, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 1]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "display(popular_items_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_lgb</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>prediction_popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 07...</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>0909370001 0751471001 0673677002 0910601003 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 04...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>0909370001 0865799006 0924243001 0809238001 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "\n",
       "                                          prediction prediction_lgb  age_bin  \\\n",
       "0  0909370001 0751471001 0673677002 0910601003 07...                     3.0   \n",
       "1  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "2  0909370001 0865799006 0924243001 0809238001 04...                     1.0   \n",
       "\n",
       "                                  prediction_popular  \n",
       "0   0909370001 0751471001 0673677002 0910601003 0...  \n",
       "1   0909370001 0865799006 0924243001 0809238001 0...  \n",
       "2   0909370001 0865799006 0924243001 0809238001 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(val_pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission_fold0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2623"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del val_df, model\n",
    "del transactions_last_week, top12, popular_items, popular_items_dict, popular_items_sr\n",
    "del val_df2, val_pred_sr, \n",
    "del submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] shape     : (4613362, 508)\n",
      "[Info] mem       : 4518.44 Mb\n",
      "[Info] candidates: 66.9 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.207565\n",
       "isin_recently    0.111548\n",
       "isin_popular     0.499141\n",
       "isin_lastw       0.052680\n",
       "isin_pair        0.174341\n",
       "isin_collabo     0.000113\n",
       "isin_tsne_knn    0.176892\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Precision: 0.00446\n",
      "[Info] Recall   : 0.09620\n",
      "[10]\ttraining's ndcg@12: 0.862679\n",
      "[20]\ttraining's ndcg@12: 0.865209\n",
      "[30]\ttraining's ndcg@12: 0.867369\n"
     ]
    }
   ],
   "source": [
    "# train last target_week(=0) data\n",
    "tr_df = make_data_df(transactions, week=0, is_labeled=True, metric_verbose=True, compress_verbose=False)\n",
    "\n",
    "# valがないのでアーリーストッピングが使えない\n",
    "params['num_iterations'] = int(np.mean(best_iterations))\n",
    "model = train(params, cols, tr_df)\n",
    "with open(f\"../models/lgb_rank/{EXP}_model_fold0.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "feature_importance_dfs.append(pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain'), 'fold': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance(gain)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trending_value</th>\n",
       "      <td>28409.510140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_dat</th>\n",
       "      <td>14114.713368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_max_groupby_article_ratio_1w</th>\n",
       "      <td>12432.757828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_pair</th>\n",
       "      <td>7407.013671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_mean_groupby_article_ratio_1w</th>\n",
       "      <td>6923.803967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_1w</th>\n",
       "      <td>3583.856174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_lastw</th>\n",
       "      <td>3155.977349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_sum_groupby_customer_ratio_1w</th>\n",
       "      <td>2653.208380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_code</th>\n",
       "      <td>2277.600321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_mean_groupby_customer_ratio_1w</th>\n",
       "      <td>2185.265988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_min_groupby_article_diff_1w</th>\n",
       "      <td>2162.597136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_group_name</th>\n",
       "      <td>2082.850734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>2082.629621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resale_customer_and_week_percent</th>\n",
       "      <td>1940.812562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_1w</th>\n",
       "      <td>1924.432976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_sum_groupby_article_ratio_2w</th>\n",
       "      <td>1838.628701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_max_groupby_customer_ratio_1w</th>\n",
       "      <td>1732.298605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase_article</th>\n",
       "      <td>1641.292754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resale_customer_percent</th>\n",
       "      <td>1231.175442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale_mean_groupby_article_diff_1w</th>\n",
       "      <td>1132.714013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         importance(gain)\n",
       "feature                                                  \n",
       "trending_value                               28409.510140\n",
       "last_dat                                     14114.713368\n",
       "sale_max_groupby_article_ratio_1w            12432.757828\n",
       "count_pair                                    7407.013671\n",
       "sale_mean_groupby_article_ratio_1w            6923.803967\n",
       "sale_1w                                       3583.856174\n",
       "count_lastw                                   3155.977349\n",
       "purchase_sum_groupby_customer_ratio_1w        2653.208380\n",
       "product_code                                  2277.600321\n",
       "purchase_mean_groupby_customer_ratio_1w       2185.265988\n",
       "sale_min_groupby_article_diff_1w              2162.597136\n",
       "product_group_name                            2082.850734\n",
       "age                                           2082.629621\n",
       "resale_customer_and_week_percent              1940.812562\n",
       "purchase_1w                                   1924.432976\n",
       "sale_sum_groupby_article_ratio_2w             1838.628701\n",
       "purchase_max_groupby_customer_ratio_1w        1732.298605\n",
       "purchase_article                              1641.292754\n",
       "resale_customer_percent                       1231.175442\n",
       "sale_mean_groupby_article_diff_1w             1132.714013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance_df = pd.concat(feature_importance_dfs, ignore_index=True, axis=0)\n",
    "display(feature_importance_df.groupby(['feature'])[['importance(gain)']].mean().sort_values('importance(gain)', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2904"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tr_df, params, model, best_iterations, feature_importance_dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129485b560254d63b5e2781f5e427b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] shape     : (4178384, 507)\n",
      "[Info] mem       : 4180.07 Mb\n",
      "[Info] candidates: 41.8 個 / customer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isin_trending    0.087299\n",
       "isin_recently    0.054201\n",
       "isin_popular     0.726738\n",
       "isin_lastw       0.081101\n",
       "isin_pair        0.081241\n",
       "isin_collabo     0.000000\n",
       "isin_tsne_knn    0.082299\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "0     0568601043 0568601006 0858856005 0779781015 0...\n",
       "1     0915529005 0924243002 0866731001 0924243001 0...\n",
       "2     0794321007 0805000001 0791587001 0924243002 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict test data\n",
    "BATCH_SIZE = 100_000\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "test_customers = submission['customer_id'].map(id_to_index_dict).unique()\n",
    "\n",
    "# バッチ処理\n",
    "def process(i):\n",
    "    if i == (len(test_customers)//BATCH_SIZE):\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : ]\n",
    "    else:\n",
    "        test_customers_batch = test_customers[i*BATCH_SIZE : (i+1)*BATCH_SIZE]\n",
    "    \n",
    "    if i == 0:  # meric_verbose=True\n",
    "        test_df = make_data_df(transactions, week=-1, is_labeled=False, use_customers=test_customers_batch, metric_verbose=True, compress_verbose=False)\n",
    "    else:       # metric_verbose=False\n",
    "        test_df = make_data_df(transactions, week=-1, is_labeled=False, use_customers=test_customers_batch, metric_verbose=False, compress_verbose=False)\n",
    "\n",
    "    all_weeks = oof_weeks + [0]\n",
    "    pred = predict(test_df, all_weeks)\n",
    "    return extract_top_sr(test_df, pred)\n",
    "\n",
    "# single process execution\n",
    "preds = []\n",
    "for i in tqdm(range(len(test_customers)//BATCH_SIZE + 1)):\n",
    "    preds.append(process(i))\n",
    "\n",
    "# # multi process execution\n",
    "# # cpus = cpu_count(logical=False)\n",
    "# cpus = 4\n",
    "# print('cpu(core): ', cpus)\n",
    "# preds = Parallel(n_jobs=cpus, verbose=0)( [delayed(process)(i) for i in range(len(test_customers)//BATCH_SIZE + 1)] )\n",
    "\n",
    "pred_sr = pd.concat(preds, axis=0)\n",
    "display(pred_sr.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16557"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # most popular items\n",
    "# transactions_last_week = transactions.loc[transactions.week == 0]\n",
    "# top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "# print(\"Top 12 popular items:\")\n",
    "# print( top12 )\n",
    "\n",
    "# customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "# transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "# popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "# popular_items_dict = {}\n",
    "# for index in popular_items.index.levels[0]:\n",
    "#     popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "# popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "# popular_items_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0568601043 0568601006 0858856005 0779781015 05...</td>\n",
       "      <td>0568601043 0568601006 0858856005 0779781015 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0915529005 0924243002 0866731001 0924243001 06...</td>\n",
       "      <td>0915529005 0924243002 0866731001 0924243001 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0794321007 0805000001 0791587001 0924243002 09...</td>\n",
       "      <td>0794321007 0805000001 0791587001 0924243002 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "\n",
       "                                          prediction  \\\n",
       "0  0568601043 0568601006 0858856005 0779781015 05...   \n",
       "1  0915529005 0924243002 0866731001 0924243001 06...   \n",
       "2  0794321007 0805000001 0791587001 0924243002 09...   \n",
       "\n",
       "                                      prediction_lgb  \n",
       "0   0568601043 0568601006 0858856005 0779781015 0...  \n",
       "1   0915529005 0924243002 0866731001 0924243001 0...  \n",
       "2   0794321007 0805000001 0791587001 0924243002 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test sub\n",
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "# submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "# submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "# submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "display(submission.head(3))\n",
    "submission[['customer_id', 'prediction']].to_csv(f'../submissions/{EXP}_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f804528fa1bcda80b2057737773baa2134c5be7e013153f88e69abab752260b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
