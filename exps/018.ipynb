{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **colab008に移した**\n",
    "# ランク学習\n",
    "# tiobfをベースにbucketsを作る\n",
    "# New: 特徴量エンジニアリング\n",
    "# MAP@12 (all): 0.027606\n",
    "# MAP@12 (cold start): 0.008750\n",
    "\n",
    "EXP = '018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "warnings.simplefilter('ignore', pd.errors.PerformanceWarning)\n",
    "data_path = Path('../input/h-and-m-personalized-fashion-recommendations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>t_diff</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31788319</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>929511001</td>\n",
       "      <td>0.059305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788320</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371691</td>\n",
       "      <td>891322004</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788321</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371721</td>\n",
       "      <td>918325001</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788322</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371747</td>\n",
       "      <td>833459002</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788323</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>1371960</td>\n",
       "      <td>898573003</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_dat  customer_id  article_id     price  sales_channel_id  \\\n",
       "31788319 2020-09-22      1371691   929511001  0.059305                 2   \n",
       "31788320 2020-09-22      1371691   891322004  0.042356                 2   \n",
       "31788321 2020-09-22      1371721   918325001  0.043203                 1   \n",
       "31788322 2020-09-22      1371747   833459002  0.006763                 1   \n",
       "31788323 2020-09-22      1371960   898573003  0.033881                 2   \n",
       "\n",
       "          t_diff  week  \n",
       "31788319       0     0  \n",
       "31788320       0     0  \n",
       "31788321       0     0  \n",
       "31788322       0     0  \n",
       "31788323       0     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = pd.read_csv(\n",
    "    data_path / f'transactions_train.csv',\n",
    "    # set dtype or pandas will drop the leading '0' and convert to int\n",
    "    dtype={'article_id': 'int32'},\n",
    "    parse_dates=['t_dat'])\n",
    "customers = pd.read_csv(data_path / 'customers.csv')\n",
    "articles = pd.read_csv(\n",
    "    '../input/h-and-m-personalized-fashion-recommendations/articles.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "\n",
    "t_max = transactions['t_dat'].max()\n",
    "transactions['t_diff'] = (t_max - transactions['t_dat']).dt.days\n",
    "transactions['week'] = transactions['t_diff'] // 7\n",
    "\n",
    "customers.loc[~customers['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "\n",
    "id_to_index_dict = dict(zip(customers[\"customer_id\"], customers.index))\n",
    "index_to_id_dict = dict(zip(customers.index, customers[\"customer_id\"]))\n",
    "transactions[\"customer_id\"] = transactions[\"customer_id\"].map(id_to_index_dict).astype('int32')\n",
    "customers['customer_id'] = customers['customer_id'].map(id_to_index_dict).astype('int32')\n",
    "\n",
    "print(transactions.shape)\n",
    "transactions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_purchase_df(transactions: pd.DataFrame, phase: str, debug: bool = False):\n",
    "    df = transactions.copy()\n",
    "\n",
    "    if phase == 'train':\n",
    "        labels = transactions.query(\"week == 1\")[['customer_id', 'article_id']].drop_duplicates().copy()\n",
    "        labels['is_purchased'] = 1\n",
    "        df = df.query('week >= 2')\n",
    "        # target_weekにラベル1（購入）が一つもないユーザは除く\n",
    "        df = df.query(\"customer_id in @labels['customer_id'].unique()\")\n",
    "        df = df.copy()\n",
    "        df['week'] = df['week'] - 2\n",
    "    elif phase == 'val':\n",
    "        labels = transactions.query(\"week == 0\")[['customer_id', 'article_id']].drop_duplicates().copy()\n",
    "        labels['is_purchased'] = 1\n",
    "        df = df.query('week >= 1')\n",
    "        # target_weekにラベル1（購入）が一つもないユーザは除く\n",
    "        df = df.query(\"customer_id in @labels['customer_id'].unique()\")\n",
    "        df = df.copy()\n",
    "        df['week'] = df['week'] - 1\n",
    "    elif phase == 'test':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"phase must be 'train', 'val', or 'test'\")\n",
    "\n",
    "    if debug == True:\n",
    "        # デバッグ時は4週間分だけ使う\n",
    "        df = df.query('week < 4')\n",
    "    \n",
    "    if phase == 'train' or phase == 'val':\n",
    "        use_customers = np.intersect1d(df['customer_id'].unique(), labels['customer_id'].unique())\n",
    "    elif phase == 'test':\n",
    "        use_customers = df['customer_id'].unique()\n",
    "    else:\n",
    "        raise ValueError(\"phase must be 'train', 'val', or 'test'\")\n",
    "\n",
    "    dummy_count_df = df.groupby(['article_id', 'week'])['week'].count().rename('dummy_count').reset_index().copy()\n",
    "    dummy_count_df['rank_in_week'] = dummy_count_df.groupby('week')['dummy_count'].rank(method='min', ascending=False)\n",
    "    dummy_articles = dummy_count_df.query('rank_in_week <= 12')['article_id'].unique()\n",
    "    dummy_count_df = dummy_count_df[dummy_count_df['article_id'].isin(dummy_articles)]\n",
    "    \n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = df.query('week == @w').groupby(['customer_id', 'article_id'])['article_id'].count().rename(f'count_{w}w').reset_index().copy()\n",
    "        tmp_dummy = dummy_count_df.query('week == @w')[['article_id', 'dummy_count']].rename(columns={'dummy_count': f'count_{w}w'})\n",
    "        if w == 0:\n",
    "            purchase_df = tmp\n",
    "            dummy_df = tmp_dummy\n",
    "            continue\n",
    "        purchase_df = purchase_df.merge(tmp, how='outer', on=['customer_id', 'article_id'])\n",
    "        dummy_df = dummy_df.merge(tmp_dummy, how='outer', on=['article_id'])\n",
    "\n",
    "    del df, dummy_count_df, dummy_articles, tmp, tmp_dummy\n",
    "    gc.collect()\n",
    "\n",
    "    dummy_df = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [np.repeat(use_customers, repeats=len(dummy_df)).reshape(-1, 1),\n",
    "            np.repeat(np.expand_dims(dummy_df.copy().to_numpy(), axis=0), axis=0, repeats=len(use_customers)).reshape(-1, dummy_df.shape[1])],\n",
    "            axis=-1),\n",
    "        columns = ['customer_id'] + list(dummy_df.columns),\n",
    "    )\n",
    "    dummy_df = dummy_df.astype({'customer_id': 'int32', 'article_id': 'int32'})\n",
    "\n",
    "    purchase_df['is_dummy'] = 0\n",
    "    dummy_df['is_dummy'] = 1\n",
    "\n",
    "    # print(purchase_df.shape)\n",
    "    # print(f\"{purchase_df.__sizeof__() // 1_000_000} MB\")\n",
    "    # display(purchase_df.head())\n",
    "    # print(dummy_df.shape)\n",
    "    # print(f\"{dummy_df.__sizeof__() // 1_000_000} MB\")\n",
    "    # display(dummy_df.head())\n",
    "\n",
    "    purchase_df = pd.concat([purchase_df, dummy_df], axis=0)\n",
    "    purchase_df = purchase_df.sort_values('customer_id').reset_index(drop=True)\n",
    "\n",
    "    if phase == 'train' or phase == 'val':\n",
    "        purchase_df = purchase_df.merge(labels, how='left', on=['customer_id', 'article_id'])\n",
    "        purchase_df['is_purchased'] = purchase_df['is_purchased'].fillna(0)\n",
    "        \n",
    "        return purchase_df\n",
    "    else:\n",
    "        return purchase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchase_df = make_purchase_df(transactions, phase='train', debug=True)\n",
    "val_purchase_df = make_purchase_df(transactions, phase='val', debug=True)\n",
    "print('saving...')\n",
    "train_purchase_df.to_csv(f'../input/ranking_features/train_purchase_df.csv', index=False)\n",
    "val_purchase_df.to_csv(f'../input/ranking_features/val_purchase_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchase_df = pd.read_csv(\n",
    "    f'../input/ranking_features/train_purchase_df.csv', \n",
    "    dtype={'customer_id': 'int32', 'article_id': 'int32'})\n",
    "print('全て欠損値の行（バグ）：', train_purchase_df.drop(['customer_id', 'article_id', 'is_purchased'], axis=1).isna().all(axis=1).sum())\n",
    "print(train_purchase_df.shape)\n",
    "print(f\"{train_purchase_df.__sizeof__() // 1_000_000} MB\")\n",
    "train_purchase_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_purchase_df = pd.read_csv(\n",
    "    f'../input/ranking_features/val_purchase_df.csv', \n",
    "    dtype={'customer_id': 'int32', 'article_id': 'int32'})\n",
    "print('全て欠損値の行（バグ）：', val_purchase_df.drop(['customer_id', 'article_id', 'is_purchased'], axis=1).isna().all(axis=1).sum())\n",
    "print(val_purchase_df.shape)\n",
    "print(f\"{val_purchase_df.__sizeof__() // 1_000_000} MB\")\n",
    "val_purchase_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_customers_feature(customers: pd.DataFrame, transactions: pd.DataFrame, phase: str, debug: bool = False):\n",
    "    df = transactions.copy()\n",
    "    customers_feature = customers.drop(['postal_code'], axis=1).copy()\n",
    "    customers_feature.loc[~customers_feature['fashion_news_frequency'].isin(['Regularly', 'Monthly']), 'fashion_news_frequency'] = None\n",
    "    customers_feature[['FN', 'Active']] = customers_feature[['FN', 'Active']].fillna(0)\n",
    "\n",
    "    if phase == 'train':\n",
    "        df = df.query('week >= 2').copy()\n",
    "        df['week'] = df['week'] - 2\n",
    "    elif phase == 'val':\n",
    "        df = df.query('week >= 1').copy()\n",
    "        df['week'] = df['week'] - 1      \n",
    "    elif phase == 'test':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"phase must be 'train', 'val', or 'test'\")\n",
    "    \n",
    "    if debug == True:\n",
    "        df = df.query('week < 12')\n",
    "\n",
    "    weekly_purchase = df.groupby(['customer_id', 'week'])['week'].count().rename('purchase').reset_index()\n",
    "    \n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_purchase.groupby('customer_id')['purchase'].agg(agg_name)\n",
    "        customers_feature[f'purchase_{agg_name}_groupby_customer'] = customers_feature['customer_id'].map(agg_sr)\n",
    "    \n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_purchase[weekly_purchase['week']==w]\n",
    "        tmp = tmp[['customer_id', 'purchase']].set_index('customer_id')['purchase']\n",
    "        customers_feature[f'purchase_{w}w'] = customers_feature['customer_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_ratio_{w}w'] = customers_feature[f'purchase_{w}w'] / customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "            customers_feature[f'purchase_{agg_name}_groupby_customer_diff_{w}w'] = customers_feature[f'purchase_{w}w'] - customers_feature[f'purchase_{agg_name}_groupby_customer']\n",
    "\n",
    "    unique_transactions = df[['customer_id', 'article_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['customer_id', 'article_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    customers_feature['repurchase_article'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article'] = customers_feature['customer_id'].map(unique_transactions.drop_duplicates(subset=['customer_id', 'article_id']).groupby('customer_id')['article_id'].count())\n",
    "    customers_feature['repurchase_article_percent'] = customers_feature['repurchase_article'] / customers_feature['purchase_article']\n",
    "\n",
    "    customers_feature['repurchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count()).fillna(0)\n",
    "    customers_feature['purchase_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['customer_id', 'week']).groupby('customer_id')['week'].count())\n",
    "    customers_feature['repurchase_week_percent'] = customers_feature['repurchase_week'] / customers_feature['purchase_week']\n",
    "\n",
    "    customers_feature['repurchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('customer_id')['customer_id'].count()).fillna(0)\n",
    "    customers_feature['purchase_article_and_week'] = customers_feature['customer_id'].map(\n",
    "        unique_transactions.groupby('customer_id')['customer_id'].count())\n",
    "    customers_feature['repurchase_article_and_week_percent'] = customers_feature['repurchase_article_and_week'] / customers_feature['purchase_article_and_week']\n",
    "    \n",
    "    return customers_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_customers_feature = make_customers_feature(customers, transactions, phase='train', debug=True)\n",
    "val_customers_feature = make_customers_feature(customers, transactions, phase='val', debug=True)\n",
    "print('saving...')\n",
    "train_customers_feature.to_csv(f'../input/ranking_features/train_customers_feature.csv', index=False)\n",
    "val_customers_feature.to_csv(f'../input/ranking_features/val_customers_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_customers_feature = pd.read_csv(\n",
    "    f'../input/ranking_features/train_customers_feature.csv',\n",
    "    dtype={'customer_id': 'int32'})\n",
    "print(train_customers_feature.shape)\n",
    "print(f\"{train_customers_feature.__sizeof__() // 1_000_000} MB\")\n",
    "train_customers_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_customers_feature = pd.read_csv(\n",
    "    f'../input/ranking_features/val_customers_feature.csv',\n",
    "    dtype={'customer_id': 'int32'})\n",
    "print(val_customers_feature.shape)\n",
    "print(f\"{val_customers_feature.__sizeof__() // 1_000_000} MB\")\n",
    "val_customers_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_articles_feature(articles: pd.DataFrame, transactions: pd.DataFrame, phase: str, debug: bool = False):\n",
    "    df = transactions.copy()\n",
    "    articles_feature = articles.drop(\n",
    "        ['prod_name', 'product_type_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'prod_name', 'department_name', 'detail_desc'], \n",
    "        axis=1).copy()\n",
    "    \n",
    "    if phase == 'train':\n",
    "        df = df.query('week >= 2').copy()\n",
    "        df['week'] = df['week'] - 2\n",
    "    elif phase == 'val':\n",
    "        df = df.query('week >= 1').copy()\n",
    "        df['week'] = df['week'] - 1      \n",
    "    elif phase == 'test':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"phase must be 'train', 'val', or 'test'\")\n",
    "\n",
    "    if debug == True:\n",
    "        df = df.query('week <= 12')\n",
    "\n",
    "    weekly_sale = df.groupby(['article_id', 'week'])['week'].count().rename('sale').reset_index()\n",
    "    \n",
    "    for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "        agg_sr = weekly_sale.groupby('article_id')['sale'].agg(agg_name)\n",
    "        articles_feature[f'sale_{agg_name}_groupby_article'] = articles_feature['article_id'].map(agg_sr)\n",
    "    \n",
    "    for w in df['week'].unique()[::-1]:\n",
    "        tmp = weekly_sale[weekly_sale['week']==w]\n",
    "        tmp = tmp[['article_id', 'sale']].set_index('article_id')['sale']\n",
    "        articles_feature[f'sale_{w}w'] = articles_feature['article_id'].map(tmp).fillna(0)\n",
    "        for agg_name in ['max', 'min', 'mean', 'sum']:\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_ratio_{w}w'] = articles_feature[f'sale_{w}w'] / articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "            articles_feature[f'sale_{agg_name}_groupby_article_diff_{w}w'] = articles_feature[f'sale_{w}w'] - articles_feature[f'sale_{agg_name}_groupby_article']\n",
    "\n",
    "    unique_transactions = df[['article_id', 'customer_id', 'week']].drop_duplicates()\n",
    "    unique_transactions['rank'] = unique_transactions.groupby(['article_id', 'customer_id'])['week'].rank(method='dense', ascending=False)\n",
    "\n",
    "    articles_feature['resale_customer'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer'] = articles_feature['article_id'].map(unique_transactions.drop_duplicates(subset=['article_id', 'customer_id']).groupby('article_id')['customer_id'].count())\n",
    "    articles_feature['resale_customer_percent'] = articles_feature['resale_customer'] / articles_feature['sale_customer']\n",
    "\n",
    "    articles_feature['resale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count()).fillna(0)\n",
    "    articles_feature['sale_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.drop_duplicates(subset=['article_id', 'week']).groupby('article_id')['week'].count())\n",
    "    articles_feature['resale_week_percent'] = articles_feature['resale_week'] / articles_feature['sale_week']\n",
    "\n",
    "    articles_feature['resale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.query('rank >= 2').groupby('article_id')['article_id'].count()).fillna(0)\n",
    "    articles_feature['sale_customer_and_week'] = articles_feature['article_id'].map(\n",
    "        unique_transactions.groupby('article_id')['article_id'].count())\n",
    "    articles_feature['resale_customer_and_week_percent'] = articles_feature['resale_customer_and_week'] / articles_feature['sale_customer_and_week']\n",
    "\n",
    "    return articles_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles_feature = make_articles_feature(articles, transactions, phase='train', debug=True)\n",
    "val_articles_feature = make_articles_feature(articles, transactions, phase='val', debug=True)\n",
    "print('saving...')\n",
    "train_articles_feature.to_csv(f'../input/ranking_features/train_articles_feature.csv', index=False)\n",
    "val_articles_feature.to_csv(f'../input/ranking_features/val_articles_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles_feature = pd.read_csv(\n",
    "    f'../input/ranking_features/train_articles_feature.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "print(train_articles_feature.shape)\n",
    "print(f\"{train_articles_feature.__sizeof__() // 1_000_000} MB\")\n",
    "train_articles_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_articles_feature = pd.read_csv(\n",
    "    f'../input/ranking_features/val_articles_feature.csv', \n",
    "    dtype={'article_id': 'int32'})\n",
    "print(val_articles_feature.shape)\n",
    "print(f\"{val_articles_feature.__sizeof__() // 1_000_000} MB\")\n",
    "val_articles_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchase_df = train_purchase_df.merge(train_customers_feature, how='left', on=['customer_id'])\n",
    "train_purchase_df = train_purchase_df.merge(train_articles_feature, how='left', on=['article_id'])\n",
    "\n",
    "val_purchase_df = val_purchase_df.merge(val_customers_feature, how='left', on=['customer_id'])\n",
    "val_purchase_df = val_purchase_df.merge(val_articles_feature, how='left', on=['article_id'])\n",
    "\n",
    "print('saving...')\n",
    "train_purchase_df.to_csv('../input/ranking_features/train_purchase_df_merged.csv', index=False)\n",
    "val_purchase_df.to_csv('../input/ranking_features/val_purchase_df_merged.csv', index=False)\n",
    "\n",
    "del train_customers_feature, train_articles_feature, val_customers_feature, val_articles_feature\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchase_df = pd.read_csv(\n",
    "    '../input/ranking_features/train_purchase_df_merged.csv',\n",
    "    dtype={'customer_id': 'int32', 'article_id': 'int32'})\n",
    "\n",
    "print(train_purchase_df.shape)\n",
    "print(f\"{train_purchase_df.__sizeof__() // 1_000_000} MB\")\n",
    "display(train_purchase_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_purchase_df = pd.read_csv(\n",
    "    '../input/ranking_features/val_purchase_df_merged.csv',\n",
    "    dtype={'customer_id': 'int32', 'article_id': 'int32'})\n",
    "\n",
    "print(val_purchase_df.shape)\n",
    "print(f\"{val_purchase_df.__sizeof__() // 1_000_000} MB\")\n",
    "display(val_purchase_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchase_df.select_dtypes(exclude='number').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['club_member_status', 'fashion_news_frequency', 'product_group_name', 'index_code']:\n",
    "    print(train_purchase_df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['customer_id', 'article_id', 'is_purchased']\n",
    "category_columns = ['club_member_status', 'fashion_news_frequency', 'product_group_name', 'index_code']\n",
    "cols = [c for c in train_purchase_df.columns if c not in exclude_columns]\n",
    "\n",
    "with tqdm(cols) as bar:\n",
    "    for c in bar:\n",
    "        if c in category_columns:\n",
    "            bar.set_description(f\"{c}(category)\")\n",
    "            train_purchase_df[c] = train_purchase_df[c].astype('category')\n",
    "            val_purchase_df[c] = val_purchase_df[c].astype('category')\n",
    "        else:\n",
    "            bar.set_description(f\"{c}(float)\")\n",
    "            train_purchase_df[c] = train_purchase_df[c].astype(float)\n",
    "            val_purchase_df[c] = val_purchase_df[c].astype(float)\n",
    "\n",
    "with open(f'../models/lgb_rank/{EXP}_category_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(category_columns, f)\n",
    "with open(f'../models/lgb_rank/{EXP}_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(cols, f)\n",
    "\n",
    "print(len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランク学習\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_iterations': 10000,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'num_threads': 4,  # for M1 Mac\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'metric': ['ndcg', 'map'],\n",
    "    'eval_at': [12],  # 上位何件のランキングをnDCGとMAPの算出に用いるか\n",
    "    'random_state': 41,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = train_purchase_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "dtrain = lgb.Dataset(train_purchase_df[cols], label=train_purchase_df['is_purchased'], group=train_query)\n",
    "val_query = val_purchase_df.groupby('customer_id')['customer_id'].count().to_list()\n",
    "dval = lgb.Dataset(val_purchase_df[cols], reference=dtrain, label=val_purchase_df['is_purchased'], group=val_query)\n",
    "\n",
    "model = lgb.train(\n",
    "    params, dtrain, valid_sets=[dtrain, dval], \n",
    "    callbacks=[lgb.early_stopping(500, first_metric_only=True), lgb.log_evaluation(500)])\n",
    "\n",
    "with open(f'../models/lgb_rank/{EXP}_model_fold1.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance(gain)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is_dummy</td>\n",
       "      <td>61178.624136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count_0w</td>\n",
       "      <td>11859.639997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>sale_0w</td>\n",
       "      <td>6981.868315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>purchase_0w</td>\n",
       "      <td>6553.532780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>count_1w</td>\n",
       "      <td>2395.191624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>purchase_article</td>\n",
       "      <td>2392.243010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>resale_customer_percent</td>\n",
       "      <td>1451.156986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>product_type_no</td>\n",
       "      <td>1300.875978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>age</td>\n",
       "      <td>1214.680098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>purchase_sum_groupby_customer_ratio_0w</td>\n",
       "      <td>1131.096905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>product_group_name</td>\n",
       "      <td>1040.590001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>purchase_article_and_week</td>\n",
       "      <td>993.720501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>product_code</td>\n",
       "      <td>969.881897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>sale_mean_groupby_article</td>\n",
       "      <td>805.627596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>sale_mean_groupby_article_ratio_0w</td>\n",
       "      <td>715.614895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>count_3w</td>\n",
       "      <td>640.842602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>sale_sum_groupby_article_ratio_0w</td>\n",
       "      <td>623.832712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>resale_customer_and_week_percent</td>\n",
       "      <td>607.589396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>repurchase_article_percent</td>\n",
       "      <td>596.339003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>graphical_appearance_no</td>\n",
       "      <td>582.245498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    feature  importance(gain)\n",
       "4                                  is_dummy      61178.624136\n",
       "0                                  count_0w      11859.639997\n",
       "147                                 sale_0w       6981.868315\n",
       "14                              purchase_0w       6553.532780\n",
       "1                                  count_1w       2395.191624\n",
       "123                        purchase_article       2392.243010\n",
       "266                 resale_customer_percent       1451.156986\n",
       "132                         product_type_no       1300.875978\n",
       "9                                       age       1214.680098\n",
       "21   purchase_sum_groupby_customer_ratio_0w       1131.096905\n",
       "133                      product_group_name       1040.590001\n",
       "129               purchase_article_and_week        993.720501\n",
       "131                            product_code        969.881897\n",
       "145               sale_mean_groupby_article        805.627596\n",
       "152      sale_mean_groupby_article_ratio_0w        715.614895\n",
       "3                                  count_3w        640.842602\n",
       "154       sale_sum_groupby_article_ratio_0w        623.832712\n",
       "272        resale_customer_and_week_percent        607.589396\n",
       "124              repurchase_article_percent        596.339003\n",
       "134                 graphical_appearance_no        582.245498"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'feature': model.feature_name(), 'importance(gain)': model.feature_importance('gain')}).sort_values('importance(gain)', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(val_purchase_df[cols], num_iteration=model.best_iteration)\n",
    "np.sort(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 1]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "popular_items_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "val_purchase_df2 = val_purchase_df.copy()\n",
    "val_purchase_df2['predict_score'] = val_pred\n",
    "val_purchase_df2['rank'] = val_purchase_df2.groupby('customer_id')['predict_score'].rank('dense', ascending=False)\n",
    "val_purchase_df2 = val_purchase_df2[val_purchase_df2['rank'] <= 12]\n",
    "val_purchase_df2 = val_purchase_df2.sort_values('rank').reset_index(drop=True)\n",
    "# val_purchase_df2['article_id'] = le.inverse_transform(val_purchase_df2['article_id'])\n",
    "val_purchase_df2['article_id'] = ' 0' + val_purchase_df2['article_id'].astype(str)\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(val_purchase_df2.groupby('customer_id')['article_id'].sum())\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "submission = submission[['customer_id', 'prediction']]\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f'../submissions/{EXP}_submission_fold1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_query, dtrain, val_query, dval, val_pred\n",
    "del transactions_last_week, top12, popular_items, popular_items_dict, popular_items_sr\n",
    "del val_purchase_df2, submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all datas\n",
    "train_purchase_df_all = pd.concat([train_purchase_df, val_purchase_df], axis=0)\n",
    "\n",
    "del train_purchase_df, val_purchase_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(cols) as bar:\n",
    "    for c in bar:\n",
    "        if c in category_columns:\n",
    "            bar.set_description(f\"{c}(category)\")\n",
    "            train_purchase_df_all[c] = train_purchase_df_all[c].astype('category')\n",
    "        else:\n",
    "            bar.set_description(f\"{c}(float)\")\n",
    "            train_purchase_df_all[c] = train_purchase_df_all[c].astype(float)\n",
    "\n",
    "train_query = train_purchase_df_all.groupby('customer_id')['customer_id'].count().to_list()\n",
    "dtrain = lgb.Dataset(train_purchase_df_all[cols], label=train_purchase_df_all['is_purchased'], group=train_query)\n",
    "\n",
    "params['num_iterations'] = model.best_iteration\n",
    "model = lgb.train(\n",
    "    params, dtrain, valid_sets=[dtrain],\n",
    "    callbacks=[lgb.log_evaluation(10)])\n",
    "\n",
    "with open(f'../models/lgb_rank/{EXP}_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_purchase_df_all, train_query, dtrain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 234/234 [07:11<00:00,  1.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "341      0928206001 0929275001 0924243001 0889550002 0...\n",
       "381      0610776002 0399256001 0882882010 0827968022 0...\n",
       "782      0902265003 0921671001 0885951001 0835704001 0...\n",
       "926      0928206001 0929275001 0918292001 0909370001 0...\n",
       "1034     0835348011 0928206001 0929275001 0918292001 0...\n",
       "Name: article_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "test_customers = transactions.query(\"week < 4\")['customer_id'].unique()\n",
    "test_customers_feature = make_customers_feature(customers, transactions, phase='test', debug=True)\n",
    "test_articles_feature = make_articles_feature(articles, transactions, phase='test', debug=True)\n",
    "\n",
    "with open(f'../models/lgb_rank/{EXP}_category_columns.pkl', 'rb') as f:\n",
    "    category_columns = pickle.load(f)\n",
    "with open(f'../models/lgb_rank/{EXP}_cols.pkl', 'rb') as f:\n",
    "    cols = pickle.load(f)    \n",
    "with open(f'../models/lgb_rank/{EXP}_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(len(test_customers)//BATCH_SIZE + 1)):\n",
    "    if i == (len(test_customers)//BATCH_SIZE):\n",
    "        transactions_batch = transactions[transactions['customer_id'].isin(test_customers[i*BATCH_SIZE:])]\n",
    "    else:\n",
    "        transactions_batch = transactions[transactions['customer_id'].isin(test_customers[i*BATCH_SIZE : (i+1)*BATCH_SIZE])]\n",
    "    test_purchase_df = make_purchase_df(transactions_batch, phase='test', debug=True)\n",
    "    test_purchase_df = test_purchase_df.merge(test_customers_feature, how='left', on=['customer_id'])\n",
    "    test_purchase_df = test_purchase_df.merge(test_articles_feature, how='left', on=['article_id'])\n",
    "\n",
    "    for c in cols:\n",
    "        if c in category_columns:\n",
    "            test_purchase_df[c] = test_purchase_df[c].astype('category')\n",
    "        elif c not in test_purchase_df.columns:\n",
    "            test_purchase_df[c] = np.nan\n",
    "            test_purchase_df[c] = test_purchase_df[c].astype(float)\n",
    "        else:\n",
    "            test_purchase_df[c] = test_purchase_df[c].astype(float)\n",
    "\n",
    "    pred = model.predict(test_purchase_df[cols], num_iteration=model.best_iteration)\n",
    "    \n",
    "    test_purchase_df['predict_score'] = pred\n",
    "    test_purchase_df['rank'] = test_purchase_df.groupby('customer_id')['predict_score'].rank('dense', ascending=False)\n",
    "    test_purchase_df = test_purchase_df[test_purchase_df['rank'] <= 12]\n",
    "    test_purchase_df = test_purchase_df.sort_values('rank').reset_index(drop=True)\n",
    "    # test_purchase_df['article_id'] = le.inverse_transform(test_purchase_df['article_id'])\n",
    "    test_purchase_df['article_id'] = ' 0' + test_purchase_df['article_id'].astype(str)\n",
    "    preds.append(test_purchase_df.groupby('customer_id')['article_id'].sum())\n",
    "    \n",
    "pred_sr = pd.concat(preds, axis=0)\n",
    "display(pred_sr.head())\n",
    "\n",
    "del test_purchase_df, pred, preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 12 popular items:\n",
      " 0924243001 0924243002 0918522001 0923758001 0866731001 0909370001 0751471001 0915529003 0915529005 0448509014 0762846027 0714790020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0     0685813003 0918522001 0715624001 0850917001 0...\n",
       "1.0     0924243001 0866731001 0909370001 0918522001 0...\n",
       "2.0     0923758001 0909370001 0924243001 0935541001 0...\n",
       "3.0     0751471001 0928206001 0924243001 0924243002 0...\n",
       "4.0     0924243001 0928206001 0930380001 0924243002 0...\n",
       "5.0     0930380001 0924243001 0751471043 0910601003 0...\n",
       "6.0     0751471043 0930380001 0865799006 0714790030 0...\n",
       "Name: top_12_popular_items, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most popular items\n",
    "transactions_last_week = transactions.loc[transactions.week == 0]\n",
    "top12 = ' 0' + ' 0'.join(transactions_last_week.article_id.value_counts().index.astype('str')[:12])\n",
    "print(\"Top 12 popular items:\")\n",
    "print( top12 )\n",
    "\n",
    "customers['age_bin'] = pd.cut(customers['age'], bins=[10, 20, 30, 40, 50, 60, 70, 100], labels=False)\n",
    "transactions_last_week = transactions_last_week.merge(customers[['customer_id', 'age', 'age_bin']], how='left')\n",
    "popular_items = transactions_last_week.groupby('age_bin')['article_id'].value_counts()\n",
    "popular_items_dict = {}\n",
    "for index in popular_items.index.levels[0]:\n",
    "    popular_items_dict[index] = ' 0'+' 0'.join(popular_items[index][:12].index.astype('str'))\n",
    "popular_items_sr = pd.Series(popular_items_dict, name='top_12_popular_items', dtype='str')\n",
    "popular_items_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0568601043 0909370001 0889550002 0898573003 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0924243001 0866731001 0909370001 0918522001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0794321007 0924243001 0827968001 0909370001 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>0924243001 0928206001 0930380001 0924243002 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>0924243001 0928206001 0930380001 0924243002 09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
       "\n",
       "                                          prediction  \n",
       "0  0568601043 0909370001 0889550002 0898573003 08...  \n",
       "1  0924243001 0866731001 0909370001 0918522001 09...  \n",
       "2  0794321007 0924243001 0827968001 0909370001 07...  \n",
       "3  0924243001 0928206001 0930380001 0924243002 09...  \n",
       "4  0924243001 0928206001 0930380001 0924243002 09...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\n",
    "\n",
    "submission['prediction_lgb'] = submission['customer_id'].map(id_to_index_dict).map(pred_sr)\n",
    "submission['prediction_lgb'] = submission['prediction_lgb'].fillna('')\n",
    "\n",
    "submission['age_bin'] = submission['customer_id'].map(id_to_index_dict).map(customers.set_index('customer_id')['age_bin'])\n",
    "submission['prediction_popular'] = submission['age_bin'].map(popular_items_sr)\n",
    "submission['prediction_popular'] = submission['prediction_popular'].fillna(top12).astype('str')\n",
    "\n",
    "submission['prediction'] = submission['prediction_lgb'] + submission['prediction_popular']\n",
    "submission['prediction'] = submission['prediction'].str.strip()\n",
    "submission['prediction'] = submission['prediction'].str[:131]\n",
    "submission = submission[['customer_id', 'prediction']]\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f'../submissions/{EXP}_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f804528fa1bcda80b2057737773baa2134c5be7e013153f88e69abab752260b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
